Teacher,Title/Subject,Year,Source title,Cited by,Link,Abstract,Author Keywords,Index Keywords,Document Type,EID,
tnt,An association rule mining approach to discover demand and supply patterns based on Thai social media data,2021,International Journal of Knowledge and  Systems Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104037866&doi=10.4018%2fIJKSS.2021040101&partnerID=40&md5=49391291f5d737def1bf7779bfa6df1d,"In the digital age, social media technology has an important role as a communication platform for interpersonal interactions in the online virtual world. In addition, social media has impacted product exchange behavior in both vendors and buyers, with a shift from the traditional sales model to communication between parties via social media. Social media marketing, an online means of buying, selling, and exchanging goods and services, is increasingly popular due to convenience, speed, and greater choices. This trend has grown rapidly and is set to expand, leading to increased interest in research which analyzes and processes social media marketing data to gain a new integrated body of knowledge to better serve online business transactions. This research covers a new field, which may cause research and development limitations requiring background knowledge in several areas, such as digital technology, data analytics, and business analysis. This research aims to develop a framework to search for association rule mining of demand and supply data on social media platforms. Data is collected from Twitter and underwent cleansing and labeling for separating into five groups. Hashtag data from tweets is then extracted and transformed to input attributes of the framework. Next, association rule mining is performed using the Apriori algorithm in order to determine frequent items and extract candidate association rules. The last stage is rule selection, which uses Twitter-specific statistical attributes, that is, number of retweets and likes, to select highly effective association rules. The findings are that it is possible to mine association rules relating to demand and supply on Twitter. Based on an analysis of the association rule results, the content of those rules reflects trending activities and events at different times. Such information can be analyzed in further research to design improvements in social media marketing. © 2021, IGI Global.",Association rule mining; Demand and supply analysis; Social media analysis,,Article,2-s2.0-85104037866,
pps,Improvement of Difficulty Parameter Equating base on Ability Parameter Distribution Adjustment,2021,KST 2021 - 2021 13th International Conference Knowledge and Smart Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105890877&doi=10.1109%2fKST51265.2021.9415809&partnerID=40&md5=0b66e2520e313c77d9a690c0860787eb,"In automated test construction, one of the remaining issues is an inaccurate estimation of exam item parameters from different examinee ability distribution in linear equating. This work proposes a method to adjust the examinee's ability distribution to prevent carrying on error rates from the previous calibration to affect later procedures. By making the examinee's ability distribution in the minor' group to be similar to the examinee's ability distribution in the major distribution, the few errors are generated. From experiments, the proposed methods yield better results than those of baseline in terms of Root Mean Square Error in all test combinations. © 2021 IEEE.",distribution; Item Response Theory (IRT); Linear Equating; parameter estimation; test,Mean square error; Automated test construction; Error rate; Parameter distributions; Root mean square errors; Errors,Conference Paper,2-s2.0-85105890877,
pps,Data Preparation for Reducing Computational Time with Transpose Stack Matrix for Action Recognition,2021,KST 2021 - 2021 13th International Conference Knowledge and Smart Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105869338&doi=10.1109%2fKST51265.2021.9415834&partnerID=40&md5=14a87accd9cf65820b2074a31d7841b5,Training model in deep learning on action recognition faces a problem with high computational cost and ineffective performance to deploy in common devices. The goal of this research is to reduce the computational cost using the data preparation technique called transpose stack matrix (TSM). TSM can be applied with convolution2D and extracts spatiotemporal features. We perform experiments using two datasets including KTH and UCF101(spilt-1). The results show that TSM requires less training time than I3D approach and has satisfactory performance. TSM has higher accuracy than I3D with 73.04% on UCF101 while TSM has accuracy approximate to I3D with 72.69% on KTH. When training on 5 epochs. © 2021 IEEE.,Action recognition; Computational cost; Deep learning,Deep learning; Action recognition; Computational costs; Computational time; Data preparation; Spatio temporal features; Stack matrix; Training model; Training time; Matrix algebra,Conference Paper,2-s2.0-85105869338,
ppr,Overhead study of telegraf as a real-time monitoring agent,2020,JCSSE 2020 - 17th International Joint Conference on Computer Science and Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098488755&doi=10.1109%2fJCSSE49651.2020.9268333&partnerID=40&md5=d7ef905c30c8146e4636e3e346f971ce,"Large-scale distributed systems have become an essential part of our everyday life. These systems have a large number of hardware and software components, often cooperating in complex and unpredictable ways. Operating these kinds of systems requires centralized monitoring to understand their overall states. While running software to collect metrics in a server is considered common nowadays, it often goes unstudied the impact metric collection software have on the base system. This is especially important in low-power, IoT applications. According to our review, one particular software, Telegraf, has never been formally studied before in terms of how much overhead Telegraf adds to the base system. In this work, we conducted several experiments to study how the base system is affected by Telegraf in two scenarios: a datacenter server and an IoT node. The results show that Telegraf is lightweight and suitable to serve as a real-time monitoring agent in both scenarios. Copyright © JCSSE 2020 - 17th International Joint Conf. on Computer Science and Software Engineering.",Datacenter; Experimental study; IoT; Monitoring; Overhead; Resource utilization,,Conference Paper,2-s2.0-85098488755,
ssr,Method Evaluation for Software Testability on Object Oriented Code,2020,"2020 59th Annual Conference of the Society of Instrument and Control Engineers of Japan, SICE 2020",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096361247&partnerID=40&md5=9aee2369577fba783126cb5f08076d28,"In software testability, there are various software measurement methods and metrics available for selection. To conduct the software testability evaluation, the combination of software expert tester judgement and appropriate metrics selection has reflected the of software quality accuracy. However, due to perspective of software expert tester, the software measurement and metric selection can lead to an inaccurate software testability result. Moreover, time and budget waste in conduct the software testability evaluation. Thus, this research aims to develop a testability measurement on model on Object-oriented programming software that will facilitate the accurate evaluation result. The investigation on how the factors of classes in a system influent the test effort. The research covers the gap analysis, automatic model creation with ANNs of AI Technology, and testability evaluation result comparison. © 2020 The Society of Instrument and Control Engineers - SICE.",Software Metrics tools; Software System Evaluation; Software Testability Metric,Artificial intelligence; Budget control; Computer software selection and evaluation; Quality control; Software quality; Appropriate metrics; Automatic modeling; Evaluation results; Method evaluation; Object-oriented code; Software Measurement; Software testability; Testability evaluation; Object oriented programming,Conference Paper,2-s2.0-85096361247,
tnt,Sentiment Classification on Thai Social Media Using a Domain-Specific Trained Lexicon,2020,"17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091874300&doi=10.1109%2fECTI-CON49241.2020.9158329&partnerID=40&md5=cf46b7b78af29a1e13c9ba3d72eafb43,"Social media contains many valuable documents which can be used to retrieve insights into any targeted topic. Sentiment analysis is a task to extract opinions of people out of their writing. Although a method using a sentiment lexicon is suitable for sentiment classification on social media documents, a lexicon created without consideration of any specific topic domain may not reflect actual intention in the domain of the considered documents. This study compares a sentiment lexicon created from documents in a specific domain with a generic lexicon. The experimental results show that using a specific lexicon improves classification performance, particularly on documents having non-neutral sentiment. © 2020 IEEE.",domain-specific lexicon; lexicon-based sentiment classification; sentiment analysis; Thai social media,Information retrieval systems; Social networking (online); Classification performance; Domain specific; Non-neutral; Sentiment classification; Sentiment lexicons; Social media; Sentiment analysis,Conference Paper,2-s2.0-85091874300,
tnt,Extraction of Trend Keywords from Thai Twitters using N-Gram Word Combination,2020,"17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091845831&doi=10.1109%2fECTI-CON49241.2020.9158061&partnerID=40&md5=b1b34958a6bbd50938a98a4892cb96fd,"Extracting keywords from text on social media facilitates people to update news and trends. It reduces time spent for identifying main content from huge amount of data, and it can be used to identify situations or events that most of people mention in each period of time. This paper proposes a method for extracting keywords from Thai text on social media. A N-gram-based word-combination technique is presented to segment words that are not in dictionaries and increase the precision of word segmentation. Posts on Twitter concerning universities in Thailand are used as a case study for extracting keywords and analyzing trends. The experimental results show that the proposed method yield the highest precision of 70%. © 2020 IEEE.",keyword extraction; Social media; Thai university; twitter data; word combination,Computer science; Computers; Electrical engineering; Mathematical techniques; N-grams; Social media; Thailand; Time spent; Word combinations; Word segmentation; Social networking (online),Conference Paper,2-s2.0-85091845831,
ppr,Toponym Extraction in Thai Tweets Using a Hybrid Approach,2020,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086138557&doi=10.1145%2f3385209.3385225&partnerID=40&md5=bfd7c33b60fb644f0597be2ece6ad13f,"Crowdsourcing has become an important tool in areas such as business and marketing. It can help organizations solve large-scale problems in areas including traffic management and political campaigning. Toponym extraction is necessary when analyzing crowdsourced data for traffic tracking or event reporting. Dictionaries and rule-based analysis are commonly used for matching and extracting entities from text. However, the creation of an effective dictionary is not an easy task, especially when the goal is to name a large number of locations. Named Entity Recognition (NER) can help address this, but the approach has certain limitations. In this paper, we describe an improved approach to toponym extraction from Twitter messages that combines a dictionary and NER. As tweets are limited to 280 characters, any locations mentioned are usually referred to using abbreviations. The variety of forms that location names take, and the unstructured language of tweets, are challenging both to the dictionary and NER methods. We divided tweets into four categories to investigate the effect of analyzing messages from different domains. The average accuracy was 49.18% when using only the dictionary, 59.30% when using only NER, and 75.43% when using the hybrid method. © 2020 ACM.",Dictionary; NER; Toponym extraction,Crowdsourcing; Location; Different domains; Event reporting; Hybrid approach; Large-scale problem; Named entity recognition; Political campaigning; Traffic management; Traffic tracking; Extraction,Conference Paper,2-s2.0-85086138557,
tnt,Simulation analysis of university hospital in the medical record department,2019,International Conference on ICT and Knowledge Engineering,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078998253&doi=10.1109%2fICTKE47035.2019.8966789&partnerID=40&md5=c0818b5a9c6d88aa7904de4c94d2180a,"Many hospitals have a problem dealing with the queue system in every department. The front-end department, which is the medical record department, is the first place to contact patients. It provides service for all type of out-patients, so out-patients have to wait for a long time. This results in low satisfaction of the patients. However, this department is working 24 hours, thus, it isdifficult to improve the queue system in a real environment. The simulation is an effective tool to solve the problem. Based on the data collection from Thammasat University hospital, the discrete event simulation model of this department is developed. Model verification and validation are conducted and the resultconfirms that the model is worked as calculated and generates the same result as in the real system. The aim of this paper is to develop the simulation model that provides an accurate analysis to help support a decision making of the hospital in the medical record department. © 2019 IEEE.",Discrete event simulation; Patient flow; University hospital,Decision making; Hospitals; Knowledge engineering; Accurate analysis; Data collection; Discrete-event simulation model; Model verification; Patient flow; Real environments; Simulation analysis; Simulation model; Discrete event simulation,Conference Paper,2-s2.0-85078998253,
ppr,EyeMath: Increasing Accessibility of Mathematics to Visually Impaired Readers,2019,ICSEC 2019 - 23rd International Computer Science and Engineering Conference,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081656218&doi=10.1109%2fICSEC47112.2019.8974682&partnerID=40&md5=6dc2b71acd7b11ad0b99e6690c51a0b4,"Mathematics education for visually impaired students is challenging because their learning materials are generally limited to braille books, and audiobooks. In order to increase the chance of learning mathematical content for people with visual impairment, this paper presents the design and development of a cloud-based mobile application called EyeMath, using serverless microservices in Amazon AWS. Users can provide images of page snippets for the application to process and read their content to the users. EyeMath segments an input image into smaller pieces and separates pieces that have only plain text from pieces with mathematical symbols. The mathematical-related pieces are further processed into an Abstract Syntax Tree (AST) and then parsed into Thai sentences. For plain text pieces, EyeMath relies on Tesseract OCR to convert them into text. Finally, results for all pieces are combined together systematically for the device's screen reader program to read aloud. The performance evaluation of the application shows high correctness in reading math content within test images and our usability testing confirms the potential usefulness of the application to visually impaired readers. © 2019 IEEE.",Image processing; Language parsing; Learning accessibility; Mathematical expression recognition; Mobile Application; Optical Character Recognition (OCR); Serverless microservices,Abstracting; Mobile computing; Optical character recognition; Optical data processing; Syntactics; Trees (mathematics); Language parsing; Learning accessibility; Mathematical expressions; Mobile applications; Optical character recognition (OCR); Serverless microservices; Image processing,Conference Paper,2-s2.0-85081656218,
yao,Sound Tooth: Mobile Oral Health Exam Recording Using Individual Voice Recognition,2019,"Proceedings of 2019 4th International Conference on Information Technology: Encompassing Intelligent Technology and Innovation Towards the New Era of Human Life, InCIT 2019",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076757161&doi=10.1109%2fINCIT.2019.8911926&partnerID=40&md5=2b552bfeb05e05a7f48a723ea2ac2f67,"Detecting voice commands accurately are not simple tasks. Still, voice recognition applications are needed, i.e., voice instruction for devices in a smart home. To recognize well, many sample sounds must be used for training. To avoid the limitation of collecting many sampled sounds, the on-site training by the sound of smartphone users could be an alternative. 'Sound Tooth', a voice command mobile application, was designed according to 'trained by the user' concept. It helps dentists to record school children oral health examination. To record them, the dentists provide short selected tooth status in a form of chosen isolated words, either in Thai or English. For this work, we applied the voice recognizer, named CMU PocketSphinx API. The system has been tested two ways: SphinxTrainer and participants' testing. The system training result reports the WER = 8.8% (~ 91% accuracy). In the participants' test, we compare the system using the speaker independent acoustic model to individual's one. The test results show the accuracy can be improved from 68% in speaker independent model to 83.3% accuracy for individual one, giving 15% better. © 2019 IEEE.",dental exams; mobile application; PocketSphinx; voice command recognition,Automation; Dentistry; Mobile computing; Acoustic model; dental exams; Mobile applications; On-site training; PocketSphinx; Speaker independent model; Speaker independents; Voice command recognition; Speech recognition,Conference Paper,2-s2.0-85076757161,
ksc,Amdahl’s law of data compression for live migration of virtual machines,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076724563&doi=10.1145%2f3361821.3361836&partnerID=40&md5=1efbf441f9b955b7bd381c6949b2b865,"Data compression is usually perceived as a mean to help improve performance of live migration of Virtual Machines (VM). However, such perception may be wrong since there are many factors involved in the combination of live migration and data compression. This paper presents a novel performance model to describe the speedup of VM live migration when used in combination with data compression mechanisms, namely the Amdahl’s Law of data compression for live migration of VMs. The speedup is calculated by dividing the total live migration time when the migration is performed in combination with data compression by that without data compression. This model introduces live migration bandwidth as a new factor in the speedup calculation. The live migration bandwidth is the data transmission rate of a live migration mechanism. According to our model, the speedup of live migration using data compression would approach zero as the live migration bandwidth increases toward infinity. A set of theorems derived from the model are presented. A number of experiments have been conducted to confirm these theorems. In the experiments, QEMU-KVM’s pre-copy live migration is used to migrate VMs running several memory-intensive OpenMP NAS Parallel benchmarks with and without data compression. Live migrations are performed over 100 Mbps and 1 Gbps networks. Experimental results confirm the validity of the proposed Amdahl’s Law of data compression model. © 2019 Association for Computing Machinery.",Cloud Computing; Data Compression; Live Migration of Virtual Machines; Performance Evaluation; Virtual Machines,Application programming interfaces (API); Bandwidth; Bandwidth compression; Cloud computing; Computation theory; Internet of things; Network security; Virtual machine; Compression mechanism; Compression model; Data transmission rates; Improve performance; Live migrations; NAS parallel benchmarks; Performance Evaluation; Performance Model; Data compression,Conference Paper,2-s2.0-85076724563,
tpb,Multi-aspect embedding for attribute-aware trajectories,2019,Symmetry,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079672643&doi=10.3390%2fSYM11091149&partnerID=40&md5=e13c12346c889a7ffd2215feff6aa5de,"Motivated by the proliferation of trajectory data produced by advanced GPS-enabled devices, trajectory is gaining in complexity and beginning to embroil additional attributes beyond simply the coordinates. As a consequence, this creates the potential to define the similarity between two attribute-aware trajectories. However, most existing trajectory similarity approaches focus only on location based proximities and fail to capture the semantic similarities encompassed by these additional asymmetric attributes (aspects) of trajectories. In this paper, we propose multi-aspect embedding for attribute-aware trajectories (MAEAT), a representation learning approach for trajectories that simultaneously models the similarities according to their multiple aspects. MAEAT is built upon a sentence embedding algorithm and directly learns whole trajectory embedding via predicting the context aspect tokens when given a trajectory. Two kinds of token generation methods are proposed to extract multiple aspects from the raw trajectories, and a regularization is devised to control the importance among aspects. Extensive experiments on the benchmark and real-world datasets show the effectiveness and efficiency of the proposed MAEAT compared to the state-of-the-art and baseline methods. The results of MAEAT can well support representative downstream trajectory mining and management tasks, and the algorithm outperforms other compared methods in execution time by at least two orders of magnitude. © 2019 by the authors.",Multi-aspect embedding; Representation learning; Trajectory similarity computation,,Article,2-s2.0-85079672643,
pkl,Association extraction from functional testing scenarios,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072033688&doi=10.1145%2f3343147.3343151&partnerID=40&md5=d5418482600eec8a745c03fd29a268df,"In this paper, we propose an approach to extract association among software pages from functional test case scenarios. This knowledge helps us to re-design user interfaces of a software application for the purpose of usability. Initially, functional test case scenarios are applied to the association extraction approach. The association sets are then obtained and post-processed. The association is considered in two aspects including 1) all frequent pages association and 2) consecutive pages association. Both associations are used to simplify user interfaces of the software application. We demonstrate our proposed method with the Mobile Application Development Quiz in [7] as a case study. © 2019 Association for Computing Machinery.",Association mining; Association rule; Functional testing; User interface design,Application programs; Association rules; Extraction; Mobile computing; Software testing; Association mining; Functional test; Functional testing; Mobile application development; Software applications; User interface designs; User interfaces,Conference Paper,2-s2.0-85072033688,
ddp,Association extraction from functional testing scenarios,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072033688&doi=10.1145%2f3343147.3343151&partnerID=40&md5=d5418482600eec8a745c03fd29a268df,"In this paper, we propose an approach to extract association among software pages from functional test case scenarios. This knowledge helps us to re-design user interfaces of a software application for the purpose of usability. Initially, functional test case scenarios are applied to the association extraction approach. The association sets are then obtained and post-processed. The association is considered in two aspects including 1) all frequent pages association and 2) consecutive pages association. Both associations are used to simplify user interfaces of the software application. We demonstrate our proposed method with the Mobile Application Development Quiz in [7] as a case study. © 2019 Association for Computing Machinery.",Association mining; Association rule; Functional testing; User interface design,Application programs; Association rules; Extraction; Mobile computing; Software testing; Association mining; Functional test; Functional testing; Mobile application development; Software applications; User interface designs; User interfaces,Conference Paper,2-s2.0-85072033688,
ppr,Multi-Container Application Migration with Load Balanced and Adaptive Parallel TCP,2019,"2019 International Conference on High Performance Computing and Simulation, HPCS 2019",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092032758&doi=10.1109%2fHPCS48598.2019.9188218&partnerID=40&md5=220853f3ca8743d9abceac3c76c339e6,"Application migration in Wide Area Network (WAN) is needed in many scenarios: disaster recovery and service hand-off in edge cloud. Modern distributed applications are virtualized with multiple virtual machines or containers. This paper focuses on parallel multi-container migration in WAN environments by utilizing multiple TCP connections over a single direct path (a.k.a parallel TCP). Our application migration middleware framework utilizes a feedback controller to determine a proper number of parallel container migration (i.e., parallel window) based on changing network bandwidth. Then a middleware's scheduler selects migration requests for the parallel window to load balance multiple pairs of hosts. The goal of our migration is to achieve the best possible balance between optimizing the total migration time and average individual migration time. This differs from most existing live migration works that attempt to optimize mainly the down time. Our proposed framework is generalized and not restricted to any particular virtualization technology implementation. For performance evaluation, we conducted a WAN-emulated experiment in static and dynamic network settings. The performance evaluation results show that the total migration time using our feedback controller is less than that of the sequential migration by 32.7% in the static network, and 43.9% in the dynamic network. Moreover, while achieving total migration time comparable to that of the best fixed parallel migration window, our approach can reduce the average individual migration time by 62.7% in the dynamic network setting. © 2019 IEEE.",Application Migration; Container Migration; Feedback Controller; Load Balancing; Middleware; Parallel TCP Stream; Scheduling; VM Migration,Containers; Feedback control; Middleware; Transmission control protocol; Application migrations; Distributed applications; Evaluation results; Feedback controller; Individual migrations; Multiple TCP connections; Total migration time; Virtualization technologies; Wide area networks,Conference Paper,2-s2.0-85092032758,
ksc,An In-Memory Checkpoint-Restart Mechanism for a Cluster of Virtual Machines,2019,JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074226885&doi=10.1109%2fJCSSE.2019.8864198&partnerID=40&md5=fb421b38aa3be7dc10c283aa8c70d412,"A cluster of virtual machines can be used to execute parallel applications in Cloud Computing environments. However, the cloud infrastructure may fail at any time for a variety of reasons. Although a coordinated checkpointing capability at the hypervisor level is highly transparent to parallel applications, existing solutions still suffer from excessive checkpoint time and downtime. They also cause significant application execution delays due to packet loss. This paper introduces IMVCCR, a novel in-memory Checkpoint-Restart mechanism for a virtual cluster. IMVCCR consists of a framework that performs coordinated checkpointing for the entire cluster. It reduces checkpoint time and downtime by applying live migration and using main memory as transient checkpoint storage. IMVCCR also uses an efficient synchronization mechanism to reduce packet loss. Preliminary experiments show that IMVCCR generates very low checkpoint times and downtimes. It also incurs low overheads in the total execution time of parallel applications. © 2019 IEEE.",,Cloud computing; Maintenance; Network security; Packet loss; Virtual machine; Application execution; Checkpoint storages; Cloud computing environments; Cloud infrastructures; Coordinated checkpointing; Parallel application; Restart mechanism; Synchronization mechanisms; Cluster computing,Conference Paper,2-s2.0-85074226885,
tpb,Grid-based DBSCAN: Indexing and inference,2019,Pattern Recognition,10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060972156&doi=10.1016%2fj.patcog.2019.01.034&partnerID=40&md5=105d6993bcf4a9ebc462f13dc911a017,"DBSCAN is one of clustering algorithms which can report arbitrarily-shaped clusters and noises without requiring the number of clusters as a parameter (unlike the other clustering algorithms, k-means, for example). Because the running time of DBSCAN has quadratic order of growth, i.e. O(n 2 ), research studies on improving its performance have been received a considerable amount of attention for decades. Grid-based DBSCAN is a well-developed algorithm whose complexity is improved to O(nlog n) in 2D space, while requiring Ω(n 4/3 ) to solve when dimension ≥ 3. However, we find that Grid-based DBSCAN suffers from two problems: neighbour explosion and redundancies in merging, which make the algorithms infeasible in high dimensional space. In this paper we first propose a novel algorithm called GDCF which utilizes bitmap indexing to support efficient neighbour grid queries. Second, based on the concept of union-find algorithm we devise a forest-like structure, called cluster forest, to alleviate the redundancies in the merging. Moreover, we find that running the cluster forest in different orders can lead to a different number of merging operations needed to perform in the merging step. We propose to perform the merging step in a uniform random order to optimize the number of merging operations. However, for high-density database, a bottleneck could be occurred, we further propose a low-density-first order to alleviate this bottleneck. The experiments resulted on both real-world and synthetic datasets demonstrate that the proposed algorithm outperforms the state-of-the-art exact/approximate DBSCAN and suggests a good scalability. © 2019 Elsevier Ltd",Density-based clustering; Grid-based DBSCAN; Union-find algorithm,Computational complexity; Forestry; Indexing (of information); Merging; Redundancy; Density-based Clustering; Grid-based; High dimensional spaces; Number of clusters; Research studies; State of the art; Synthetic datasets; Union find; K-means clustering,Article,2-s2.0-85060972156,
wdp,On Studying of Scalability in Single-Controller Software-Defined Networks,2019,"2019 11th International Conference on Knowledge and Smart Technology, KST 2019",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065079284&doi=10.1109%2fKST.2019.8687678&partnerID=40&md5=3bf9c62aed48438b044aa05401a59986,"Software-Defined Networks (SDN) introduce a new networking architecture that separates the control and data planes from each other. The SDN controller inside the control plane governs and monitors all network devices in the data plane using the OpenFlow protocol. The centralized manner of SDNs results in the well-known scalability problem. This paper studied the impact of network size increases to the service inside SDNs. The performance metrics used to indicate the quality of network service are end-To-end delays, throughput, and percentage of successful transmissions as network size increased. We focused on SDNs controlled by two market-leading controllers, namely OpenDaylight (ODL) and Open Network Operating System (ONOS). Our results indicated that increasing of network sizes impacted SDNs controlled and monitored by ODL and ONOS in a similar manner. However, we found that ODL and ONOS yielded different degree of scalability. © 2019 IEEE.",component; scalability; software-defined networks,Scalability; Software defined networking; component; Degree of scalability; End to end delay; Network services; Networking architecture; Performance metrics; Scalability problems; Single controllers; Controllers,Conference Paper,2-s2.0-85065079284,
lpp,Age differences in menu item selection for smartphone: The effects of icon background colors and icon symbols,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072219702&doi=10.1145%2f3328243.3328251&partnerID=40&md5=f4936282ee679760d788b83a8d8b1f8b,"Previous research found that icon background colors and symbols influenced menu selection time. However, the previous research did not consider non-generic users; especially, children and older adult users that have gathered momentum. Literature highlights the age-related differences in various dimensions, and how they are related to technology use. The purpose of the present study is to extend the previous research on the influence of icon background colors and symbols [1], by examining the age-related difference in smartphone menu item selection task, with the manipulation of icon background colors and icon symbols. A three-way ANOVA indicated that there are significant main effects of age group, background colors, and icon symbols, as well as high-order effects of the three variables. © 2019 Association for Computing Machinery.",Age difference; Background color; Children; Elderly; Icon; Menu; Mobile; Senior; Smartphone; Visual search,Human computer interaction; Smartphones; User interfaces; Age differences; Children; Elderly; Icon; Menu; Mobile; Senior; Visual search; Color,Conference Paper,2-s2.0-85072219702,
nth,Age differences in menu item selection for smartphone: The effects of icon background colors and icon symbols,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072219702&doi=10.1145%2f3328243.3328251&partnerID=40&md5=f4936282ee679760d788b83a8d8b1f8b,"Previous research found that icon background colors and symbols influenced menu selection time. However, the previous research did not consider non-generic users; especially, children and older adult users that have gathered momentum. Literature highlights the age-related differences in various dimensions, and how they are related to technology use. The purpose of the present study is to extend the previous research on the influence of icon background colors and symbols [1], by examining the age-related difference in smartphone menu item selection task, with the manipulation of icon background colors and icon symbols. A three-way ANOVA indicated that there are significant main effects of age group, background colors, and icon symbols, as well as high-order effects of the three variables. © 2019 Association for Computing Machinery.",Age difference; Background color; Children; Elderly; Icon; Menu; Mobile; Senior; Smartphone; Visual search,Human computer interaction; Smartphones; User interfaces; Age differences; Children; Elderly; Icon; Menu; Mobile; Senior; Visual search; Color,Conference Paper,2-s2.0-85072219702,
nth,A preliminary study to evaluate graphical passwords for older adults,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072195206&doi=10.1145%2f3328243.3328255&partnerID=40&md5=31afc1c11d297ea2340b5e3c6766bdd3,"Picture-based authentication offers visual cues (e.g. images) that helps users recalling their passwords. Human facial images were utilized in making graphical passwords, as humans are better at remembering human faces than other types of images. Prior research found that users performed better with the partitioned facial images than with the full-face portrait images; nevertheless, age-related differences were not considered. This present research is a preliminary study to assess older adults’ understanding toward Passface and overall experimental procedure, by examining two levels of facial image presentation: full-face portrait image, and partitioned facial image. The results showed that the partitioned facial image aided memory and recall better than the full-face portrait image, which was reflected by registration time, authentication time and success rate. In general, majority of older adults were able to understand Passface and overall experimental procedure. © 2019 Association for Computing Machinery.",Authentication; Elderly; Graphical password; Older adults; Passface; Shoulder-surfing; Usable security,Human computer interaction; Elderly; Graphical password; Older adults; Passface; Shoulder surfing; Usable security; Authentication,Conference Paper,2-s2.0-85072195206,
lpp,The Influence of Icon Background Colors and Icon Symbols on Menu Item Selection for Smartphone,2019,NICS 2018 - Proceedings of 2018 5th NAFOSTED Conference on Information and Computer Science,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061905569&doi=10.1109%2fNICS.2018.8606854&partnerID=40&md5=f35b0fb4c886647d36dab2e321e6bf3d,"Pictorial-based designs are common for smartphone menu patterns; and the use of icons is prevalent. Icon symbols and icon background colors are two variables that contribute to icon usability. Using pictorial symbol has been a standard for icon design but using textual symbol has become more common [1]. Icon background colors also contribute to icon usability. Two common design decisions of background colors of a set of icons: multicolor, and one-color. Presently, no experimental research has been conducted to examine whether these the two variables influenced menu item selection on smartphone. This study investigated whether icon background colors and icon symbols affect menu selection time and satisfaction. A two-way ANOVA indicated that there were significant main effects of both icon background color and icon symbol, as well as an interaction effect of the two variables on menu selection time. © 2018 IEEE.",Background color; Icon; Menu; Mobile; Mobile HCI; Smartphone; Visual Search,Smartphones; Icon; Menu; Mobile; Mobile HCI; Visual search; Color,Conference Paper,2-s2.0-85061905569,
nth,The Influence of Icon Background Colors and Icon Symbols on Menu Item Selection for Smartphone,2019,NICS 2018 - Proceedings of 2018 5th NAFOSTED Conference on Information and Computer Science,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061905569&doi=10.1109%2fNICS.2018.8606854&partnerID=40&md5=f35b0fb4c886647d36dab2e321e6bf3d,"Pictorial-based designs are common for smartphone menu patterns; and the use of icons is prevalent. Icon symbols and icon background colors are two variables that contribute to icon usability. Using pictorial symbol has been a standard for icon design but using textual symbol has become more common [1]. Icon background colors also contribute to icon usability. Two common design decisions of background colors of a set of icons: multicolor, and one-color. Presently, no experimental research has been conducted to examine whether these the two variables influenced menu item selection on smartphone. This study investigated whether icon background colors and icon symbols affect menu selection time and satisfaction. A two-way ANOVA indicated that there were significant main effects of both icon background color and icon symbol, as well as an interaction effect of the two variables on menu selection time. © 2018 IEEE.",Background color; Icon; Menu; Mobile; Mobile HCI; Smartphone; Visual Search,Smartphones; Icon; Menu; Mobile; Mobile HCI; Visual search; Color,Conference Paper,2-s2.0-85061905569,
wdc,Applying formal logic validation to enhance natural language understanding,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066045737&doi=10.1145%2f3316615.3316688&partnerID=40&md5=cf7c0edda897c3583b3bec383d21d179,"Inconsistencies and ambiguities of annotation can cause vagueness in the results obtained by natural language understanding (NLU). The quality of the type systems used for annotation affects the quality of annotation. To achieve highly accepted sets of annotated documents, the Fleiss’ kappa score has been widely used to observe the level of agreement from annotated results, submitted by different human annotators. The challenge is that the kappa score cannot be used to validate the type systems nor to identify any incorrect annotations. Thus, we proposed an application of formal logic for validating type systems and annotations against expert rules. Experiments have been done by using four different type systems and annotation sets created by an expert and three novices. Our proposed formal logic model was used to validate the novice type systems and annotations against the expert rules. The results show that the technique could help identifying inconsistencies between expert and novice annotations, by using a model checker. The number of detected inconsistencies impacts the level of achieved F1 score. Thus, the proposed formal logic technique could be used to guide novice annotators to develop accepted type systems. This will help to enhance the performance of the generated machine learning models used by the NLU. © 2019 Association for Computing Machinery.",IBM watson; Inconsistency detection; Natural language understanding; Validation,Application programs; Learning systems; Model checking; Expert and novices; IBM watson; Inconsistency detection; Machine learning models; Model checker; Natural language understanding; Type systems; Validation; Computer circuits,Conference Paper,2-s2.0-85066045737,
pkw,Applying formal logic validation to enhance natural language understanding,2019,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066045737&doi=10.1145%2f3316615.3316688&partnerID=40&md5=cf7c0edda897c3583b3bec383d21d179,"Inconsistencies and ambiguities of annotation can cause vagueness in the results obtained by natural language understanding (NLU). The quality of the type systems used for annotation affects the quality of annotation. To achieve highly accepted sets of annotated documents, the Fleiss’ kappa score has been widely used to observe the level of agreement from annotated results, submitted by different human annotators. The challenge is that the kappa score cannot be used to validate the type systems nor to identify any incorrect annotations. Thus, we proposed an application of formal logic for validating type systems and annotations against expert rules. Experiments have been done by using four different type systems and annotation sets created by an expert and three novices. Our proposed formal logic model was used to validate the novice type systems and annotations against the expert rules. The results show that the technique could help identifying inconsistencies between expert and novice annotations, by using a model checker. The number of detected inconsistencies impacts the level of achieved F1 score. Thus, the proposed formal logic technique could be used to guide novice annotators to develop accepted type systems. This will help to enhance the performance of the generated machine learning models used by the NLU. © 2019 Association for Computing Machinery.",IBM watson; Inconsistency detection; Natural language understanding; Validation,Application programs; Learning systems; Model checking; Expert and novices; IBM watson; Inconsistency detection; Machine learning models; Model checker; Natural language understanding; Type systems; Validation; Computer circuits,Conference Paper,2-s2.0-85066045737,
pps,Preface,2019,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059058033&partnerID=40&md5=398671504b99915888ffbacfae0e24fc,,,,Editorial,2-s2.0-85059058033,
nth,The effect of icon size and grid size on smartphone menu selection,2018,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064838118&doi=10.1145%2f3301761.3301774&partnerID=40&md5=4d20184a1b7614fb318041d5c4b217d7,"Menu interface design is an important part for the smartphone application. It's the main screen that leads the user to the function of an application. For grid menu design, selecting the number of grid size and icon size is an important part. The previous research has shown that icon sizes affect search speed and accuracy of menu selection [3]. But the research was on a desktop computer, which was unlike mobile phones [2]. To create a guideline for developers who want to design applications which have to place icons on a grid menu and to study the effect of icon sizes and grid sizes on smartphone menu selections, this study was conducted by comparing two icon sizes: 48x48 pixels and 60 x 60 pixels arranged on three grid sizes: 2x3, 3x3, and 3x4. The studies conducted the menu selection time that the participant uses to find the target icon. Results showed that the icon size did not affect menu selection time. Participants select the menu which has grid size 3x4 faster than the other two sizes. There was a significant difference between grid size 2x3 and 3x4. © 2018 AssociationforComputingMachinery",Grid size; Icon; Icon size; Menu; Mobile; Smartphone,Electronic commerce; Personal computers; Pixels; Grid size; Icon; Icon sizes; Menu; Mobile; Smartphones,Conference Paper,2-s2.0-85064838118,
lpp,The effect of icon size and grid size on smartphone menu selection,2018,ACM International Conference Proceeding Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064838118&doi=10.1145%2f3301761.3301774&partnerID=40&md5=4d20184a1b7614fb318041d5c4b217d7,"Menu interface design is an important part for the smartphone application. It's the main screen that leads the user to the function of an application. For grid menu design, selecting the number of grid size and icon size is an important part. The previous research has shown that icon sizes affect search speed and accuracy of menu selection [3]. But the research was on a desktop computer, which was unlike mobile phones [2]. To create a guideline for developers who want to design applications which have to place icons on a grid menu and to study the effect of icon sizes and grid sizes on smartphone menu selections, this study was conducted by comparing two icon sizes: 48x48 pixels and 60 x 60 pixels arranged on three grid sizes: 2x3, 3x3, and 3x4. The studies conducted the menu selection time that the participant uses to find the target icon. Results showed that the icon size did not affect menu selection time. Participants select the menu which has grid size 3x4 faster than the other two sizes. There was a significant difference between grid size 2x3 and 3x4. © 2018 AssociationforComputingMachinery",Grid size; Icon; Icon size; Menu; Mobile; Smartphone,Electronic commerce; Personal computers; Pixels; Grid size; Icon; Icon sizes; Menu; Mobile; Smartphones,Conference Paper,2-s2.0-85064838118,
lpp,The Influence of Label Font Size on Menu Item Selection for Smartphone,2018,ICAICTA 2018 - 5th International Conference on Advanced Informatics: Concepts Theory and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059959275&doi=10.1109%2fICAICTA.2018.8541307&partnerID=40&md5=fa87b89c0b707b7226de36d611dd9db5,"Prior studies did not examine how menu efficiency was related to menu components: icons, menu patterns, and label. Moreover, the research has never investigated whether label font size has an influence on findability of menu item. This study examined whether label font sizes influenced menu item selection time, on different menu design variations. The ANOVA test indicated that there was a significant effect on menu selection time. The results did not suggest that the significance was from the influence of label font size. However, the study found that menu pattern and icon shape had stronger influence on menu selection time. © 2018 IEEE.",Icon; Label Font Size; Menu; Mobile; Mobile HCI; Smartphone,Font size; Icon; Menu; Mobile; Mobile HCI; Smartphones,Conference Paper,2-s2.0-85059959275,
nth,The Influence of Label Font Size on Menu Item Selection for Smartphone,2018,ICAICTA 2018 - 5th International Conference on Advanced Informatics: Concepts Theory and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059959275&doi=10.1109%2fICAICTA.2018.8541307&partnerID=40&md5=fa87b89c0b707b7226de36d611dd9db5,"Prior studies did not examine how menu efficiency was related to menu components: icons, menu patterns, and label. Moreover, the research has never investigated whether label font size has an influence on findability of menu item. This study examined whether label font sizes influenced menu item selection time, on different menu design variations. The ANOVA test indicated that there was a significant effect on menu selection time. The results did not suggest that the significance was from the influence of label font size. However, the study found that menu pattern and icon shape had stronger influence on menu selection time. © 2018 IEEE.",Icon; Label Font Size; Menu; Mobile; Mobile HCI; Smartphone,Font size; Icon; Menu; Mobile; Mobile HCI; Smartphones,Conference Paper,2-s2.0-85059959275,
ppr,A New Mobile Application to Reduce Anxiety in Pediatric Patients Before Bone Marrow Aspiration Procedures,2018,Hospital pediatrics,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054069636&doi=10.1542%2fhpeds.2018-0073&partnerID=40&md5=f0b70639252cafd2eb9fa198144a1083,"OBJECTIVES: Insufficient preparation for children who are undergoing bone marrow aspiration can cause anxiety and negative outcomes. Nonpharmacological therapies have been proven to reduce fear in children who are undergoing painful procedures. We have therefore developed a mobile application to help reduce these patients' anxiety by providing them with procedural information and coping skills.METHODS: This single-blinded, randomized controlled trial included 60 patients age 5 to 12 years old who were undergoing bone marrow aspiration procedures in Thailand that were conducted between May 2015 and May 2016. Sixty participants were randomly assigned to the intervention group (mobile application added to usual care) or the control group (usual care only). Preprocedural anxiety levels were evaluated by visual analog scales (child anxiety visual analog scale); this was repeated in the intervention group immediately after patients used the mobile application. On the day of the procedure, the patients' cooperation levels were assessed by using the modified Yale Preoperative Anxiety Scale. The total amount of sedative drugs that were used was also recorded. The paired t test and the Wilcoxon signed rank test were used to analyze within-person change, whereas the t test and the Wilcoxon rank sum test were used for group comparisons.RESULTS: The child anxiety visual analog scale score of patients in the intervention group decreased significantly after they used the mobile application (P < .0012). The modified Yale Preoperative Anxiety Scale score of patients in the intervention group was significantly lower than that in the control group (P < .01). There was no difference in sedative use between the 2 groups.CONCLUSIONS: This mobile application possibly had effectiveness in routine use for reducing anxiety and increasing patients' cooperation in bone marrow aspiration procedures. Copyright © 2018 by the American Academy of Pediatrics.",,,Article,2-s2.0-85054069636,
ksc,Efficient Pre-Copy Live Migration of Virtual Machines for High Performance Computing in Cloud Computing Environments,2018,"2018 3rd International Conference on Computer and Communication Systems, ICCCS 2018",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054800966&doi=10.1109%2fCCOMS.2018.8463286&partnerID=40&md5=9023a364bb06b42439f83e9269d77b64,"Pre-copy live migration is the most popular migration mechanism implemented in most hypervisors. However, it is not suitable for Virtual Machines (VMs) running computation-intensive and memory-intensive workloads because its operational behaviors depend on a configuration parameter, namely the maximum tolerable downtime. Although the default value of this parameter is good enough for normal web-based applications, it is too low for most HPC applications, which are computation-intensive and memoryintensive. Performing a migration with an inappropriate parameter may cause extensively long migration time or downtime. Defining this parameter is nontrivial since application workloads are generally unpredictable. This difficulty also makes it hard for cloud management systems to operate VM live migration automatically. In this paper, we propose the Memory-bound Pre-copy Live Migration (MPLM) mechanism that performs VM live migration without requiring the maximum tolerable downtime parameter. MPLM presents a new perspective of VM live migration, where the migration is performed based on the present state of VM computation without enforcing downtime constraints. During live migration, MPLM separates memory pages into dirty and non-dirty sets and transmits them in a multiplexing manner. MPLM has been implemented in a modified version of the QEMU-KVM software. Experiments have been conducted to evaluate MPLM performances against those of the traditional pre-copy mechanism. We performed a number of live migrations of VMs running four OpenMP NAS Parallel Benchmark programs. Experimental results show that MPLM is more efficient and easier to use than the pre-copy mechanism. © 2018 IEEE.",cloud computing; live migration; virtualization,Application programming interfaces (API); Cloud computing; Maintenance; Network security; Parameter estimation; Virtualization; Cloud computing environments; Computation intensives; Configuration parameters; High performance computing; Live migrations; Migration mechanisms; NAS parallel benchmarks; Web-based applications; Virtual machine,Conference Paper,2-s2.0-85054800966,
ssr,Safety Property Analysis of Service-Oriented IoT Based on Interval Timed Coloured Petri Nets,2018,"Proceeding of 2018 15th International Joint Conference on Computer Science and Software Engineering, JCSSE 2018",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057743652&doi=10.1109%2fJCSSE.2018.8457326&partnerID=40&md5=720faf07ea263dc0b56cb942fe739d38,"In recent years, the number of Internet of Things (IoT) systems has been increasing. Through design and analysis, IoT systems can be verified and monitored. However, it is difficult to find safety property with general-use models which we are familiar with such as UML model. In this paper, we proposed safety property analysis of service-oriented IoT based on Interval timed coloured Petri Nets (ITCPN). We model IoT design with StateMate which is easy to use and is similar to UML diagram. Then, transforming this diagram to ITCPN model which can be analysed and verified by model checking with Linear Temporal Logic (LTL). We also illustrated the usefulness of our approach with an example of infusion Pump. © 2018 IEEE.",IoT; ITCPN; LTL; Model checking; Petri Nets,Model checking; Petri nets; Safety engineering; Temporal logic; Unified Modeling Language; Design and analysis; Infusion pump; Internet of Things (IOT); ITCPN; Linear temporal logic; Safety property; Service Oriented; Timed coloured Petri nets; Internet of things,Conference Paper,2-s2.0-85057743652,
ksc,A Transparent Hypervisor-level Checkpoint-Restart Mechanism for a Cluster of Virtual Machines,2018,"Proceeding of 2018 15th International Joint Conference on Computer Science and Software Engineering, JCSSE 2018",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057742393&doi=10.1109%2fJCSSE.2018.8457176&partnerID=40&md5=3794161c24a04db83bc9060d5ba0e18b,"A cluster of virtual machines is a common platform for running MPI applications in cloud computing environments. However, most traditional methods to provide fault tolerance to these applications are not fully transparent and require specific, checkpointing-enabled MPI software. This paper presents a novel Transparent Hypervisor-level Checkpoint-Restart mechanism, namely the Virtual Cluster Checkpoint-Restart (VCCR), to perform checkpoint and restart operations at hypervisor-level. VCCR is highly transparent to MPI applications and guest OS. In VCCR, a software framework consisting of a controller and agent processes is created to perform checkpoint and restart operations for the entire cluster. The checkpoint and restart protocols of VCCR are designed based on the principles of barrier synchronization and virtual time to maintain global consistency and efficiency. We have developed a prototype of VCCR on top the QEMU-KVM software and conducted two preliminary experiments using NAS Parallel Benchmark. Experimental results confirm that VCCR can correctly and efficiently checkpoint and restart a cluster of virtual machines. © 2018 IEEE.",Checkpointing; Cluster computing; Virtualization,Application programs; Cloud computing; Cluster computing; Computer programming; Fault tolerance; Message passing; Network security; Software agents; Virtual machine; Virtualization; Barrier synchronization; Check pointing; Checkpoint-and-restart; Cloud computing environments; Global consistency; NAS parallel benchmarks; Restart mechanism; Software frameworks; Software prototyping,Conference Paper,2-s2.0-85057742393,
ppr,PRAGMA Cloud Scheduler: Improving Usability of the PRAGMA Cloud Testbed,2018,"ICSEC 2017 - 21st International Computer Science and Engineering Conference 2017, Proceeding",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053441106&doi=10.1109%2fICSEC.2017.8443838&partnerID=40&md5=a564a0afed3ddaa6dec07f3c9ace9e6b,"The Pacific Rim Application and Grid Middleware Assembly (PRAGMA) is a community of research scientists and institutions from around the Pacific Rim that collaborate together to enable scientific expeditions in areas such as biodiversity distribution and lake ecology. Previously, a web-based cloud scheduler reservation system was proposed to enable users to run application experiments using virtual clusters on a persistent distributed international infrastructure, called the PRAGMA Cloud Testbed. As the testbed has expanded to a broader group of users and more complex resources, the existing PRAGMA cloud scheduler became not as user-friendly as it was with the initial simple usage scenario, and insufficient for the administrators to handle anticipated more complicated use cases. This paper presents a redesign of the user interface component of this PRAGMA cloud scheduler and its additional features in order to improve the usability of the testbed for a broad range of users so that they can easily create and manage virtual clusters and run experiments for their long tail of science. This scheduler has a client-server architecture that requires minimal installation and management effort for participating testbed sites. We extended the existing PRAGMA technologies that the scheduler was built upon to facilitate automatic access to testbed resources for heterogeneous sites and applications. These extensions include unified management of virtual cluster images and expanded virtual cluster porting to a variety of cloud hosting environments. The new user interface was designed and built using node.js, React, SASS and Google Maps API and is based on principles of human computer interaction and the SOLID principles of object-oriented design. As a result, we have the new PRAGMA cloud scheduler that is more user-friendly, understandable, flexible and maintainable. © 2017 IEEE.",Cloud; Resource Sharing; Scheduling; Software Design; Usability; Virtualization,Application programming interfaces (API); Biodiversity; Client server computer systems; Clouds; Human computer interaction; Middleware; Object oriented programming; Reservation systems; Scheduling; Software design; Systems analysis; Testbeds; Virtualization; Client-server architectures; Management efforts; Object oriented design; Resource sharing; Usability; Usage scenarios; User interface components; Virtual clusters; User interfaces,Conference Paper,2-s2.0-85053441106,
wdp,Modeling and forecasting of end-to-end available bandwidth in wide area networks,2018,"2018 22nd International Computer Science and Engineering Conference, ICSEC 2018",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066468372&doi=10.1109%2fICSEC.2018.8712685&partnerID=40&md5=5a39fb8e8304fa86d20253ddc57b7bf8,"This paper studied modeling and forecasting end-to-end available bandwidth in Wide Area Networks using time series models. We used the ARIMA(p,d,q) and SARIMA (p,d,q)(P,D,Q)[s] models to model available bandwidth data sets published by Stanford Linear Accelerator Center (SLAC) in 2009. We found that both models can be used to model available bandwidth along an end-to-end path. In addition, our results indicated that SARIMA outperforms ARIMA in forecasting future available bandwidth. © 2018 IEEE.",ARIMA model; Available bandwidth; Forecasting; SARIMA model; Time series; Wide networks,Forecasting; Time series; Wide area networks; ARIMA modeling; Available bandwidth; End to end; End-to-end path; Modeling and forecasting; Sarima models; Stanford Linear Accelerator Center; Time series models; Bandwidth,Conference Paper,2-s2.0-85066468372,
pps,Exploring Efficiency of Data Mining Techniques for Missing Link in Online Social Network,2018,"2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing, iSAI-NLP 2018 - Proceedings",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065066845&doi=10.1109%2fiSAI-NLP.2018.8692951&partnerID=40&md5=0c8bafd4758308ec85bbddd2dfabcda9,"Missing link in Online Social Network (OSN) is an interesting problem for capturing missing relation and understanding user's behavior. The existing work introduced social features for training predictive models, but they used only SVM prediction technique for solving the problem. However, we suspect that other prediction techniques may give better performance. This study investigates prediction performances of SVM, k-NN, Decision Tree, Neural Networks, Naïve Bayes, Logistic Regression and Random Forest using two OSN datasets (high-density and low density). We realize that the Random Forest technique has the best performance with F1-measure score. Moreover, this technique is most robust technique for the both datasets. © 2018 IEEE.",Link prediction; Missing link; Online social network; Prediction technique,Artificial intelligence; Barium compounds; Behavioral research; Data mining; Decision trees; Forecasting; Natural language processing systems; Nearest neighbor search; Link prediction; Logistic regressions; Missing link; On-line social networks; Online social networks (OSN); Prediction performance; Prediction techniques; Predictive models; Social networking (online),Conference Paper,2-s2.0-85065066845,
pps,Activity Recognition using Kinect and Comparison of Supervised Learning Models for Activity Classification,2018,"2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing, iSAI-NLP 2018 - Proceedings",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065054658&doi=10.1109%2fiSAI-NLP.2018.8692801&partnerID=40&md5=82d94e81af7bbeeb7b91d9fbdf9ef53a,"This work presents a method to develop activity recognition using Kinect as a motion-sensing device and supervised learning for classification. Data from Kinect are continuous and independent frame representing in three dimensional axes from 20 human joints. The data then are trained for classify activities using supervised learning algorithms. The activities are 10 basic motion-gestures such as standing, waving, Thai-style greeting and walking. To compare supervised learning for classification in the task, four algorithms including neural networks, naive bayes, decision tree and support vector machine are applied to generate classification models. From experiment results, the best overall classification model was from neural network algorithm at about 75% accuracy while the second best was support vector machine with slightly lower accuracy. From analysis, the most incorrect activities were 'wai' (Thai greeting) and 'walking' in which were often misinterpreted to their similar activities as 'bowing' and running', respectively. © 2018 IEEE.",Activity Recognition; Classification; Data mining; Kinect; Motion Detection,Classification (of information); Data mining; Decision trees; Machine learning; Natural language processing systems; Pattern recognition; Supervised learning; Support vector machines; Activity classifications; Activity recognition; Classification models; Kinect; Motion detection; Motion gestures; Motion sensing; Neural network algorithm; Learning algorithms,Conference Paper,2-s2.0-85065054658,
ssr,Refactoring opportunity identification methodology for removing long method smells and improving code analyzability,2018,IEICE Transactions on Information and Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049395535&doi=10.1587%2ftransinf.2017KBP0026&partnerID=40&md5=fc0a4d2c34cb1d45a6a232c1dea550e6,"An important step for improving software analyzability is applying refactorings during the maintenance phase to remove bad smells, especially the long method bad smell. Long method bad smell occurs most frequently and is a root cause of other bad smells. However, no research has proposed an approach to repeating refactoring identification, suggestion, and application until all long method bad smells have been removed completely without reducing software analyzability. This paper proposes an effective approach to identifying refactoring opportunities and suggesting an effective refactoring set for complete removal of long method bad smell without reducing code analyzability. This approach, called the long method remover or LMR, uses refactoring enabling conditions based on program analysis and code metrics to identify four refactoring techniques and uses a technique embedded in JDeodorant to identify extract method. For effective refactoring set suggestion, LMR uses two criteria: code analyzability level and the number of statements impacted by the refactorings. LMR also uses side effect analysis to ensure behavior preservation. To evaluate LMR, we apply it to the core package of a real world Java application. Our evaluation criteria are 1) the preservation of code functionality, 2) the removal rate of long method characteristics, and 3) the improvement on analyzability. The result showed that the methods that apply suggested refactoring sets can completely remove long method bad smell, still have behavior preservation, and have not decreased analyzability. It is concluded that LMR meets the objectives in almost all classes. We also discussed the issues we found during evaluation as lesson learned. © Copyright 2018 The Institute of Electronics Information and Communication Engineers.",Code analyzability; Long method bad smell; Refactoring; Software engineering; Software maintenance,Codes (symbols); Computer software maintenance; Odors; Software engineering; Bad smells; Code analyzability; Effective approaches; Evaluation criteria; Java applications; Opportunity identifications; Refactorings; Side-effect analysis; Application programs,Conference Paper,2-s2.0-85049395535,
pkl,Ensemble features selection algorithm by considering features ranking priority,2018,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022225003&doi=10.1007%2f978-3-319-60663-7_5&partnerID=40&md5=3a86adaed6870f950cfd9e54c83b5e57,"Feature selection is a pre-processing for choosing relevant features and ignores features that tend to have no predictive information. Feature selection is applied to improve the accuracy of classification process. High relevant features have a tendency to get high classification performance. This paper proposed the ensemble of multiple feature ranking techniques by considering ranker priority for feature selection. Five individual feature ranking algorithms (information gain, gain ratio, symmetrical uncertainty, reliefF and oneR) are investigated and considered together as ensemble, based on ranking priority. The lung cancer, lymphoma, breast cancer, ovarian cancer and leukemia datasets were gathered from Kent Ridge bio-medical data and Machine Learning data repository. The datasets are applied to ensemble features selection algorithm. The obtained results are compared to results from individual feature ranking algorithms and the existing ensemble algorithm. The selected features are applied to classification algorithms. Area under the curve (AUC), precision and recall values from six classification algorithms are used to evaluate the obtained features. The experimental results show that the selected features from proposed ensemble features selection algorithm are greater than those of individual feature ranking techniques and the existing ensemble features selection algorithm. © Springer International Publishing AG 2018.",Ensemble; Feature selection; Ranker,Diseases; Feature extraction; Accuracy of classifications; Area under the curves; Classification algorithm; Classification performance; Ensemble; Precision and recall; Predictive information; Ranker; Classification (of information),Conference Paper,2-s2.0-85022225003,
pps,Comparison of document clustering methods based on bees algorithm and firefly algorithm using Thai documents,2018,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044431032&doi=10.1007%2f978-3-319-70016-8_8&partnerID=40&md5=244fed9fc1eab9d834cd3ce1ad641b5c,"Several researches performed experiments to compare performances of data clustering methods based on nature-inspired algorithms using various data types including text document dataset. According to the results, although Firefly algorithm showed high performance to cluster text document over several nature-inspired algorithms but Bees algorithm had better performance to solve complex problems than several nature-inspired algorithms. However, none of the experiments compared the performances of the both algorithms to cluster text documents. Therefore, we compare the document clustering performances of the clustering methods based on Firefly algorithm and Bees algorithm. © Springer International Publishing AG 2018.",Bees algorithm; Document clustering; Firefly algorithm; k-means,Bioluminescence; Cluster analysis; Information retrieval; Natural language processing systems; Optimization; Bees algorithms; Clustering methods; Complex problems; Data clustering methods; Document Clustering; Firefly algorithms; K-means; Nature inspired algorithms; Clustering algorithms,Conference Paper,2-s2.0-85044431032,
pps,An improved short pause based voice activity detection using long short-term memory recurrent neural network,2018,Communications in Computer and Information Science,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079741576&doi=10.1007%2f978-981-13-3149-7_20&partnerID=40&md5=d42f7411c789f20684ff6aa76f781b8b,"Generally, voice activity detection (VAD) commonly uses a silence over 100-ms as an endpoint of speech. Previously, the short pause based VAD is proposed to reduce the waiting time of caption result in automatic captioned relay service. This technique reduces the waiting time of caption result well. However, an accuracy of caption result is not maintained as it should be. The problem inherits to short-time energy feature which difficult and inaccurate to search the smallest characteristic like short pause or unvoiced sounds. Therefore, we propose the new technique that combines a Mel Frequency Cepstral Coefficient and Long Short-term Memory Recurrent Neural Network. This technique is called a pause classifier, which is able to capture the smallest characteristic like the short pause or unvoiced sounds. The experimental result shows an effective to reduce the waiting time while maintaining WER of caption result. The average waiting time reduced, the automatic speech recognition results are more continuous and constant. This will directly affect the user experience in automatic captioned relay service. © Springer Nature Singapore Pte Ltd.",,Brain; Long short-term memory; Relay control systems; User experience; Automatic speech recognition; Average waiting-time; Mel frequency cepstral co-efficient; Relay services; Short-time energy; Voice activity detection; Waiting-time; Speech recognition,Conference Paper,2-s2.0-85079741576,
nth,Semi-automatic framework for generating RDF dataset from open data,2018,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044411338&doi=10.1007%2f978-3-319-70016-8_1&partnerID=40&md5=c87568d1e9a8a6aa895b4d0e353dbb35,"Most of datasets in open data portals are mainly in tabular format in spreadsheet, e.g. CSV and XLS. To increase the value and reusability of these datasets, the datasets should be made available in RDF format that can support better data querying and data integration. In this paper, we present a semi-automatic framework for generating and publishing RDF dataset from existing datasets in tabular format. This framework provides automatic schema detection functions, i.e. data type and nominal type detection. In addition, user can create RDF dataset from existing dataset step-by-step without required knowledge about RDF and OWL. Evaluation of the schema detection using some datasets from data.go.th shows that the technique can achieve high precision in most datasets and high recall for the datasets with small data input errors. © Springer International Publishing AG 2018.",Automatic schema detection; Open data publishing process; RDF dataset publishing,Natural language processing systems; Reusability; Semantic Web; Data querying; Data type; Detection functions; High-precision; Open datum; Semi-automatics; Small data; Data integration,Conference Paper,2-s2.0-85044411338,
pps,Automatic ontology development from semi-structured data in web-portal: Towards ontology of thai rice knowledge,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057274081&doi=10.1007%2f978-3-030-04284-4_18&partnerID=40&md5=d3966294ee3656bd5278eeb15562722b,"Heavyweight ontology is difficult to develop even for experienced ontology engineer, but it is required for semantic based computer software as core knowledge. Most of existing automated ontology development methods however focuses on lightweight ontology, taxonomy-instance extraction. This work presents a method to automatically construct relation-heavy ontology from semi-structured web content providing deep knowledge in specific domain. Classes, instances and hierarchical relation are derived from the category content from the web. Relations are extracted based on frequent expression details. Templates of relation and its range are extracted from common content with partial difference. Similar contexts are grouped with similarity and form as relation to attach to ontology classes. The case study of this work is Thai rice knowledge including rice variety, disease, weed and pest provided in website from responsible government. The complete ontology is used as core knowledge for personalised web service. The service assists in filter content in summary that matched to users’ information. Courtesy to the generated relation-heavy ontology, it is able to recommend relevant chained concepts to users based on semantic relation. From evaluation from an expert, the generated ontology obtained about 97% accuracy. © Springer Nature Switzerland AG 2018.",Knowledge extraction; Ontology learning; Pattern-based detection; Semi-structured content; Textual template,Extraction; Matched filters; Ontology; Pattern recognition; Semantic Web; Semantics; Web services; Knowledge extraction; Ontology learning; Pattern-based; Semi-structured; Textual template; Portals,Conference Paper,2-s2.0-85057274081,
tnt,Evaluation of demand-side management application based on human activity schedules,2017,"2017 IEEE 6th Global Conference on Consumer Electronics, GCCE 2017",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045760126&doi=10.1109%2fGCCE.2017.8229418&partnerID=40&md5=1086a19113065d2634359f9221cba10e,"Demand-side management (DSM) is one of the main applications for a community energy management in the smart grid. It can control and manage energy on the demand side, e.g., electricity consumption. Various DSM scheduling algorithms have been developed and have reported good evaluation results using simulation. However, in order to develop a practical DSM application, the effects of human activity scheduling must be further investigated. This paper demonstrates evaluating a DSM application which takes into account human activity schedule information. First, schedules regarding home appliance usage are extracted from daily activity schedule information. Then, based on the obtained schedules, user preferences of appliance usage time intervals and a community electricity consumption schedule are computed. Finally, after applying user preferences and the consumption schedule, optimal appliance schedules are determined. Evaluation results show the reduction of energy costs, which translates into savings on the user electricity bills. © 2017 IEEE.",,Demand side management; Domestic appliances; Electric power utilization; Electric utilities; Energy management; Scheduling algorithms; Daily activity; Demand Side Management (DSM); Electricity bill; Electricity-consumption; Energy cost; Evaluation results; Human activities; Time interval; Smart power grids,Conference Paper,2-s2.0-85045760126,
tnt,Towards a home environment testbed: Experiments with human body simulators and a real house,2017,"2017 IEEE 6th Global Conference on Consumer Electronics, GCCE 2017",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045745495&doi=10.1109%2fGCCE.2017.8229249&partnerID=40&md5=a988ce33bcd7a11467b181cf207d5222,"This paper illustrates a design and implementation of a home environment testbed. The proposed testbed aims to support experiments on real home environments with human subjects. However, conducting experiments with real human subjects has several limitations regarding costs, time, and user privacy. In order to avoid such issues, human body simulators (HBSims), which are designed to support 24-hour in-house repeatable experiments, are employed. Using HBSims, the patterns of human humidity and temperature are generated. Based on human activity schedules together with home appliance usage time obtained from one family, HBSims were employed in experiments in a real house. The evaluation results demonstrate that (1) our proposed testbed can potentially be used for real home environment experiments and (2) HBSims can be used instead of real human subjects for studying the effects of a human body in real home environments. © 2017 IEEE.",home environments; home services; human body simulators; smart home,Automation; Domestic appliances; Intelligent buildings; Simulators; Design and implementations; Evaluation results; Home environment; Home services; Human activities; Human bodies; Humidity and temperatures; Smart homes; Testbeds,Conference Paper,2-s2.0-85045745495,
ddp,Formal verification of ABAP by Z specification,2017,"Proceedings of the 2017 14th International Joint Conference on Computer Science and Software Engineering, JCSSE 2017",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031753140&doi=10.1109%2fJCSSE.2017.8025943&partnerID=40&md5=c2affdf0ef08c77d73fa800a75293989,"This paper has proposed a formal verification of ABAP by Z specification. An ABAP programming language is used to create a customized program in SAP ERP. The program must satisfy a business requirement. It likely has a defect from the developed program. Since a specification is created as the business requirement and a program should have functioned as in the specification, the formal verification is needed to assure the correctness of the function in the program. Both an ABAP program and its specification are translated into Z specification and they are verified by Isabelle in order to ensure that the ABAP program conforms to its specification. We also give some experimental result to show the effectiveness of our method. © 2017 IEEE.",ABAP; Formal Verification; HOL; Isabelle; SAP ERP; Z Specification,Enterprise resource planning; Software engineering; Specifications; ABAP; Business requirement; Isabelle; Z specifications; Formal verification,Conference Paper,2-s2.0-85031753140,
ksc,Performance comparisons and data compression of time-bound live migration and pre-copy live migration of virtual machines,2017,"Proceedings - 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2017",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030853010&doi=10.1109%2fSNPD.2017.8022747&partnerID=40&md5=e5e4ece3f85ebd6f1711a09608305d62,"Pre-copy live migration is a popular virtual machine (VM) live migration mechanism that has been used in most hypervisors. In this mechanism, a key parameter, namely the maximum tolerable downtime, is required for each migration. However, defining an appropriate value for this parameter is nontrivial, especially for the migration of a virtual machine running CPU-intensive and memory-intensive applications. If an inappropriate value is used, the migration may suffer poor performance. Time-bound Live Migration (TLM) is another live migration mechanism that was proposed to solve this problem. The TLM can operate automatically without the need for the maximum tolerable downtime. Thus, the operational burden and performance penalty due to the configuration of the maximum tolerable downtime are eliminated. The contributions of this study are two folds. First, a novel compression framework for TLM, namely TLMZ, is proposed. Second, the live migration performances of TLM and TLMZ are evaluated against those of the pre-copy mechanism. In our comparisons, the pre-copy mechanism was configured to operate using a range of maximum tolerable downtime values and a number of compression options. An extensive number of experiments have been conducted. In our experiments, the TLM, TLMZ, and pre-copy mechanisms are used to migrate VMs running four OpenMP NAS parallel benchmarks. Experimental results show that TLM and TLMZ are practical solutions for the migrations of VMs running CPU-intensive and memory-intensive applications. © 2017 IEEE.",Compression; Live migration; Virtualization,Application programming interfaces (API); Artificial intelligence; Compaction; Data compression; Maintenance; Network security; Software engineering; Virtualization; CPU-intensive; Hypervisors; Live migrations; NAS parallel benchmarks; Performance comparison; Performance penalties; Poor performance; Practical solutions; Virtual machine,Conference Paper,2-s2.0-85030853010,
pps,Implement of salary prediction system to improve student motivation using data mining technique,2017,"Proceedings - 11th 2016 International Conference on Knowledge, Information and Creativity Support Systems, KICSS 2016",5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025472111&doi=10.1109%2fKICSS.2016.7951419&partnerID=40&md5=4615fa1634eee6d81dd6d43cee860d39,"This paper presents a salary prediction system using a profile of graduated students as a model. A data mining technique is applied to generate a model to predict a salary for individual students who have similar attributes to the training data. In this work, we also made an experiment to compare five data mining techniques including Decision trees, Naive Bayes, K-Nearest neighbor, Support vector machines, and Neural networks to find the suitable technique to the salary prediction. In the experiment, 13,541 records of graduated student data were used with 10-fold cross validation method. Results showed that K-Nearest neighbor provided the best efficiency to be used as a model for salary prediction. For usage evaluation, a questionnaire survey was conducted with 50 user samplings and a result showed that the system was effective in boosting students' motivation for studying and also gave them a positive future viewpoint. The result also informed that they found they satisfied with the implemented system since the system was easy to use, and the prediction results were simple to understand without requiring any background knowledge. © 2016 IEEE.",Classification technique; Decision trees; Educational data mining; K-Nearest neighbor; Motivation; Naive Bayes; Neural networks; Salary prediction system; Support vector machines,Classifiers; Compensation (personnel); Decision trees; Education; Forecasting; Forestry; Motivation; Nearest neighbor search; Neural networks; Students; Support vector machines; Surveys; Trees (mathematics); Wages; Classification technique; Educational data mining; K-nearest neighbors; Naive bayes; Prediction systems; Data mining,Conference Paper,2-s2.0-85025472111,
pps,Random Forest for Salary Prediction System to Improve Students' Motivation,2017,"Proceedings - 12th International Conference on Signal Image Technology and Internet-Based Systems, SITIS 2016",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019262168&doi=10.1109%2fSITIS.2016.106&partnerID=40&md5=0b33ff9dbad33496eae6b97a2938446f,"A salary prediction model was generated for graduate students using a data mining technique to generate for individuals with similar training attributes. An experiment was also conducted to compare the two data mining techniques Decision Trees ID3, C4.5 and Random Forest to determine the most suitable technique for salary prediction, tuned with key important parameters to improve the accuracy of the results. Random Forest gave the best accuracy at 90.50%, while Decision Trees ID3 and C4.5 returned lower accuracies at 61.37% and 73.96%, respectively for 13,541 records of graduate students using a 10-fold cross-validation method. Random Forest generated the best efficiency model for salary prediction. A questionnaire survey was conducted to determine usage evaluation with 50 samples. Results indicated that the system was effective in boosting students' motivation for studying, and also gave them a positive future viewpoint. The results also suggested that the students were satisfied with the implemented system since it was easy to use, and the prediction results were simple to understand without any previous background statistical knowledge. © 2016 IEEE.",Classification technique; Decision trees; Educational data mining; Motivation; Random Forest; Salary prediction system,Compensation (personnel); Data mining; Decision trees; Education; Forestry; Motivation; Students; Surveys; Wages; 10-fold cross-validation; Classification technique; Educational data mining; Graduate students; Prediction systems; Questionnaire surveys; Random forests; Statistical knowledge; Forecasting,Conference Paper,2-s2.0-85019262168,
pps,Automated Test Assembly with Minimum Redundant Questions Based on Bee Algorithm,2017,"Proceedings - 12th International Conference on Signal Image Technology and Internet-Based Systems, SITIS 2016",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019196456&doi=10.1109%2fSITIS.2016.108&partnerID=40&md5=bdfdea41017070b2b23d97793976e0e6,"An ideal test form should contain questions with different level of difficulties and non-redundant questions. This paper proposed an automated test assembly algorithm to minimize the redundant question in a test form based on Bee algorithm. A neighborhood search in Bee algorithm is improved by using a new technique, called Min-SumDistance (MSD). The MSD is the distance of considered question compared to others in the test form. The sum of question pairs distance indicates to the redundant question in the test form. A question content is represented in two forms as an unigram with TF and TF-IDF scores. The experiments using 200 questions from Information Technology Professional Examination(ITPE). To evaluate the performance of MSD method, we count a number of enemy pairs of the test form and compared to the random method. The experimental results show that our proposed algorithm yields the significant numbers of redundant questions. © 2016 IEEE.",Automated Test Assembly; Bee Algorithm; Neighborhood Search; TF-IDF,Automation; Optimization; Automated test; Bee Algorithm; Information technology professionals; Level of difficulties; Neighborhood search; Non-redundant; Random methods; TF-IDF; Testing,Conference Paper,2-s2.0-85019196456,
pps,Reducing waiting time in automatic captioned relay service using short pause in voice activity detection,2017,"2017 9th International Conference on Knowledge and Smart Technology: Crunching Information of Everything, KST 2017",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017518452&doi=10.1109%2fKST.2017.7886109&partnerID=40&md5=35d4b3c4f51e85d9af6766384d1e8e49,"The Automatic Captioned Relay Service is crucial for hearing disabilities or hard-of-hearing to communicate with others in real life. This service uses an Automatic Speech Recognition (ASR) to transcribe speech to a caption. If can reduce waiting time from non-streaming speech recognition, the relay service will support more users. In this paper, we proposed a method for improving a voice activity detection (VAD) using a short pause as endpoint based on dual-threshold method. This method reduces waiting time of the captions results. The experimental results show that the propose method reduces 19.10% of waiting time without losing the accuracy of captions when compare with traditional VAD. © 2017 IEEE.",Automatic captioned relay service; Dual-threshold method; Short pause; Voice activity detection,Audition; Deep neural networks; Relay control systems; Automatic speech recognition; Hard of hearings; Hearing disabilities; Relay services; Short pause; Threshold methods; Voice activity detection; Waiting-time; Speech recognition,Conference Paper,2-s2.0-85017518452,
wdp,Fine-Grained bandwidth allocation in software-defined networks,2017,"20th International Computer Science and Engineering Conference: Smart Ubiquitos Computing and Knowledge, ICSEC 2016",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016230654&doi=10.1109%2fICSEC.2016.7859905&partnerID=40&md5=ce9bbdc1c7cbac05c832eebe3ab1f785,"We study the prototype for fine-grained bandwidth allocating managers (FGBAM) which allot communication bandwidth inside heterogeneous software-defined networks (SDN) based on user-defined policies. FGBAM can be beneficial in enabling Quality-of-Service (QoS) during both normal periods and disaster recovery time. FGBAM consists of two parts: The web-based front end and the back end for enforcing bandwidth allocation policies. The web-based front end facilitates network administrators in specifying bandwidth allocation policies for packet flows of interest. The back end of FGBAM enforces bandwidth allocation policies by acting as a proxy between the network devices (i.e. switches and routers) and SDN controllers. Our bandwidth manager has four desirable properties. Firstly, FGBAM supports fine-grained bandwidth allocation policies. That is, it allows allotting communication bandwidth of one port among many packet flows. Secondly, using the proxy manner, FGBAM is transparent to both SDN controllers and network devices in a software-defined network. It requires no changes in configurations of both SDN controllers and network devices. Thirdly, FGBAM can cooperate with market-leading SDN controllers of many kinds, Beacon, Floodlight, NOX, Trema, just to name a few. Last but not least, FGBAM can enable QoS in heterogeneous environment in which an SDN composes of network devices running different releases of the OpenFlow protocol. Being able to provide QoS in heterogeneous SDNs is crucial while vendors and network administrators are slowly catching up with the evolutionary of the protocol. We evaluated the performance of FGBAM in a simulated environment. The results showed that FGBAM can allocate fine-grained communication bandwidth for packet flows of interest. In addition, the results were achieved without any change applied to configurations of network devices and SDN controllers. © 2016 IEEE.",Bandwidth allocation; Component; Openflow; Quality-of-service (QoS); Software-defined networks (SDN),Bandwidth; Computer software; Controllers; Electric lighting; Frequency allocation; Internet protocols; Managers; Software defined networking; Web crawler; Websites; Bandwidth allocation policies; Communication bandwidth; Component; Heterogeneous environments; Heterogeneous software; Network administrator; Openflow; User-defined policies; Quality of service,Conference Paper,2-s2.0-85016230654,
lpp,Usability Studies on Mobile User Interface Design Patterns: A Systematic Literature Review,2017,Advances in Human-Computer Interaction,31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042064383&doi=10.1155%2f2017%2f6787504&partnerID=40&md5=420d0d0bf5b74349e6b402ff5b0f31bf,"Mobile platforms have called for attention from HCI practitioners, and, ever since 2007, touchscreens have completely changed mobile user interface and interaction design. Some notable differences between mobile devices and desktops include the lack of tactile feedback, ubiquity, limited screen size, small virtual keys, and high demand of visual attention. These differences have caused unprecedented challenges to users. Most of the mobile user interface designs are based on desktop paradigm, but the desktop designs do not fully fit the mobile context. Although mobile devices are becoming an indispensable part of daily lives, true standards for mobile UI design patterns do not exist. This article provides a systematic literature review of the existing studies on mobile UI design patterns. The first objective is to give an overview of recent studies on the mobile designs. The second objective is to provide an analysis on what topics or areas have insufficient information and what factors are concentrated upon. This article will benefit the HCI community in seeing an overview of present works, to shape the future research directions. © 2017 Lumpapun Punchoojit and Nuttanont Hongwarittorrn.",,,Review,2-s2.0-85042064383,
nth,Usability Studies on Mobile User Interface Design Patterns: A Systematic Literature Review,2017,Advances in Human-Computer Interaction,31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042064383&doi=10.1155%2f2017%2f6787504&partnerID=40&md5=420d0d0bf5b74349e6b402ff5b0f31bf,"Mobile platforms have called for attention from HCI practitioners, and, ever since 2007, touchscreens have completely changed mobile user interface and interaction design. Some notable differences between mobile devices and desktops include the lack of tactile feedback, ubiquity, limited screen size, small virtual keys, and high demand of visual attention. These differences have caused unprecedented challenges to users. Most of the mobile user interface designs are based on desktop paradigm, but the desktop designs do not fully fit the mobile context. Although mobile devices are becoming an indispensable part of daily lives, true standards for mobile UI design patterns do not exist. This article provides a systematic literature review of the existing studies on mobile UI design patterns. The first objective is to give an overview of recent studies on the mobile designs. The second objective is to provide an analysis on what topics or areas have insufficient information and what factors are concentrated upon. This article will benefit the HCI community in seeing an overview of present works, to shape the future research directions. © 2017 Lumpapun Punchoojit and Nuttanont Hongwarittorrn.",,,Review,2-s2.0-85042064383,
wdc,Usability evaluation of a raspberry-Pi telepresence robot controlled by android smartphones,2017,Advances in Intelligent Systems and Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992609074&doi=10.1007%2f978-3-319-42975-5_18&partnerID=40&md5=f7869ffc99a8eebabb803e0c95a7c609,"Telepresence robots in static pan-tilt form can be a viable and affordable choice for tele-education. However, cost considerations may impose limitations on usability and expected performance. The goal of this study was to explore the usability of a low-cost, static pan-tilt telepresence robot operated using an Android smartphone. Experiments were carried out with 26 participants from two age groups (14 students M = 20 years, 12 staff M = 25 years) in a laboratory. Each participant interacted with the robot to perform two tasks. The opinions of the participants preand post-experiments, and the time they took to complete the tasks, were recorded. The results show that the average latency (of 3.1 ± 0.8 s for one robot movement) is quite acceptable. The students were faster than the staff when controlling the robot remotely but slower when working at the robot site. Correlation analysis shows that confidence in the robot and the likelihood of adoption is strongly related to data privacy features. All the methods used to control the robot remotely show positive interaction to each other. This implies that the majority of participants were focussed on the control methods and data privacy provided in the robot platform, and were willing to accept a small delay in robot movement. © Springer International Publishing Switzerland 2017.",Higher education; Performance evaluation; Raspberry pi; Telepresence robot; Usability testing,Android (operating system); Data privacy; Education; Robotics; Smartphones; Students; Usability engineering; Visual communication; Android smartphone; Correlation analysis; Higher education; Performance evaluation; Positive interaction; Telepresence robots; Usability evaluation; Usability testing; Educational robots,Conference Paper,2-s2.0-84992609074,
ppr,EnGeno: Towards enabling a medical genogram library for supporting home-visit patient diagnosis,2016,"2016 IEEE 5th Global Conference on Consumer Electronics, GCCE 2016",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010282162&doi=10.1109%2fGCCE.2016.7800549&partnerID=40&md5=ed9e1a4e88da445887c83578f2103294,"A genogram refers to a family tree used for medical purposes. It is a useful diagnosis tool for physicians to understand patients' family information. There are many programs that attempt to support genogram creation, but each has its own limitation. For examples, some programs require users to pay usage fee and to install them in computers in order to view and edit genograms. While free Web-based genogram tools offer more economical alternatives to the formers, their users usually cannot save genograms for later edition. Moreover, it is not possible to integrate and reuse functionalities from these existing programs in users' own systems. Therefore, we propose a JavaScript library, called 'enGeno', to address the above issues. From our performance evaluation, the enGeno library can efficiently provide physicians capabilities to store and retrieve information about their patients. © 2016 IEEE.",diagnosis tool; Genogram; JavaScript library; medical diagram,Computer software reusability; High level languages; Java programming language; Diagnosis tools; Family tree; Genogram; Javascript; medical diagram; Patient diagnosis; Web based; Diagnosis,Conference Paper,2-s2.0-85010282162,
yao,Design and implementation of a decentralized message bus for microservices,2016,"2016 13th International Joint Conference on Computer Science and Software Engineering, JCSSE 2016",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942636&doi=10.1109%2fJCSSE.2016.7748869&partnerID=40&md5=75212a349becd229bc14e866af57582b,"A new software architecture, known as microservices, becomes rapidly popular recently. Microservices could help developers cope well with the problems of software complexity and demands on an adaptive development process that needs to respond to changes quickly. In this architecture, a single monolithic large application would be divided into small multiple isolated services. They are separately deployed and communicated to other services via remote calls. This architectural style allows any changes on one service not affecting the others. However, if services directly make remote calls, it would create interdependencies and tight couplings between them. To remove such problem, this paper proposes a decentralized message bus to use as a communication tool between services. Our message bus provides a framework for services to collaborate. It divides into four main components, public API, message bus, messaging and service discovery. The API uses the HTTP and RESTful style of communication. We use decentralized service discovery to avoid a single point of failure of the system. The messaging uses a simple TCP connection with only a header and body in its message. We also define three necessary communication messages for the services, viz. request/response, notification and publish/subscribe. The proposed framework is implemented and tested with a real-world scenario. It works correctly without any problem. Also, to realize how it could be scaled, we run the system continuously with incremental services and traffics. From the observation on the resource consumption of CPU, memory and network I/O, we found that the network consumption grows linearly while the CPU and memory usages have little change in consumption. © 2016 IEEE.",message bus; microservices; software architecture,Buses; Complex networks; Memory architecture; Network architecture; Software architecture; Software engineering; Architectural style; Communication messages; Design and implementations; Message bus; microservices; Real-world scenario; Resource consumption; Software complexity; Computer software,Conference Paper,2-s2.0-85006942636,
ddp,Formal verification of concurrency in go,2016,"2016 13th International Joint Conference on Computer Science and Software Engineering, JCSSE 2016",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006869236&doi=10.1109%2fJCSSE.2016.7748882&partnerID=40&md5=df91b215439460ffb79ad9417c08c9fa,"Go is a programming language which is mainly designed for the concurrent programming. It supports concurrency derived from Hoare's Communicating Sequential Processes (CSP). Since a go-routine introduced by Go allows us to execute program simultaneously, a program behavior is asynchronous. It likely causes a failure. Hence, a formal verification is needed to assure the correctness of the program. In this paper, we have proposed a method to verify a program implemented by Go. A refinement checker for CSP or Failures Divergence Refinement (FDR) in [1] is used as a verification tool. We also give some experimental results to show effectiveness of our method. © 2016 IEEE.",CSP; FDR; Go programming language (Go),Computer operating procedures; Computer programming; Computer programming languages; Software engineering; Communicating sequential process; Concurrent programming; Failures-divergences; Go programming language (Go); It supports; Program behavior; Verification tools; Formal verification,Conference Paper,2-s2.0-85006869236,
pps,Improving students' motivation to study using salary prediction system,2016,"2016 13th International Joint Conference on Computer Science and Software Engineering, JCSSE 2016",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006839939&doi=10.1109%2fJCSSE.2016.7748896&partnerID=40&md5=7b04e99c6d4a2bec845edfe7b0ddade6,"This paper proposes a salary prediction system for increasing students' motivation in studying. A decision tree technique is used to generate a prediction model with seven features. We evaluated the system efficiency using 13,541 records of graduated student data in 10-fold cross validation. An accuracy result in overall is 41.39%. Furthermore, we exploit questionnaires to evaluate effectiveness of the system with 50 student samples. The result shows that the system can effectively boost students' motivation in studying and also show them a positive viewpoint of their future. At last, the sampling students answer the positive satisfaction in using the system since they found the system is easy to use and the prediction results are simple and comprehensible. © 2016 IEEE.",Decision Tree model; graduated student history; Motivation; Salary prediction system,Compensation (personnel); Decision trees; Education; Forecasting; Motivation; Software engineering; Surveys; Wages; 10-fold cross-validation; Decision tree modeling; Decision tree techniques; Prediction model; Prediction systems; System efficiency; Students,Conference Paper,2-s2.0-85006839939,
ssr,Changeability prediction model for Java class based on multiple layer perceptron neural network,2016,"2016 13th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2016",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988850566&doi=10.1109%2fECTICon.2016.7561392&partnerID=40&md5=f6659eeff39f716709843a1c2efd019a,"A quality model for assessing the changeability level of Java code is important for software development. It permits developer to know which classes to be improved for having a better software maintainability. Moreover, a good quality model must be created based on a set of well-selected attributes and metrics. Currently, no research work proposes a changeability assessment model that takes into consideration the metrics covering ten relevant object-oriented attributes. We propose a class changeability prediction model developed by using the multilayer perceptron (MLP) as a classifier method and a training data set of 137 Java classes from jEdit open source project for training the model. Model accuracy attains 89.81% and the model can perfectly separate Java classes with good changeability level from those with poor or fair changeability levels. © 2016 IEEE.",changeability; maintainability; software maintenance,Classification (of information); Computer software maintenance; Engineering research; Maintainability; Network layers; Object oriented programming; Open source software; Software design; Assessment models; changeability; Multi layer perceptron; Multiple layer perceptron; Open source projects; Prediction model; Software maintainability; Training data sets; Computer software,Conference Paper,2-s2.0-84988850566,
pkl,Multi-objective selection approach for association mining based on interesting measures,2016,"2016 3rd International Conference on Digital Information Processing, Data Mining, and Wireless Communications, DIPDMWC 2016",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992047567&doi=10.1109%2fDIPDMWC.2016.7529359&partnerID=40&md5=48bc20d9a028789843920ba6fa2a7f0e,"In the era of digital information, the size of data collection has been growing significantly. Knowledge results in term of association rules obtained from the set of data are numerous and hard to select. This paper proposes the approach for selecting the interesting subsets of association rules from big association results. The selective criterion is based on well-known interesting measures including confidence, support and lift. The interesting measures are simultaneously considered in multi-objective context. While, confidence guarantees the accuracy of the association results. Support promotes popular association patterns and lift indicates rare association patterns. Thai stock market data in period of April 10, 2013 to September 5, 2014 were investigated and applied to the selection approach. The results showed that multi-objective selection approach reduces 246,084 association rules into 11 nondominated association rules. © 2016 IEEE.",association mining; association rule; interesting measure; multi-objective; selection approach; stock index data,Association rules; Data handling; Finance; Information science; Wireless telecommunication systems; Association mining; interesting measure; Multi objective; selection approach; Stock indices; Data mining,Conference Paper,2-s2.0-84992047567,
tnt,Automatic electrocardiogram signal quality assessment in continuous wireless monitoring,2016,Maejo International Journal of Science and Technology,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968735887&partnerID=40&md5=ecfb2f100e37fcaeb6153cc2acb25d0d,"This paper presents an automatic signal quality assessment method for continuously monitoring electrocardiogram (ECG) signals using wireless sensors attached to human bodies, with particular attention being given to ECG signals captured while the subjects are performing daily routine activities. In this study signal recordings from three databases are used: two ECG databases acquired using wireless body sensor networks from young subjects and elderly subjects during their daily routine activities, and the Massachusetts Institute of Technology - Boston’s Beth Israel Hospital arrhythmia database. From these databases, ECG signals are divided into small segments, each 5 seconds long, and are then labelled with two levels of quality, i.e. ‘low-quality’ and ‘high-quality’. For feature extraction, two levels of statistical features are employed: (i) window-based temporal features and (ii) segment-based features. The latter are derived from statistical values of the window-based temporal features and ECG signal amplitudes. A correlation-based feature selection algorithm is applied to find an optimal set of features. For signal quality classification, four machine-learning-based classification algorithms, i.e. Instance-based Learning, Decision Tree, Multilayer Perceptron and Rule Induction, are compared. © 2016 by Maejo University.",ECG quality assessment; Home health monitoring; Wireless sensor networks,,Article,2-s2.0-84968735887,
ksc,"TPLCR: Time-Bound, Pre-copy Live Checkpointing and Parallel Restart of Virtual Machines Using Distributed Memory Servers",2016,"Proceedings - 2015 3rd International Symposium on Computing and Networking, CANDAR 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964725798&doi=10.1109%2fCANDAR.2015.108&partnerID=40&md5=778821ebf7d25303d2531df632c89aec,"Live checkpointing of virtual machines is the ability to save the state of a virtual machine to storage while the machine is running. This paper presents a novel Time-bound, Pre-Copy Live Checkpointing and parallel Re-start mechanism (TPLCR) that implements live checkpointing based on a time-bounded, pre-copy live migration algorithm. The performance improvements of TPLCR rely on the use of multiple Distributed Memory Servers to allow fast, in-memory checkpointing and parallel restart. Along with the new TPLCR protocol, we introduce the Checkpoint-Restart Service to manage the checkpoint and restart operations in a datacenter. This paper describes a prototype implementation of TPLCR based on KVM. A series of checkpointing experiments were conducted using four CPU and memory intensive Class D NAS Parallel Benchmark kernels. Experimental results show that TPLCR checkpoint-restart performance is significantly better than traditional approaches. © 2015 IEEE.",Checkpoint-Restart; fault-Tolerance; virtualization,Java programming language; Memory architecture; Checkpoint-and-restart; Checkpoint-Restart; Distributed Memory; NAS parallel benchmarks; Prototype implementations; Traditional approaches; Virtual machines; Virtualizations; Fault tolerance,Conference Paper,2-s2.0-84964725798,
lpp,Children's and adults' schemes in categorization of basic objects and mobile applications,2016,"ICACSIS 2015 - 2015 International Conference on Advanced Computer Science and Information Systems, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964443439&doi=10.1109%2fICACSIS.2015.7415155&partnerID=40&md5=f4f8e13d15755a0694150b23e6a03bce,"Diversity of users has become recent design concerns, and children are one of those user groups. Organization of contents is one of research areas in designing for children. Research suggests differences in abilities between adults and children; for instance, attention, logic and memory skills, and linguistic abilities. This is related to the way children navigate and access information. Efficient system organization must correspond with user's categorization scheme. Prior study suggests differences in the way children categorized objects than the predetermined categories; however, it did not provide a comparison between children and adults in the way they generated the categories. Moreover, influence of expertise on categorization schemes has been highlighted in psychological literature. Primary objective of this study was to investigate how children and adults utilize categorization schemes based on their domain expertise. This study was carried out under two different circumstances: 1) when the objects were concrete and both age groups were domain experts, and 2) when objects were more abstract and both age groups could be either novices or experts. Similarity and differences between adults and children were found. The results of both tasks showed indicated that categorization schemes employed by participants depends on the information they were exposed to. © 2015 IEEE.",age differences; card sort; categorization; CCI; children; classification; HCI; information architecture; mobile,Classification (of information); Expert systems; Human computer interaction; Information systems; Age differences; Card sort; categorization; children; Information architectures; mobile; Education,Conference Paper,2-s2.0-84964443439,
nth,Children's and adults' schemes in categorization of basic objects and mobile applications,2016,"ICACSIS 2015 - 2015 International Conference on Advanced Computer Science and Information Systems, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964443439&doi=10.1109%2fICACSIS.2015.7415155&partnerID=40&md5=f4f8e13d15755a0694150b23e6a03bce,"Diversity of users has become recent design concerns, and children are one of those user groups. Organization of contents is one of research areas in designing for children. Research suggests differences in abilities between adults and children; for instance, attention, logic and memory skills, and linguistic abilities. This is related to the way children navigate and access information. Efficient system organization must correspond with user's categorization scheme. Prior study suggests differences in the way children categorized objects than the predetermined categories; however, it did not provide a comparison between children and adults in the way they generated the categories. Moreover, influence of expertise on categorization schemes has been highlighted in psychological literature. Primary objective of this study was to investigate how children and adults utilize categorization schemes based on their domain expertise. This study was carried out under two different circumstances: 1) when the objects were concrete and both age groups were domain experts, and 2) when objects were more abstract and both age groups could be either novices or experts. Similarity and differences between adults and children were found. The results of both tasks showed indicated that categorization schemes employed by participants depends on the information they were exposed to. © 2015 IEEE.",age differences; card sort; categorization; CCI; children; classification; HCI; information architecture; mobile,Classification (of information); Expert systems; Human computer interaction; Information systems; Age differences; Card sort; categorization; children; Information architectures; mobile; Education,Conference Paper,2-s2.0-84964443439,
nth,Gestalt geometric CAPTCHA,2016,"ICACSIS 2015 - 2015 International Conference on Advanced Computer Science and Information Systems, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964501474&doi=10.1109%2fICACSIS.2015.7415185&partnerID=40&md5=e65c773362f7bad589e79d07c820cfab,"This research investigated a new Image-Based CAPTCHA called Gestalt Geometric CAPTCHA, which does not require the use of a database of images, and is based on the Gestalt Theory of human recognition. The aim was to develop a type of CAPTCHA that is easier for human users and harder for bots. We experimentally tested the use and effectiveness of Gestalt Geometric CAPTCHA in terms of time for completion, authentication pass rate, and user satisfaction, in comparison with reCAPTCHA, Ironclad CAPTCHA, and ShapeCAPTCHA. We also tested our novel CAPTCHA for robustness against two shape detection and classification programs, ShapeChecker and Shape-detect. The results were promising, but some issues remain for future improvement. © 2015 IEEE.",Authentication; CAPTCHA; Completely Automatic Public Turing test to tell Computer and Human Apart; Gestalt; Human Recognition; Security; User Interface,Artificial intelligence; Authentication; Electronic mail filters; Geometry; Information systems; Optical character recognition; User interfaces; CAPTCHAs; Completely automatic public turing test to tell computer and human apart; Gestalt; Human recognition; Security; Network security,Conference Paper,2-s2.0-84964501474,
mvp,Standardized cost estimation in Thai government's software development projects,2016,ICSEC 2015 - 19th International Computer Science and Engineering Conference: Hybrid Cloud Computing: A New Approach for Big Data Era,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964317425&doi=10.1109%2fICSEC.2015.7401449&partnerID=40&md5=76d3b7f70aeed5863a53e05b884c30fb,"Software features and costs are often unquantifiable due to the abstract nature of software. In many cases, this results in the estimated costs of software development projects to be potentially highly biased, highly inaccurate, or highly unjustified. Hence, current software estimation methodologies can open up areas for corruption as estimated budgets and costs are difficult to verify and validate. The Thai COCOMO Framework and cost estimation model were developed in order to overcome this problem in software development projects for Thai government by providing standard and transparency to the software estimation process with justification to the associated costs. This paper discusses the areas in software project estimation that are prone to corruption and ways that the COCOMO model and framework can be used to address them. © 2015 IEEE.",COCOMO II; cost estimation; software engineering,Big data; Budget control; Computer software; Cost engineering; Costs; Crime; Software design; Software engineering; COCOMO II; Cost estimation models; Cost estimations; Nature of software; Software development projects; Software estimation; Software features; Software project; Cost estimating,Conference Paper,2-s2.0-84964317425,
pkl,Interesting-based association rules for highway traffic data,2016,ICSEC 2015 - 19th International Computer Science and Engineering Conference: Hybrid Cloud Computing: A New Approach for Big Data Era,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964355858&doi=10.1109%2fICSEC.2015.7401413&partnerID=40&md5=2328e3460c36830cf81d1b773f77ae5a,"Highway no.7 is the main route for transportation between Bangkok to the eastern seaboard. The eastern seaboard is the major economic region of Thailand. This paper applies theory of rough set to find the hidden relationship among date, time, entering station, exit station and vehicular traffic. The transactional data was gathered from the Division of Inter City Motorway, Department of Highways in the period of January 2014 to April 2015. The set of transactional data consists of 242,066 records. The relations in term of association rules are obtained. The original association results are numerous and difficult to select. This paper proposed the interesting of association rules by using indicators that are generalization, leverage, coverage and lift. The association rules with indicator values are compared and discussed. © 2015 IEEE.",Association Rule; Interesting Indicator; Rough Set Theory; Thai Highway Traffic,Association rules; Big data; Computation theory; Transportation; Transportation routes; Bangkok; Economic regions; Highway traffic; Indicator values; Thailand; Transactional data; Rough set theory,Conference Paper,2-s2.0-84964355858,
yao,Analysis of range-based key properties for sharded cluster of MongoDB,2016,"2015 IEEE 2nd International Conference on InformationScience and Security, ICISS 2015",8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964643225&doi=10.1109%2fICISSEC.2015.7370983&partnerID=40&md5=338565cb03585a330c21111a5e6bc45f,"MongoDB is one of the most popular NoSQL database nowadays. It is an open-source document-oriented database with flexible schema. To increase performances, it can easily scale both vertically and horizontally. For horizontal scaling, MongoDB uses auto-sharding technique to divide data and distribute it over multiple machines. However, in this technique, a DB administrator must choose a shard key for MongoDB to split its collection. Selecting a right key could improve the performance and capability of a database. Contrarily, a wrong key takes down performances and, in some serious case, could lead to a system halt. Therefore, it is important to choose a correct key. MongoDB has a general suggestion on the properties of its ideal shard key. For instance, a good shard key should have high degree randomness for write scaling and should contain high locality for range-query reading. In order to understand the impact of these properties on a shard key, this paper has analyzed and evaluated such suggested properties. We discussed how the variation of a shard key's choices could impact the DB performance and it gives the base to help a MongoDB admin to understand and could select a good shard key for his/her system. © 2015 IEEE.",Horizontal scaling; MongoDB; Shard key selection; Sharding,Database systems; Information science; Auto-Sharding; Horizontal scaling; MongoDB; Multiple machine; Nosql database; Open sources; Shard key selection; Sharding; Query processing,Conference Paper,2-s2.0-84964643225,
nth,An RDF platform for generating web API for open government data,2016,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006120635&partnerID=40&md5=bc2ce82fd4299a6db8a87403beccf92c,"Most of datasets in open data portals are mainly in tabular format in spreadsheet, e.g. CSV and XLS. To increase the value and reusability of these datasets, the datasets should be made available in RDF format that can support better data querying and data integration. However, publishing and querying RDF requires different knowledge and skills. In this poster, we present a platform for publishing and querying the dataset in RDF that does not require the user's knowledge of RDF and SPARQL. This framework supports semi-Automatic construction of RDF data and RESTFul APIs from the datasets in tabular format. The framework provides automatic schema detection, i.e. data type detection, and ontology and RDF data mapping generation. RESTful API is provided on top of the SPARQL data querying service for each published RDF dataset. A platform prototype was developed and demonstrated using some datasets from the Data.go.th website. Some current research directions include automatic dataset API generation based on Web crawler and validator and development of intelligent search engine over the dataset APIs.",Dataset management; Open data platform; RDF data publishing,Data integration; Reusability; Search engines; Semantics; Data querying; Intelligent search engine; Open datum; Rdf and sparql; RDF data; Restful api; Semi-automatics; Web crawlers; Semantic Web,Conference Paper,2-s2.0-85006120635,
nth,A framework for linking RDF datasets for Thailand open government data based on semantic type detection,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006025087&doi=10.1007%2f978-3-319-49304-6_31&partnerID=40&md5=b73f6c2abc3236f2caf50526d948dd0e,"Most of datasets in open government data portals are mainly in tabular format in spreadsheet, e.g. CSV and XLS. To increase the value and reusability of these datasets, the datasets should be made available in RDF format that can support better data querying and data integration. Our previous work proposed a semi-automatic framework for generating RDF datasets from existing datasets in tabular format. In this paper, we extend our framework to support automatic linking of the RDF datasets. One of the important steps is mapping some literal values that appear in a dataset to some standard URIs. Several previous researches use semantic search API such as DBpedia or Sindice for URI mapping. However, this approach is not appropriate for the datasets of Thailand open data portal (Data.go.th) because there is insufficient data for Thai name entities. In addition, a name may match with more than one URI, i.e. word ambiguity. For example, the name “Bangkok” may match with those referenced by URIs of a province, a hospital or a university. To resolve these issues, our framework proposes that finding semantic types is essential to resolve word ambiguity in retrieving a proper URI for a name entity. This paper presents a framework for finding semantic types and mapping name entities to URIs, i.e. URI lookup. A Name Entity Recognition (NER) technique is applied in finding semantic type of a column in a CSV dataset. The results are used for creating ontology and RDF data that include the URI mappings for name entities. We evaluate two approaches by comparing the performance of a semantic search API, i.e. Wikipedia and the NER technique using some datasets from the Data.go.th website. © Springer International Publishing AG 2016.",Automatic linked dataset creation; Automatic ontology creation; Finding semantic types; Name entity recognition (NER),Data integration; Digital libraries; Mapping; Reusability; Semantics; Automatic linked dataset creation; Automatic linking; Automatic ontology; Data querying; Name entity recognition; Semantic search; Semantic types; Semi-automatics; Semantic Web,Conference Paper,2-s2.0-85006025087,
wdc,Simulating crowd movement in agent-based model of large-scale flood,2015,"ICAICTA 2015 - 2015 International Conference on Advanced Informatics: Concepts, Theory and Applications",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960970143&doi=10.1109%2fICAICTA.2015.7335368&partnerID=40&md5=cb4a55fdef07bde9da4cf9430a6aa4d6,"Crowd movement during natural disasters and major accidents can affect the success of evacuation procedure. Thus to develop an effective evacuation plan, a wide-range of scenarios causing different routes and patterns of crowd movement should be considered. Recent researches in agent-based simulation have achieved techniques to simulate crowd movement in emergency scenarios at city scale. However simulating crowd movement at a macroscopic level for disasters which might affect several cities, like large-scale flood, is still a challenge. This paper addresses this issue and makes three contributions. First, the development of an agent-based layered model to simulate large-scale flood using GIS is demonstrated. Second, the simulation of crowd agents' movement on available roads is presented. Third, the preliminary experiments running on private Cloud server is reported. The experiments cover case studies illustrated the movement of crowd agents around Thailand while several parts of the country were inundated. The 2D animation depict the movement of crowd; and the simulation results show the status of the agents and the amount of individuals which required shelters. To simulate one day events, the simulator took 4-9 hours execution time depending on the severity of floods and available facilities. © 2015 IEEE.",Cloud Computing; Crowd Movement; Disaster Simulation; Distributed Artificial Intelligence; Multiagent Systems,Artificial intelligence; Autonomous agents; Cloud computing; Computation theory; Computational methods; Disasters; Distributed computer systems; Information science; Multi agent systems; Agent based simulation; Crowd movements; Disaster simulation; Distributed Artificial Intelligence; Emergency scenario; Evacuation procedures; Macroscopic levels; Movement of crowds; Floods,Conference Paper,2-s2.0-84960970143,
lpp,Research ethics in human-computer interaction: A review of ethical concerns in the past five years,2015,"Proceedings of 2015 2nd National Foundation for Science and Technology Development Conference on Information and Computer Science, NICS 2015",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959892952&doi=10.1109%2fNICS.2015.7302187&partnerID=40&md5=7710540150f50edd29964e98c9190df3,"In recent years, novel technologies have been introduced to Human-Computer Interaction (HCI) research community. Research contexts and the range of studies have been widen. As technology progresses, new ethical issues are posed. This paper examined and reviewed ethical issues and concerns in recent HCI research. An overview of the existing codes of conduct was also provided, in order to get clear a comprehension on the existing concerns in research protocols. The review focused on top-twenty conference series and journals in HCI, published during 2010-June 2015. The purpose was to comprehend the latest ethical issues from the premier work in HCI research community. Altogether 13 categories of ethical concerns were identified. Some of them were consistent with the existing codes, while some of the issues were specific to HCI research. There were some identified issues that should be accentuated, as they currently were not in any of the existing codes. © 2015 IEEE.",Computer Ethics; HCI; Human-Computer Interaction; Research Ethics; Review,Codes (symbols); Philosophical aspects; Reviews; Computer ethics; Ethical concerns; Ethical issues; Hci researches; Human-computer interaction researches; Research ethics; Research protocol; Technology progress; Human computer interaction,Conference Paper,2-s2.0-84959892952,
nth,Research ethics in human-computer interaction: A review of ethical concerns in the past five years,2015,"Proceedings of 2015 2nd National Foundation for Science and Technology Development Conference on Information and Computer Science, NICS 2015",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959892952&doi=10.1109%2fNICS.2015.7302187&partnerID=40&md5=7710540150f50edd29964e98c9190df3,"In recent years, novel technologies have been introduced to Human-Computer Interaction (HCI) research community. Research contexts and the range of studies have been widen. As technology progresses, new ethical issues are posed. This paper examined and reviewed ethical issues and concerns in recent HCI research. An overview of the existing codes of conduct was also provided, in order to get clear a comprehension on the existing concerns in research protocols. The review focused on top-twenty conference series and journals in HCI, published during 2010-June 2015. The purpose was to comprehend the latest ethical issues from the premier work in HCI research community. Altogether 13 categories of ethical concerns were identified. Some of them were consistent with the existing codes, while some of the issues were specific to HCI research. There were some identified issues that should be accentuated, as they currently were not in any of the existing codes. © 2015 IEEE.",Computer Ethics; HCI; Human-Computer Interaction; Research Ethics; Review,Codes (symbols); Philosophical aspects; Reviews; Computer ethics; Ethical concerns; Ethical issues; Hci researches; Human-computer interaction researches; Research ethics; Research protocol; Technology progress; Human computer interaction,Conference Paper,2-s2.0-84959892952,
pkl,Quality-based association rules for stock index data by using rough set theory,2015,"ECTI-CON 2015 - 2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956969569&doi=10.1109%2fECTICon.2015.7207089&partnerID=40&md5=190422850895fd19c1d14683c8385a4b,"The stock index indicators, e.g., Moving Average (MA), Relative Strength Index (RSI), Price Rate of Change (PROC), Moving Average Convergence Divergence (MACD), or Stochastic Oscillator (STH) are often used as the main factor in trading. Stock traders decide to trade by using these indicators. This paper applies theory of rough set to find the hidden relationship among these indicators which affects the market price. The transactional data was gathered from Thai stock market in the period of April 10, 2013 to September 5, 2014. This paper focuses on the exact relationship that always occurs from the set of data. The obtained relation may appear infrequent but every time the cause appears, the result is always happen. The association results show that the set of indicators affects the price change. When the indicators are changed substantially, the price will change significantly. © 2015 IEEE.",Association Rule; Rough Set; Stock Indicator,Association rules; Commerce; Costs; Finance; Stochastic systems; Moving average convergence divergence (MACD); Moving averages; Rate of change; Relative strength index; Stochastic oscillators; Stock indicators; Thai stock markets; Transactional data; Rough set theory,Conference Paper,2-s2.0-84956969569,
ssr,Automatic code locations identification for replacing temporary variable with query method,2015,"ECTI-CON 2015 - 2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956970597&doi=10.1109%2fECTICon.2015.7207086&partnerID=40&md5=b797016d93debc5bc201dea5fbbd9452,"Automatic application of refactoring techniques can help developer save effort for removing bad smells from their code which improves software maintainability. To remove automatically long method bad smell, which is one of the most serious bad smells, we need an automatic application of six refactoring techniques. However, only one refactoring technique 'Extract Method' can be automated. In this research, we propose an algorithm to identify code locations which will be extracted for creating a query method. We performed an experiment to conclude the workability and correctness of our algorithm. This is the most important step towards an automatic application of refactoring technique 'replace temp with query'. © 2015 IEEE.",Bad Smell; Program Dependency Graph; Program Slicing; Refactoring Application; Software Maintenance,Codes (symbols); Computer software maintenance; Odors; Program processors; Automatic application; Automatic codes; Bad smells; Program dependency graphs; Program slicing; Query methods; Refactorings; Software maintainability; Application programs,Conference Paper,2-s2.0-84956970597,
tnt,False alarm reduction in BSN-based cardiac monitoring using signal quality and activity type information,2015,Sensors (Switzerland),10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922570577&doi=10.3390%2fs150203952&partnerID=40&md5=64c2d05095437a24af635edec5c2aab3,"False alarms in cardiac monitoring affect the quality of medical care, impacting on both patients and healthcare providers. In continuous cardiac monitoring using wireless Body Sensor Networks (BSNs), the quality of ECG signals can be deteriorated owing to several factors, e.g., noises, low battery power, and network transmission problems, often resulting in high false alarm rates. In addition, body movements occurring from activities of daily living (ADLs) can also create false alarms. This paper presents a two-phase framework for false arrhythmia alarm reduction in continuous cardiac monitoring, using signals from an ECG sensor and a 3D accelerometer. In the first phase, classification models constructed using machine learning algorithms are used for labeling input signals. ECG signals are labeled with heartbeat types and signal quality levels, while 3D acceleration signals are labeled with ADL types. In the second phase, a rule-based expert system is used for combining classification results in order to determine whether arrhythmia alarms should be accepted or suppressed. The proposed framework was validated on datasets acquired using BSNs and the MIT-BIH arrhythmia database. For the BSN dataset, acceleration and ECG signals were collected from 10 young and 10 elderly subjects while they were performing ADLs. The framework reduced the false alarm rate from 9.58% to 1.43% in our experimental study, showing that it can potentially assist physicians in diagnosing a vast amount of data acquired from wireless sensors and enhance the performance of continuous cardiac monitoring. © 2015 by the authors; licensee MDPI, Basel, Switzerland.",Activity classification; Arrhythmia classification; Body sensor network; False alarm reduction; Machine learning; Rule-based expert system; Signal quality classification,Alarm systems; Algorithms; Artificial intelligence; Diagnosis; Diseases; Electrocardiography; Errors; Expert systems; Heart; Image resolution; Learning algorithms; Learning systems; Wireless sensor networks; Activity classifications; Arrhythmia classification; False alarm reductions; Rule based expert systems; Signal quality; Body sensor networks,Article,2-s2.0-84922570577,
lpp,A comparative study on sensor displacement effect on realistic sensor displacement benchmark dataset,2015,Advances in Intelligent Systems and Computing,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931292224&doi=10.1007%2f978-3-319-19024-2_10&partnerID=40&md5=c2c6e55b253bfe454cb5e92e8cfd278b,"Activity Recognition (AR) research is growing and plays a major role in various fields. The approach of using wearable sensors for AR is well-accepted, as it compensates the need to install cameras in image processing approach which can lead to privacy violation. Using wearable sensors can suffer from one disadvantage - sensor displacement. There have been a number of research which studies sensor displacement problem. However, the conclusion cannot be made as which classifier is better than another in recognizing displacement data, as the prior experiments were performed under different conditions and focused on different parts of the body. This work aims to evaluate recognition performance of different algorithms - SVM, C4.5, and Naïve Bayes - on ideal-placement and displacement data on whole body activities, by adopting REALDISP dataset to make such evaluation. The accuracy of all algorithms on ideal placement data was above 90%, where SVM yielded the highest accuracy. Displacement data were tested against classification models constructed from ideal-placement data. The results shows that there was a dramatic drop in recognition performance. The accuracy of all algorithms on displacement data was between 50-60%, and C4.5 could handle displacement data the best. © Springer International Publishing Switzerland 2015.",Activity Recognition; C4.5; Naïve Bayes; Sensor Displacement; SVM,Algorithms; Classification (of information); Image processing; Pattern recognition; Sodium; Wearable technology; Activity recognition; Benchmark datasets; C4.5; Classification models; Comparative studies; Privacy violation; Sensor displacement; SVM; Wearable sensors,Article,2-s2.0-84931292224,
nth,A comparative study on sensor displacement effect on realistic sensor displacement benchmark dataset,2015,Advances in Intelligent Systems and Computing,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931292224&doi=10.1007%2f978-3-319-19024-2_10&partnerID=40&md5=c2c6e55b253bfe454cb5e92e8cfd278b,"Activity Recognition (AR) research is growing and plays a major role in various fields. The approach of using wearable sensors for AR is well-accepted, as it compensates the need to install cameras in image processing approach which can lead to privacy violation. Using wearable sensors can suffer from one disadvantage - sensor displacement. There have been a number of research which studies sensor displacement problem. However, the conclusion cannot be made as which classifier is better than another in recognizing displacement data, as the prior experiments were performed under different conditions and focused on different parts of the body. This work aims to evaluate recognition performance of different algorithms - SVM, C4.5, and Naïve Bayes - on ideal-placement and displacement data on whole body activities, by adopting REALDISP dataset to make such evaluation. The accuracy of all algorithms on ideal placement data was above 90%, where SVM yielded the highest accuracy. Displacement data were tested against classification models constructed from ideal-placement data. The results shows that there was a dramatic drop in recognition performance. The accuracy of all algorithms on displacement data was between 50-60%, and C4.5 could handle displacement data the best. © Springer International Publishing Switzerland 2015.",Activity Recognition; C4.5; Naïve Bayes; Sensor Displacement; SVM,Algorithms; Classification (of information); Image processing; Pattern recognition; Sodium; Wearable technology; Activity recognition; Benchmark datasets; C4.5; Classification models; Comparative studies; Privacy violation; Sensor displacement; SVM; Wearable sensors,Article,2-s2.0-84931292224,
ddp,Formal verification of concurrency errors in JavaScript web applications,2015,"Network Security and Communication Engineering - Proceedings of the 2014 International Conference on Network Security and Communication Engineering, NSCE 2014",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961644512&doi=10.1201%2fb18660-96&partnerID=40&md5=ade146b2bc72eb01406642fbd9c4e6e8,"This paper proposes a timed trace theoretic verification to detect concurrency errors in Java Script web applications. They are caused because the parsing operation stops parsing unexpectedly and event handling operations are executed in an unexpected order. Experimented with the eight benchmark applications, the proposed approach shows its effectiveness. © 2015 Taylor & Francis Group, London.",,Benchmarking; Formal verification; Benchmark applications; Concurrency errors; Event handling; Java scripts; Javascript; Trace theoretic verification; WEB application; Network security,Conference Paper,2-s2.0-84961644512,
ddp,Formal verification of JADE behaviour: A modeling approach,2015,"Proceedings of the 2015 12th International Joint Conference on Computer Science and Software Engineering, JCSSE 2015",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945946007&doi=10.1109%2fJCSSE.2015.7219792&partnerID=40&md5=ec42145a499c56cd9a178881ddf1d25e,"Agent is an autonomous entity who can perform actions to achieve its goal. A multi-agent system (MAS) is a system composed of multiple interacting agents doing activities for a complex system. Since in the real world, MAS have agents acting behaviors concurrently in temporal context, it is hard to completely eliminate failures in such a system. Therefore, a formal verification of a system model is required to ensure the correctness of the MAS design. In this paper, we present the timed trace theoretic verification to detect safety and timing failures in MAS model based on Java Agent Development Framework (JADE), an Application Programming Interface (API) for developing MAS. An example is used to illustrate our proposed method. © 2015 IEEE.",formal verification; jade; multi-agent system; time petri net; timed trace theory,Application programming interfaces (API); Autonomous agents; Java programming language; Multi agent systems; Petri nets; Software engineering; Autonomous entities; Interacting agents; jade; Java agent development framework; System modeling; Time Petri nets; Timed trace theory; Trace theoretic verification; Formal verification,Conference Paper,2-s2.0-84945946007,
ddp,Formal verification of multi-agent system based on JADE: A semi-runtime approach,2015,Advances in Intelligent Systems and Computing,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931260342&doi=10.1007%2f978-3-319-19024-2_30&partnerID=40&md5=3e7f54498050c93662bec22c2e843b5c,"Since a multi-agent system (MAS) is a system composed of multiple interacting agents, verifying multi-agent interaction is acquiring increasing importance. Several researches have been proposed to verify the multi-agent interaction, so we present our proposed method as an alternative way to verify how MAS meets their specification in term of messaging among agents. This paper describes an ongoing effort for a formal verification of MAS based on Java Agent Development Framework (JADE) in semi-runtime approach. The timed trace theoretic verification is applied to detect time constraint failures in such a system. We use a well-known book trading case as an example to illustrate our proposed method. © Springer International Publishing Switzerland 2015.",Formal verification; JADE; Multi-agent system; Semi-runtime verification; Time petri net,Formal verification; Petri nets; Interacting agents; JADE; Java agent development framework; Multi-agent interaction; Run-time verification; Time constraints; Time Petri nets; Trace theoretic verification; Multi agent systems,Article,2-s2.0-84931260342,
wdc,Accelerating real-time face detection on a raspberry pi telepresence robot,2015,"5th International Conference on Innovative Computing Technology, INTECH 2015",11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946548937&doi=10.1109%2fINTECH.2015.7173482&partnerID=40&md5=aa362ced28f2018755c87ded85bd18cd,"Effective face detection in real-time is an essential procedure for achieving autonomous motion in telepresence robots. Since the procedure demand high computation power, using it to create autonomous motion in low-cost robots is a challenge. This paper addresses this issue and making three contributions. First, the process to enabling the real-time face detection on Raspberry Pi's graphical processor is presented. Second, the development of an autonomous pan-tilt telepresence robot to follow an interlocutor face using two Raspberry Pi-1 model B is demonstrated. Third, the evaluation on resource requirements when operating the robot in various scenarios is described. The face detection module ran in average at 16.7 Quarter VGA frames per second, while mediating real-time video conversation remotely between two parties. The results confirmed that vision-based autonomous motion can be added to a low-cost telepresence robots with acceptable performance. Thus, making secure telecommunication via robots is viable with less budget constraint. © 2015 IEEE.",Face detection; GPU; LBP; Raspberry Pi; robot,Budget control; Gears; Robots; Visual communication; Acceptable performance; Face detection module; GPU; LBP; Real-time face detection; Resource requirements; Secure telecommunication; Telepresence robots; Face recognition,Conference Paper,2-s2.0-84946548937,
wdc,Accelerating range-based loops on heterogeneous systems,2015,"Proceedings of the 2015-7th International Conference on Knowledge and Smart Technology, KST 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925849086&doi=10.1109%2fKST.2015.7051466&partnerID=40&md5=51b4e24f844080d3f4559eb70209a010,"Range-based loop is a powerful construct due to its clear and concise syntax. The abstraction of loop index in a range-based loop implies loop-level parallelism ready to be exploited. Despite its advantage on hidden parallelism and programmability, the magnitude of performance gain by accelerating range-based loop on heterogeneous systems is still not well studied. This paper addresses this issue and make three contributions. First, the review showing the magnitude of performance gain from CUDA/OpenCL code, generated by ten exisiting auto-parallelizing compilers is presented. Second, the performance comparison between range-based and traditional loops acceleration on four workloads from the SHOC benchmark is reported. Third, the performance limitation on using directive-based compiler to accelerate range-based loop is discussed. The results show that transforming scientific workloads to exploit range-based loops is a challenge. The review results show that code generated by auto-parallelizing achieved an average of 37±23 folds speedup relative to sequential CPU, while the proposed range-based compiler achieved higher speedup than the average (44.8±22x). The evaluation against four workloads from highly-tuned benchmark shows that range-based loop acceleration achieved in average 72% of the benchmark's performance. This highlights range-based loops as a promising target for auto parallelizing compiling code on heterogeneous systems. © 2015 IEEE.",directive-based compiler; GPU; heterogeneous systems; loop parallelization; OpenCL,Codes (symbols); Parallel architectures; Program compilers; directive-based compiler; GPU; Heterogeneous systems; Loop parallelization; OpenCL; Benchmarking,Conference Paper,2-s2.0-84925849086,
ppr,Improved real-time scheduling of periodic tasks on multiprocessors,2015,Concurrency Computation ,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929933446&doi=10.1002%2fcpe.2969&partnerID=40&md5=a0006a4ceb77545dc805dc7808e8a7dd,"There is an increasing number of high-performance periodic real-time applications in areas such as control systems, autonomous robots and financial systems. This article presents a novel algorithm, called Notional Approximation for Balancing Load Residues (NABLR), for scheduling these applications on high-performance computing resources. The algorithm utilizes a combination of task residual loads and runtime laxities to carefully plan task execution between two consecutive job arrivals, so that available resources can be fully utilized and avoid deadline misses as possible. The empirical study in our article presented at the 2011 International Conference on High Performance Computing and Simulation (HPCS) was further extended by including additional static task sets and a new adaptive task set generated by our motivating application in brain-machine interfaces, which simulates the control of movement of a prosthetic limb according to activities of input signals. Out of 25,000 task sets, NABLR can schedule up to 76% of the sets versus 43% by the best known efficient algorithm (named anticipating slack earliest deadline first until zero laxity [ASEDZL]), while incurring significantly smaller overheads than those of a known optimal algorithm (on average, 80% fewer preemptions, migrations, and 75% fewer scheduler invocations), and being comparable to those of suboptimal schedulers (within only 12% more preemptions/migrations). Additionally, the evaluation results show that NABLR completes more task instances when compared with ASEDZL, which yields a greater system output accuracy. Copyright © 2012 John Wiley & Sons, Ltd.",global multiprocessor scheduling; real-time applications; resource allocation; sharing and management,Algorithms; Approximation algorithms; Brain; Brain computer interface; Computational complexity; Multiprocessing systems; Real time systems; Resource allocation; Brain machine interface; Earliest deadline first; Evaluation results; Global Multiprocessor Scheduling; High performance computing; High-performance computing resources; Real - time scheduling; Real-time application; Scheduling,Conference Paper,2-s2.0-84929933446,
wlr,Dynamic node lifetime estimation for wireless sensor Networks,2014,IEEE Sensors Journal,28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897874180&doi=10.1109%2fJSEN.2013.2295303&partnerID=40&md5=f3cf7554020db339aa41c665f08c1502,"Wireless sensor networks (WSNs) consist of a large number of nodes each with limited battery power. As networks of these nodes are usually deployed unattended, network lifetime becomes an important concern. This paper proposes a novel, feasible, dynamic approach for node lifetime estimation that works for both static and dynamic loads. It covers several factors that have an impact on node lifetime, including battery type, model, brand, self-discharge, discharge rate, age, and temperature. The feasibility of the proposed scheme is evaluated by using the real testbed experiments with two wireless sensor platforms: Mica2 and N740 NanoSensor, two operating systems: TinyOS and Contiki, and different brands of alkaline and nickel-metal-hydride batteries. The deviation of the proposed estimation is in the range of -3.5% -2.5%. Three major contributions are presented in this paper: 1) the impact factors on node lifetime; 2) lifetime equations for any starting voltage, ageing, charge cycles, and temperatures; and 3) the dynamic node lifetime estimation technique, which is proposed and implemented on real hardware and software platforms in WSNs. © 2001-2012 IEEE.",battery capacity; current consumption; Lifetime; wireless sensor networks,Battery capacity; Current consumption; Dynamic approaches; Hardware and software; Lifetime; Nickel metal hydride battery; Static and dynamic loads; Wireless sensor network (WSNs); Dynamic loads; Estimation; Wireless sensor networks; Sensor nodes,Article,2-s2.0-84897874180,
lpp,The ethics of computer research: A survey of user acceptance towards mobile HCI research practices and factor influencing the willingness to participate and to share information in research,2014,"2014 International Computer Science and Engineering Conference, ICSEC 2014",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988274713&doi=10.1109%2fICSEC.2014.6978227&partnerID=40&md5=db56c9f966477e8d2e746f9b32c17104,"Mobile technology is a recent research interest in HCI community; nevertheless, this novel technology poses new ethical issues. The technology allows researchers to employ various research practices, as well as to carry out remote research in a large scale; however, it also allows the access to user's personal information which can reveal their identity without user's awareness. In 2013, an ethical guideline was proposed by McMillan et. al [1], to tackle these ethical loopholes. However, the understanding how different research practices are accepted by users is lacking. In addition, what factors influence user's willingness to participate and to share their information in research is not much known. This paper surveyed user acceptance towards current mobile HCI research practices and factors influencing the willingness to participate and to share information in research. The results indicates that being informed of what data would be collected is the most crucial factor to the acceptability of research practices. It also has strong influence on the decision to participate in research. © 2014 IEEE.",Ethics; HCI; Mobile HCI; Moral; Research ethics; Survey; User's perspective,Human computer interaction; Philosophical aspects; Surveying; Surveys; Ethics; Mobile HCI; Moral; Research ethics; User's perspective; Information dissemination,Conference Paper,2-s2.0-84988274713,
nth,The ethics of computer research: A survey of user acceptance towards mobile HCI research practices and factor influencing the willingness to participate and to share information in research,2014,"2014 International Computer Science and Engineering Conference, ICSEC 2014",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988274713&doi=10.1109%2fICSEC.2014.6978227&partnerID=40&md5=db56c9f966477e8d2e746f9b32c17104,"Mobile technology is a recent research interest in HCI community; nevertheless, this novel technology poses new ethical issues. The technology allows researchers to employ various research practices, as well as to carry out remote research in a large scale; however, it also allows the access to user's personal information which can reveal their identity without user's awareness. In 2013, an ethical guideline was proposed by McMillan et. al [1], to tackle these ethical loopholes. However, the understanding how different research practices are accepted by users is lacking. In addition, what factors influence user's willingness to participate and to share their information in research is not much known. This paper surveyed user acceptance towards current mobile HCI research practices and factors influencing the willingness to participate and to share information in research. The results indicates that being informed of what data would be collected is the most crucial factor to the acceptability of research practices. It also has strong influence on the decision to participate in research. © 2014 IEEE.",Ethics; HCI; Mobile HCI; Moral; Research ethics; Survey; User's perspective,Human computer interaction; Philosophical aspects; Surveying; Surveys; Ethics; Mobile HCI; Moral; Research ethics; User's perspective; Information dissemination,Conference Paper,2-s2.0-84988274713,
ddp,Hierarchical verification of WS-BPEL,2014,"2014 International Computer Science and Engineering Conference, ICSEC 2014",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988222717&doi=10.1109%2fICSEC.2014.6978226&partnerID=40&md5=c6b5120fbc9ec1c82dee42cf227d4923,"This paper proposes hierarchical verification of Web Service-Business Process Execution or WS-BPEL together with the partial order reduction. This approach is based on the framework of timed trace theoretic verification. It certainly supports hierarchical verification. The system is verified with modular structure. The cost of verification is considerably minimized. Experimenting with the case study, the proposed method shows its effectiveness. © 2014 IEEE.",Hierarchical Verification; WS-BPEL,State space methods; Hierarchical verification; Modular structures; Partial order reductions; Service business; Trace theoretic verification; WS-BPEL; Web services,Conference Paper,2-s2.0-84988222717,
wdc,Computing platforms for large-scale multi-agent simulations: The niche for heterogeneous systems,2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958541155&doi=10.1007%2f978-3-319-10840-7_51&partnerID=40&md5=2b7a110c9f8e36df7221b0754cdcd8da,"A rapid shift of computing platforms for large-scale multi-agent simulation (MAS) towards higher parallelism using tools from simulation frameworks has made the impact of MAS logic on performance become transparent. This limits the perspective of developing MAS logic towards a sustained high performance direction. This paper presents a review of 62 works related to large-scale MASs published on Scopus from 2010 - April 2014. The review was compiled in three aspects (a) the recent direction of computing platforms, (b) the state of the art in simulation frameworks, and (c) the synergy between MAS logic and scalable performance achieved. The results confirm that the nature of dynamic interactions of autonomous agents among themselves, groups, and environments has most impact on performance of computing platforms. The analysis of the results shows the correspondence between the nature of MAS logic and the execution model of heterogeneous systems. This features heterogeneous systems as a promising platform for the even larger-scale MASs in the future. © 2014 Springer International Publishing Switzerland.",agent-based simulation; multi-agent simulation; platform; review; simulation framework,Autonomous agents; C (programming language); Reviews; Autonomous agents; C (programming language); Computation theory; Engineering education; Reviews; Agent based simulation; Computing platform; Dynamic interaction; Heterogeneous systems; Multi agent simulation; platform; Scalable performance; Simulation framework; Platform; Engineering education; Multi agent systems,Conference Paper,2-s2.0-84958541155,
scw,Thai face cartoon detection and recognition using eigenface model,2014,Advanced Materials Research,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901508865&doi=10.4028%2fwww.scientific.net%2fAMR.931-932.1412&partnerID=40&md5=6c2c5aee78bbde1c2d8deddedd105ade,"In this paper, an effective method for Thai face cartoon detection and recognition is used based on haar like feature and eigenface model. The basic idea of this method is to detection and recognition a cartoon Thai from the database based on a cartoon drawn by an artist. This method consists of three steps. We first manually, haar like feature is applied for Thai face cartoon detection. Second, those faces are extracted feature using eigenface. Final, those features are recognized using Euclidean distance. For experimental result, detection rate of 95% and recognition rate of 97%. © (2014) Trans Tech Publications, Switzerland.",Artificial neural networks; Computer vision; Face detection; Pattern recognition; Thai face cartoon,Computer vision; Neural networks; Pattern recognition; Detection rates; Eigenfaces; Euclidean distance; Haar-like features; Thai face cartoon; Face recognition,Conference Paper,2-s2.0-84901508865,
yao,A chain calling in coordination for multi-tenant collaborative cloud services,2014,"2014 International Computer Science and Engineering Conference, ICSEC 2014",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988291976&doi=10.1109%2fICSEC.2014.6978212&partnerID=40&md5=ba08d9b580a822721e4d58f9a36488c2,"Currently, a cloud service is widely available but its access control is usually limited and tied only to its tenancy in isolation. To take full advantage from cloud services, multiple tenancies with some level of mutual trust would seek to collaborate and share their resources. However, building a collaborative application from inter-related chain callings to various services on a single or multiple cloud systems encounters an access control challenge and it becomes a big barrier to its adoption. To provide an appropriate fine grained chain calling authorization, this paper proposes an extension to Multi-Tenant Authorization System Model (MTAS), named ""Chain Calling Coordination in MTAS"" (C-MTAS). In the MTAS, a service with several chain callings would require the model to break a tenant's role into too many sub-roles with a limited trust scope. This would increase unintentional number of roles that could lead to breaches. It would be also hard to maintain. We, instead, propose to separate a tenant element to make a non-redundant, clear and simplified set of roles and permissions. The benefit of our model to the MTAS is shown by applying both models to the same concrete scenario. We found that our model gives a cleaner and smaller set of rules as compared to the MTAS's. We also illustrate how to use our model via a practically feasible example policy in the XACML format. The prototype system is built as an Authorization as a Service (AaaS) platform, a middle layer on the part of the cloud services, which can be used by the same or across providers. Finally, it is tested on different hardware sets. The results showed that the model could be scalable. © 2014 IEEE.",Authorization; Cloud computing; Collaboration service; MTAS; Multi-tenancy; Trust,Chains; Cloud computing; Distributed database systems; Trusted computing; Web services; Authorization; Collaboration services; MTAS; Multi tenancies; Trust; Access control,Conference Paper,2-s2.0-84988291976,
ddp,Formal verification of WS-BPEL using timed trace theory,2014,Advanced Materials Research,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901508103&doi=10.4028%2fwww.scientific.net%2fAMR.931-932.1452&partnerID=40&md5=2453b43db839889699400d4537c39e06,"A web service composition is able to create a new service by incorporating some existing web services. Currently, Web Service Business Process Execution Language or WS-BPEL is a promising language used to describe the web service composition. Since in the real world most of business processes have been involved temporal context and they are quite complex interaction, it is impossible to completely eliminate all failures in them. Therefore, a formal verification is required to assure the correctness and reliability of the web service composition. In this paper, timed trace theory has been applied to verify the web service composition with temporal constraints. Both safety and timing failures can be examined. Experimenting with a ticket reservation system, the proposed approach shows its effectiveness. © (2014) Trans Tech Publications, Switzerland.",Safety failure; Time petri net; Timed trace theory; Timing failure; WS-BPEL,Petri nets; Quality of service; Reservation systems; Safety engineering; Websites; Safety failures; Time Petri nets; Timed trace theory; Timing failures; WS-BPEL; Web services,Conference Paper,2-s2.0-84901508103,
ppr,Fuzzy scheduling of real-time ensemble systems,2014,"Proceedings of the 2014 International Conference on High Performance Computing and Simulation, HPCS 2014",4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908626179&doi=10.1109%2fHPCSim.2014.6903680&partnerID=40&md5=abd696146a9c6ea5841f8bd04059524e,"This paper addresses the problem of resource scheduling in real-time ensemble systems. An ensemble system uses multiple simple computational models (called 'experts') to produce its outputs. Real system requirements of ensemble systems (e.g., size, weight, power and cost constraints) often lead to limited availability of computational resources required to support concurrent execution of all their experts. In practical systems, uncertainties in execution time and resource utilization complicate even further the scheduling of these experts. We propose a fuzzy-logic feedback-based resource scheduler (FuzzyFES) that can provide real-time execution of all relevant experts while minimizing the impact of limited resources and uncertainties on the system performance. FuzzyFES consists of a fuzzy-logic controller (FZ), a task utilization adaptor (TUA) and a real-time task scheduler (RTS) working harmoniously in a closed loop with an ensemble system to be scheduled. By considering the uncertainties that may be present in the systems and deployment environments, FZ determines the total allowable CPU utilization for the ensemble system. TUA then calculates the amount of resource utilization to be allocated to each expert not exceeding the total allowable utilization. The assigned utilization from TUA ensures that critical experts achieve their best performance while guaranteeing minimum execution time needed by others. RTS creates a real-time schedule for the experts to execute on multiple processors according to the allotted utilization. Our performance evaluation of a case-study ensemble system with limited resources demonstrates that FuzzyFES can schedule experts to produce outputs closely similar to those of the same system with sufficient resources, although the limited-resource system has up to 40% fewer resources. The results also confirm FuzzyFES's efficiency and show that execution-time imprecision and occasional fluctuation of resource availability can be tolerated by at least 45% more than when the experts are scheduled in an open-loop manner. © 2014 IEEE.",ensemble systems; ensemble systems; feedback control; fuzzy logic; real-time scheduling; resource allocation,Computation theory; Feedback control; Fuzzy logic; Resource allocation; Scheduling; Computational model; Computational resources; Concurrent execution; Ensemble systems; Fuzzy logic controllers; Real - time scheduling; Resource availability; Resource utilizations; Real time systems,Conference Paper,2-s2.0-84908626179,
ppr,Dynamic scheduling of real-time mixture-of-experts systems on limited resources,2014,IEEE Transactions on Computers,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903593526&doi=10.1109%2fTC.2013.50&partnerID=40&md5=a6738ece5d73aac92b1f5bc3d77cd3f3,"A Mixture-of-Experts (MoE) system generates an output in each operating cycle by combining results of multiple models (the 'experts'). The contribution of any given expert to a final solution depends on a parameter called responsibility, which can vary from cycle to cycle. When resources are insufficient to run all experts, two problems arise: 1) how much utilization is to be allocated to experts and 2) how can a schedule be created based on these allocations. Problem (1) can be formulated as a succession of optimization problems, each of which calculates experts' allocations in a cycle. Explicit mappings from responsibilities to allocation weights are needed to solve each of these problems in every cycle using a technique called 'task compression (TC).' We refer to this baseline approach as TT-TC. Two other proposed heuristics TT-TC* and TT-Top reduce TC's execution time to O(N) for N experts. To address (2), the proposed EPOC scheduler converts the heuristics' allocations into schedules that satisfy capacity, execution, and learning constraints across cycles. Simulations demonstrate that our approaches enable real-time computation and significantly decrease the average percentage error of limited-resource outputs (i.e., 0.2%-40% and 0.3%-0.5% when scheduled with TT-TC* and TT-Top, respectively, versus 0.2%-97% when using TT-TC). © 2014 IEEE.",constrained optimization; ensemble systems; Mixture of experts; real-time; scheduling,Constrained optimization; Mixtures; Dynamic scheduling; Ensemble systems; Mixture of experts; Operating cycle; Optimization problems; Percentage error; real-time; Real-time computations; Scheduling,Article,2-s2.0-84903593526,
ksc,"Time-bound, thread-based live migration of virtual machines",2014,"Proceedings - 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2014",19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904572728&doi=10.1109%2fCCGrid.2014.107&partnerID=40&md5=ac0559591c37339222cd1c564e61605f,"Live migration of virtual machines is the ability to move running virtual machines between two computers with minimal downtime. Although various migration mechanisms such as pre-copy, post-copy, and state compression have been proposed, they may suffer long migration times when the migrating virtual machines run large computation and memory intensive workloads. This paper presents the design and implementation of a novel Time-bound, thread-based Live Migration (TLM) mechanism, where additional threads are added to the pre-copy live migration algorithm to handle virtual machine state transfers within a bounded time period. In the time-bound principle, the upper-bound migration time of a virtual machine is proportional to the size of the virtual machine's memory. We propose a CPU over-committing mechanism to minimize migration downtime and avoid performance impacts to other virtual machines when the migration threads are in operation. We have implemented a prototype implementation of TLM on KVM, and conducted experiments by migrating virtual machines running a number of Class D OpenMP and MPI NAS parallel benchmarks. Experimental results showed the following: (i) TLM finished live migration in a bounded time period. Users are able to measure progress of migration operation. (ii) The CPU over-committing mechanism can be used to minimize live migration downtime. However, communication performance of virtual machines during live migration also declined as the number of over-committed CPUs reduced. The patterns of decline depended on execution behaviors of the applications on the virtual machines. (iii) The execution time increases of the OpenMP and MPI versions of the MG and IS benchmarks in our experiments were approximately equal to the migration times of TLM. (iv) We evaluated our CPU over-committing mechanism against the auto-convergence mechanism recently developed in kvm-1.6. We found that both mechanisms have their pros and cons, and their performance results are varied with application. Based on these results, we believe that the TLM design is practical for live migration of virtual machines running memory-intensive workloads, and the time-bound principle is an important new feature for pre-copy live migration optimization. © 2014 IEEE.",cloud computing; live migration; virtualization,Application programming interfaces (API); Cloud computing; Experiments; Grid computing; Maintenance; Program processors; Communication performance; Design and implementations; Live migrations; Migration mechanisms; NAS parallel benchmarks; Performance impact; Prototype implementations; Virtualizations; Computer simulation,Conference Paper,2-s2.0-84904572728,
yao,Modification of MELD score by including Serum Albumin to improve prediction of mortality outcome of cirrhotic patient based on Thai cirrhotic patients,2014,"2014 11th Int. Joint Conf. on Computer Science and Software Engineering: ""Human Factors in Computer Science and Software Engineering"" - e-Science and High Performance Computing: eHPC, JCSSE 2014",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904569305&doi=10.1109%2fJCSSE.2014.6841850&partnerID=40&md5=5178c1276f4714712ee14c140f7783b2,"Nowadays, the Model for End-stage Liver Disease (MELD) has become a popular model and replaced the Child-Pugh score for the assessment of the mortality opportunity of patients with cirrhosis in 3-month period. The model predicts the severity of the disease based on 3 biochemical parameters: serum creatinine, serum total bilirubin, and INR. However, in the past, the first model like Child-Pugh score signified the importance of Serum Albumin, a protein producing in a liver. It is, thus, expected that the Serum Albumin has an effect on patients' mortality prediction. In this research, our main focus is to refine and evaluate the effect of Serum Albumin to mortality of Thai cirrhotic patients if included into the MELD model. We use the data collection from 158 Thai cirrhotic patients with different degrees of severity. They were treated at the Liver Unit and Clinic, King Chulalongkorn Memorial Hospital, The Thai Red Cross Society. The collected data were divided into the periods of 3 months, 6 months, 1 year and 2 years respectively[1]. The Kaplan-Meier statistic was used to analyze the survival opportunity of each period. Also, the Cox-Regression was utilized to evaluate the relationship and the statistical significance of the substance in each period in order to find the connection between the Serum Albumin and mortality opportunities. Results of the study show that of all the data from 158 patients, with the Serum Albumin level between 1.0 and 3.5 g/dL, when tested by Pearson's Chi-squared[2], Log Rank Test and Wilcoxon rank-sum (Mann-Whitney)[3] has the statistical significance at the 1% level of confidence (p < 0.001). Moreover, the correlation of the results using Cox Regression demonstrated also that Serum Albumin influenced the mortality opportunity at the hazard ratio of 5.14 (95%CI:2.971-8.920) with level of confidence p-value < 0.0001. Thus, we believe that the Serum Albumin affected the mortality prediction model. We also propose two refined MELD models[4], ThaiMELD-Albumin and ThaiMELD-CTP[5]. For the efficiency assessment of the models, we compare our models to others using the ROC. We found that ThaiMELD-Albumin had 0.85 (95% CI: 0.68-1.00) and it is better than MELD, MELD-Albumin and 5vMELD, while ThaiMELD-CTP is just better than MELD. Consequently, ThaiMELD-Albumin is better for prediction of the mortality opportunity for Thai patients than the MELD, MELD-Albumin or 5vMELD. While ThaiMELD-CTP which just added a scale value to MELD could give a better assessment than MELD itself. Therefore, our model could benefit to Thai patients for the assessment of mortality opportunity as well as symptoms' severity. It could, perhaps, be further used for the consideration of liver transplantation in Thailand. © 2014 IEEE.",Child-Pugh score; Cox Regression; MELD; MELD Albumin; MELDNa; ROC; Survival Analysis,Forecasting; Human engineering; Regression analysis; Software engineering; Child-Pugh score; Cox regression; MELD; MELD Albumin; MELDNa; ROC; Survival analysis; Body fluids,Conference Paper,2-s2.0-84904569305,
tnt,False alarm reduction in continuous cardiac monitoring using 3D Accélération signals,2014,"2014 International Computer Science and Engineering Conference, ICSEC 2014",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988305177&doi=10.1109%2fICSEC.2014.6978218&partnerID=40&md5=794061ef18d73a013266c15b2b1a58e5,"In continuous cardiac monitoring through wireless Body Sensor Networks (BSNs) using ECG signals, signal quality can be deteriorated due to several factors, including, noise, low battery power and network transmission problems. Body movements occurring when a subject performs activities of daily living (ADLs) are also major causes of high false alarm rates. This paper presents a hybrid framework for false alarm reduction in continuous cardiac monitoring, where classification models constructed using machine learning algorithms are used for labeling input signals and a rule-based expert system is used for combining the classification results into make a final decision. From their extracted low-level features, ECG signal portions are labeled with heartbeat types and also signal quality levels. Meanwhile, low-level features from 3D acceleration signals are used for predicting types of activities. Taking signal quality levels and activity types into considerations, the rule-based expert system then determines whether abnormal ECG portions should trigger alarms or should be ignored. The proposed framework is validated using two datasets: one is obtained from the MIT-BIH arrhythmia database and the other is acquired from 10 subjects while they are performing ADLs. The results of the experiments demonstrate that our proposed framework can reduce false alarm rates in continuous cardiac monitoring and potentially assist physicians in diagnosing a vast amount of data acquired from wireless sensors. © 2014 IEEE.",Activity classification; Arrhythmia classification; Body sensor network; Machine learning; Rule-based expert system; Signal quality classification,Alarm systems; Algorithms; Artificial intelligence; Body sensor networks; Diseases; Electrocardiography; Errors; Expert systems; Heart; Image resolution; Learning algorithms; Learning systems; Wireless sensor networks; Activities of daily living (ADLs); Activity classifications; Arrhythmia classification; Classification results; False alarm reductions; Rule based expert systems; Signal quality; Wireless body sensor networks; Biomedical signal processing,Conference Paper,2-s2.0-84988305177,
tnt,Toward continuous ambulatory monitoring using a wearable and wireless ECG- Recording system: A study on the effects of signal quality on arrhythmia detection,2014,Bio-Medical Materials and Engineering,9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891133376&doi=10.3233%2fBME-130823&partnerID=40&md5=6c19857a456cfda5c85c9037a8392000,"Five well-known arrhythmia classification algorithms were compared in this paper based on the recommendations in AAMI standard. They are C4.5, k-Nearest Neighbor, Multilayer Perceptron, PART, and Support Vector Machine, respectively, with inputs related to heartbeat intervals and ECG morphological features. They were evaluated on three independent datasets, including the MIT-BIH arrhythmia database, a collection of ECG signals acquired from healthy subjects by the wireless Body Sensor Network (BSN) nodes, and a third dataset captured also by the BSN nodes. Results showed the overall accuracy on the MIT-BIH arrhythmia database was approximately 99.04%, with high sensitivity, specificity, and selectivity. When tested with ECG signals acquired from the human subjects, which were partially deteriorated due to several factors, e.g., motion artifacts and data transmission problems, the overall accuracy of 94.19% and that of 81.22% were obtained for static activities and dynamic activities, respectively. In addition, the effects of the signal quality from these human subjects on false alarms were investigated. When false alarms occurring in signal segments with low quality were excluded, the number of false detections reduced from 14.17% to 8.65%. When evaluated on signals generated by the patient simulator, which included several types of premature ventricular contraction without artifacts from body movements, a high classification accuracy was also observed. © 2014 - IOS Press and the authors. All rights reserved.",ambulatory monitoring; Arrhythmia classification; body sensor network; signal quality; wireless ECG,"Ambulatory monitoring; Arrhythmia classification; Classification accuracy; Multi layer perceptron; Premature ventricular contraction; Signal quality; Wireless body sensor networks; Wireless ecg; Biomedical engineering; Biotechnology; Body sensor networks; Diseases; Electrocardiography; Errors; Signal detection; adult; ambulatory monitoring; artifact; body movement; conference paper; diagnostic accuracy; disease classification; electrocardiogram; false positive result; female; heart arrhythmia; heart ventricle extrasystole; human; human experiment; k nearest neighbor; male; middle aged; normal human; perceptron; priority journal; simulator; support vector machine; wireless communication; ambulatory monitoring; Arrhythmia classification; body sensor network; signal quality; wireless ECG; Adult; Aged; Aged, 80 and over; Arrhythmias, Cardiac; Artifacts; Computer Simulation; Databases, Factual; Electrocardiography; Equipment Design; Female; Healthy Volunteers; Humans; Male; Middle Aged; Monitoring, Ambulatory; Movement; Reproducibility of Results; Signal Processing, Computer-Assisted; Support Vector Machines; Wireless Technology; Young Adult",Conference Paper,2-s2.0-84891133376,
pps,Maximum clique algorithm and its approximation for uniform test form assembly,2014,IEEE Transactions on Learning Technologies,12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904166797&doi=10.1109%2fTLT.2013.2297694&partnerID=40&md5=9a317052afc18b7aa74175dda8114166,"Educational assessments occasionally require uniform test forms for which each test form comprises a different set of items, but the forms meet equivalent test specifications (i.e., qualities indicated by test information functions based on item response theory). We propose two maximum clique algorithms (MCA) for uniform test form assembly. The proposed methods can assemble uniform test forms with allowance of overlapping items among uniform test forms. First, we propose an exact method that maximizes the number of uniform test forms from an item pool. However, the exact method presents computational cost problems. To relax those problems, we propose an approximate method that maximizes the number of uniform test forms asymptotically. Accordingly, the proposed methods can use the item pool more efficiently than traditional methods can. We demonstrate the efficiency of the proposed methods using simulated and actual data. © 2013 IEEE.",Item response theory; Maximum clique problem; Test assembly; Uniform test forms,Algorithms; Lakes; Approximate methods; Computational cost problems; Educational assessment; Item response theory; Maximum clique problems; Test assembly; Test information; Test specifications; Testing,Article,2-s2.0-84904166797,
ojs,Adaptive learning using an integration of competence model with knowledge space theory,2013,"Proceedings - 2nd IIAI International Conference on Advanced Applied Informatics, IIAI-AAI 2013",7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890109589&doi=10.1109%2fIIAI-AAI.2013.15&partnerID=40&md5=54c09dd9d1ca219265a6f4b9ec0e0bb1,"A model for the interactions in an assessment to support learning identifies the need for response options and for contingent feedback, both of which pose problems when computer-aided. Apart from the difficulties of allowing arbitrary student responses, of judging them for correctness or error, and of providing appropriate specific and contingent feedback, explicitly identifying a range of options or alternatives from which a student may make selections remains an unsolved research problem. The 'Knowledge Space Theory (KST)' model of the domain 'problems' provides some opportunity for response options. The 'Competence Based Assessment (COMBA)' model of the required knowledge provides some opportunity for relevant feedback. The paper explores ComKoS, a model which integrates both approaches. We propose to apply ComKoS and IMS QTI in Moodle to instantiate the design and development of one kind of adaptive testing system. This implementation overcomes limitations in adaptability, interoperability, portability, and reusability. Key benefits of this implementation are identified and possibilities suggestion for future work is provided. © 2013 IEEE.",competence; computer adaptive test; e-assessment; IMS QTI; knowledge space theory; Moodle,competence; Computer-adaptive tests; E assessments; IMS QTI; Knowledge space theory; Moodle; Reusability; Information science,Conference Paper,2-s2.0-84890109589,
ojs,Learning in Moodle using Competence-Based Knowledge Space Theory and IMS QTI,2013,"2013 International Computer Science and Engineering Conference, ICSEC 2013",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893532901&doi=10.1109%2fICSEC.2013.6694752&partnerID=40&md5=9ff6af69febb353810ee6569c5c78b43,"The Modular Object Oriented Developmental Learning Environment (Moodle) LMS is currently a powerful tool in the educational process, as it supports web-based courses and quizzes, along with providing tools for communication and collaboration. Moodle can be considered as the best platform to support adaptive learning. Using a quiz component is critical to providing an adequate level of adaptation in e-assessment. This paper proposes an implementation of Competence-Based Knowledge Space Theory (CbKST) within Moodle which delivers assessments using an IMS QTI-compliant application. This implementation enhances adaptability, interoperability, portability, and reusability of e-assessments. The paper explores CbKST, a framework for domain and learner knowledge representation which constitutes the basis for meaningful learning paths adapted to the learners' knowledge state. We introduce IMS QTI and its application for managing, verifying, and delivering e-assessments. The proposed system architecture is presented, and key benefits of the implementation are identified and suggestions for future work are provided. © 2013 IEEE.",competence-based knowledge space theory; computer adaptive test; e-assessment; IMS QTI; Moodle,Computer-adaptive tests; E assessments; IMS QTI; Knowledge space theory; Moodle; Computer aided instruction; Computer science; Knowledge representation; Reusability; Tools; Interoperability,Conference Paper,2-s2.0-84893532901,
wlr,Energy efficient and fairness scheduling of periodic real-time tasks for wireless embedded systems,2013,"2013 Computing, Communications and IT Applications Conference, ComComAp 2013",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881257321&doi=10.1109%2fComComAp.2013.6533619&partnerID=40&md5=adc85a75f0b7112349571c5945babe36,"One of the major challenges of computer system design is the management and conservation of energy while satisfying QoS requirements. This paper considers two methods for reducing energy consumption of the CPU and network interface card by using Dynamic Voltage Scaling (DVS) and Transmission Power Control (TPC) techniques. Moreover, energy fairness is introduced for real-time scheduling. The effectiveness of the proposed scheme is evaluated and compared to Red Task Only (RTO) and Blue When Possible algorithms (BWP) using performance metrics, such as the percentage of energy consumption, successful ratio and total successful task instances. The simulation results show the decrease of variation in energy consumption for proving the idea of fairness. In addition, the energy efficient technique for prolonging lifetime of embedded node leads to an increase in the number of successful transmission messages. © 2013 IEEE.",Dynamic Voltage Scaling; Earliest Deadline First; Energy Consumption; Quality of Service; Real-Time Scheduling; Transmission Power Control,Dynamic voltage scaling; Earliest deadline first; Energy-efficient techniques; Management and conservations; Real time scheduling; Reducing energy consumption; Transmission power control; Wireless embedded systems; Energy efficiency; Energy utilization; Power control; Quality of service; Scheduling; Voltage stabilizing circuits,Conference Paper,2-s2.0-84881257321,
wjr,Ontological approach to enhance results of business process mining and analysis,2013,Business Process Management Journal,27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878230971&doi=10.1108%2f14637151311319905&partnerID=40&md5=94965d59a72de58646bd5d9957c9827f,"Purpose: The purpose of this paper is to propose a solution to the problem of a lack of machine processable semantics in business process management. Design/methodology/approach: The paper introduces a methodology that combines domain and company-specific ontologies and databases to obtain multiple levels of abstraction for process mining and analysis. The authors valuated this approach with a real case study from the apparel domain, using a prototype system and techniques developed in the Process Mining Framework (ProM). The results of this approach are compared with similar research. Findings: Semantically enriching process execution data can successfully raise analysis from the syntactic to the semantic level, and enable multiple perspectives of analysis on business processes. Combining this approach with complementary research in semantic business process management (SBPM) can provide results comparable to multidimensional analysis in data warehouse and on line analytical processing (OLAP) technologies. Originality/value: The approach and prototype described in this paper improve the richness of semantics available for open-source process mining and analysis tools like ProM, and the richness and detail of the resulting analysis. © Emerald Group Publishing Limited.",Business process; Multi-perspective process analysis; Multidimensional analysis; Ontological approach; Ontology layers; Ontology-database mapping; Process analysis; Process mining and analysis; Semantic annotation log; Semantic business process management; Semantic enhancement; Semantic process mining and analysis; Semantics,,Article,2-s2.0-84878230971,
wdc,Creating GPU-enabled agent-based simulations using a PDES tool,2013,Advances in Intelligent Systems and Computing,4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891087540&doi=10.1007%2f978-3-319-00551-5_28&partnerID=40&md5=6d6bfd46b24877b0e09b4cd7154b4b62,"By offloading some computation to graphical processing units (GPUs), agent-based simulation (ABS) can be accelerated up to thousands of times faster. To exploit the power of GPUs, modellers can use available simulation frameworks to auto-generated GPU codes without requiring any knowledge of GPU programming languages. However, such frameworks only support computation on the GPUs of a particular vendor. This paper proposes techniques, implemented in a synchronous parallel discrete event simulation (PDES) tool, to allow modellers to create ABS models, and to specify computation regions in the models for multiple vendor's GPUs or CPUs. The technique comprises a set of meta-language tags and a compilation framework to convert user-defined GPU execution regions to OpenCL. A well-known cellular ABS models, the Conway's Game of Life, have been implemented and evaluated on two platforms i.e., the NVIDIA GeForce 240M LE and AMD Radeon HD6650M. The preliminary results demonstrate two findings: (a) the proposed technique allows the example ABS model to be executed on a PDES engine successfully; (b) the generated GPU-enabled ABS model can achieve fourteen times faster than its multicore version. © Springer International Publishing Switzerland 2013.",Acceleration; Agent-based simulation; GPU; OpenCL; PDES,Acceleration; Artificial intelligence; Discrete event simulation; Distributed computer systems; Tools; Agent based simulation; GPU; Graphical processing unit (GPUs); Multiple vendors; OpenCL; Parallel discrete event simulations; PDES; Simulation framework; Program processors,Article,2-s2.0-84891087540,
ojs,Ontology-driven automatic generation of questions from competency models,2013,Advances in Intelligent Systems and Computing,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876220246&doi=10.1007%2f978-3-642-37371-8_18&partnerID=40&md5=c01ed7fe745d2f3212a9175b1b5036a7,"The paper explores some pedagogical affordances of machine-processable competency models. Self-assessment is a crucial component of learning. However, creating effective questions is time-consuming because it may require considerable resources and the skill of critical thinking. There are very few systems currently available which generate questions automatically, and these are confined to specific domains. Using ontologies and Semantic Web technologies certain limitations in automation, integration, and reuse of data across diverse applications can be overcome. This paper presents a system for automatically generating questions from a competency framework. This novel design and implementation involves an ontological database that represents the intended learning outcome to be assessed across a number of dimensions, including the level of cognitive ability and the structure of the subject matter. This makes it possible to guide learners in developing questions for themselves, and to provide authoring templates which speed the creation of new questions for self-assessment. The system generates a list of all the questions that are possible from a given learning outcome. Such learning outcomes were collected from the INFO1013 'IT Modeling' course at the University of Southampton. The way in which the system has been designed and evaluated is discussed, along with its educational benefits. © 2013 Springer-Verlag.",adaptivity; assessment; competency modelling; knowledge representation; ontology-driven,Knowledge representation; Teaching; Adaptivity; assessment; Considerable resources; Educational benefits; Intended learning outcomes; ontology-driven; Semantic Web technology; University of Southampton; Information technology,Conference Paper,2-s2.0-84876220246,
pps,Maximum clique algorithm for uniform test forms assembly,2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880005078&doi=10.1007%2f978-3-642-39112-5_46&partnerID=40&md5=7e0861e26c10b61bc3bb862340f95c8c,"Educational assessments occasionally require uniform test forms for which each test form consists of a different set of items, but the forms meet equivalent test specifications (i.e., qualities indicated by test information functions based on item response theory). We propose two maximum clique algorithms (MCA) for uniform test forms assembly. The proposed methods can assemble uniform test forms with allowance of overlapping items among uniform test forms. First, we propose an exact method that maximizes the number of uniform test forms from an item pool. However, the exact method presents computational cost problems. To relax those problems, we propose an approximate method that maximizes the number of uniform test forms asymptotically. Accordingly, the proposed methods can use the item pool more efficiently than traditional methods can. We demonstrate the efficiency of the proposed methods using simulated and actual data. © 2013 Springer-Verlag Berlin Heidelberg.",Item response theory; Maximum clique problem; Test assembly; Uniform test forms,Artificial intelligence; Computation theory; Approximate methods; Computational cost problems; Educational assessment; Item response theory; Maximum clique problems; Test assembly; Test information; Test specifications; Testing,Conference Paper,2-s2.0-84880005078,
ssr,"APSEC 2013 tutorials, industry track, and postgraduate symposium",2013,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936872261&partnerID=40&md5=bb3f8e171b8bcdafd0f38776855d93a6,,,,Editorial,2-s2.0-84936872261,
ssr,"APSEC 2013 Tutorials, industry track, and postgraduate symposium: Message from the chairs",2013,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897425815&doi=10.1109%2fAPSEC.2013.99&partnerID=40&md5=40fb9eaf63691b24a6c827b027da0ff4,,,,Editorial,2-s2.0-84897425815,
ssr,Modeling code analyzability at method level in J2EE applications,2013,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897446260&doi=10.1109%2fAPSEC.2013.113&partnerID=40&md5=d9cdf620d4b88ead3cf5fefda70204c3,"One of the main reasons for improving code structure is to make the cause of error code easily identified. Thus, developers need an analyzability prediction model to evaluate the analyzability of code in order to locate classes to be improved. To identify classes to be improved, developers must analyze all methods of classes for finding problem methods. Therefore, analyzability prediction model must be created for calculating analyzability level of method. Currently, J2EE applications are legacy systems and need to be continually maintained. Hence, code analyzability prediction at method level in J2EE application helps developers to know which method should be improved for increasing code understanding and reducing time for finding error causes. However, there is a lack of analyzability prediction model for J2EE application and existing research works on analyzability prediction model do not focus on method level. Therefore, this paper proposes how to create analyzability prediction model at method level for J2EE applications through ordinal logistic regression. © 2013 IEEE.",Analyzability; Component; J2EE; Maintainability; Software maintenance,Computer software maintenance; Forecasting; Legacy systems; Maintainability; Software engineering; Analyzability; Code structure; Code understanding; Component; J2EE; J2EE application; Ordinal logistic regression; Prediction model; Codes (symbols),Conference Paper,2-s2.0-84897446260,
scw,Flower search by image on mobile phone,2012,"Proceedings - 2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (NISS, ICMIA and NASNIT), ISSDM 2012",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881001974&partnerID=40&md5=700a6a415e1f4bf835e10ca0ec6c364a,"We propose an image search system for flowers on a mobile phone. Mobile phones have more limited resources than desktop computers in terms of CPU, RAM and data storage. The database that we created has 45 classes. We used 182 training images and 246 test images. We used an HSV histogram as a color feature. The accuracy rate using only color features was 44.86% with radius C=20. We use SURF as a shape feature. The accuracy rate using only shape-based feature was 47.31% with SURF vectors S=25. We combine both color and shape features to achieved accuracy 61.61%. © 2012 AICIT.",Computer vision; Flower retrieval; Flower search; HSV; Image processing; Mobile phone; RGB; SIFT; Smart phone; SURF,Flower retrieval; Flower search; HSV; RGB; SIFT; SURF; Cellular telephones; Computer vision; Data mining; Image processing; Information science; Mobile phones; Personal computers; Search engines,Conference Paper,2-s2.0-84881001974,
wlr,RPL router discovery for supporting energy-efficient transmission in single-hop 6LoWPAN,2012,IEEE International Conference on Communications,10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871999842&doi=10.1109%2fICC.2012.6364812&partnerID=40&md5=411213f4ac26792f550b8a5fabda6be3,"In Wireless Sensor Networks (WSNs), controlling transmission power is a commonly used technique to extend battery life. This paper describes a novel mechanism using measured RSS (Received Signal Strength) to calculate optimal transmission power. This technique works in multipath environments and with nodes with differing transmission capability. Our technique achieves automatic configuration employing modifications to RPL (Routing Protocol for Low-power and lossy networks) router discovery without requiring extra steps or messages. Consequently, each node can send packets with ideal transmission power, which will usually be lower than maximum power and will help to prolong its lifetime. We evaluate the effectiveness of the proposed scheme, using performance metrics such as energy consumption and packet loss, on an WSN testbed. Several factors that impact the RSS, such as antenna, multipath environment, output power and the node's capabilities are also investigated. Moreover, two RSS estimation techniques are evaluated and compared to the average measured RSS. The experimental results show that energy consumption is reduced by using the proposed technique. © 2012 IEEE.",6LoWPAN; Energy Consumption; Energy-Efficient Transmission; IEEE 802.15.4; RPL; RSS; Wireless Sensor Networks,6LoWPAN; Automatic configuration; Battery life; Energy efficient; Estimation techniques; IEEE 802.15.4; Lossy networks; Low Power; Maximum power; Multipath environments; Optimal transmission; Output power; Performance metrics; Received signal strength; Router discovery; RPL; Single-hop; Transmission capability; Transmission power; Wireless sensor network (WSNs); Energy efficiency; Energy utilization; Equipment testing; Low power electronics; Multipath propagation; Sensor nodes; Standards; Wireless sensor networks; RSS,Conference Paper,2-s2.0-84871999842,
wdc,P-HASE: An efficient synchronous PDES tool for creating scalable simulations,2012,Communications in Computer and Information Science,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868285036&doi=10.1007%2f978-3-642-34387-2_27&partnerID=40&md5=59aa44bfd59d89bb5c67711aa9ab47f3,"Synchronous, parallel discrete event simulation (PDES) is the simplest and lightweight approach to speedup large-scale simulations by scheduling as many events, of the same simulation cycle, to be executed concurrently. The scheduling technique to achieve perfect load balance and scalability is a key challenge for an efficient synchronous PDES. In this paper, we proposed a technique for balancing loads to fit the number of available processors on multicores. The technique has been implemented on a synchronous PDES tool called P-HASE (the Parallel - Hierarchical computer Architecture design and Simulation Environment) using the NET 4.0 concurrency runtime and OpenMP. Eight simulation models have been evaluated on 4-, 8-, and 16- core machines. The results show that the models using P-HASEare faster than HASE for 18 - 6.5 times; and maintain their performance when changing the numbers of processors. The results confirm that the simulation models created by using the P-HASE tool are highly scalable for multicore architecture. © 2012 Springer-Verlag.",.NET; Framework; Parallel Discrete Event Simulation; PDES; Scalable Modelling; Synchronous; Task-based Parallelism,.NET; Framework; Parallel discrete event simulations; PDES; Synchronous; Task-based; .NET; Framework; Parallel discrete event simulations; PDES; Scalable Modelling; Synchronous; Task-based parallelisms; Scalable Modelling; Task-based parallelisms; Application programming interfaces (API); Discrete event simulation; Information dissemination; Scheduling; Software architecture; Application programming interfaces (API); Computer architecture; Distributed computer systems; Information dissemination; Multiprocessing systems; Parallel processing systems; Scheduling; Software architecture; Parallel processing systems; Parallel processing systems; Discrete event simulation; Tools,Conference Paper,2-s2.0-84868285036,
wdp,Conceptual framework of the Green Building Information Management System,2012,"Construction Research Congress 2012: Construction Challenges in a Flat World, Proceedings of the 2012 Construction Research Congress",4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866260984&doi=10.1061%2f9780784412329.067&partnerID=40&md5=8bf60403aab440e27eb141f15e14ab77,"This paper presents the conceptual framework by which we developed the Green Building Information Management System (GBIMS). GBIMS is a web-based client-server system designed to assist all project participants in accessing and retrieving information necessary for the planning and control of green building projects. Its schema includes two main components: project life cycle and information. The life cycle of a green building project was broken down into the three hierarchical levels of stages, processes, and activities using an adaptation of the OmniClass Construction Classification System. Important information concerning green building project management was compiled from the LEED rating system, including credits, project participants and their roles, certification documents, design and construction standards, and green materials. The design of GBIMS follows a three-tiered software architecture, which consists of three independent modules, namely, the user-interface module, the functional process logic module, and the data access and storage module. The relations among objects (entities) in GBIMS are illustrated through a class diagram. The GBIMS conceptual framework provides the foundation for the development of a comprehensive web-based green building project management system. © 2012 ASCE.",,Broken down; Class diagrams; Classification system; Client-server systems; Conceptual frameworks; Data access; Design and construction; Green building projects; Green buildings; Green materials; Hierarchical level; Information concerning; Information management systems; LEED rating system; Planning and control; Process logic; Project life cycle; Project participants; Storage modules; Buildings; Life cycle; Logic devices; Project management; Research; User interfaces; Websites; Information management,Conference Paper,2-s2.0-84866260984,
wdc,Griffon - GPU programming APIs for scientific and general purpose computing (Extended version),2012,International Journal of Artificial Intelligence,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863606046&partnerID=40&md5=068c505276903be5dd7f21776783bc60,"Applications can accelerate up to hundreds of times faster by offloading some computation from CPU to execute at graphical processing units (GPUs). This technique is so called the generalpurpose computation on graphic processing units (GPGPUs). Recent research on accelerating various applications by GPGPUs using a programming model from NVIDIA, called Compute Unified Device Architecture (CUDA), have shown significant improvement on performance results. However, writing an efficient CUDA program requires in-depth understanding of GPU architecture in order to develop a suitable data-parallel strategy, and to express it in a low-level style of code. Thus, CUDA programming is still considered complex and error-prone. This paper proposes a new set of application program interfaces (APIs), called Griffon, and its compiler framework for automatic translation of C programs to CUDA-based programs. Griffon APIs allow programmers to exploit the performance of multicore machines using OpenMP and offloads computations to GPUs using Griffon directives. The compiler framework uses a new graph algorithm for efficiently exploiting data locality. Experimental results on a 16-core NVIDIA Geforce 8400M GS using six workloads show that Griffon-based programs can accelerate from 1.5 up to 89 times faster than their sequential implementation running on CPU. © 2012 by IJAI (CESER Publications).",Accelerating Computing; Automatic translation; CUDA; GPU; Parallel Programming,,Article,2-s2.0-84863606046,
tnt,Towards continuous electrocardiogram monitoring based on rules and ontologies,2011,"Proceedings - 2011 11th IEEE International Conference on Bioinformatics and Bioengineering, BIBE 2011",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155176955&doi=10.1109%2fBIBE.2011.61&partnerID=40&md5=0d3abf4efdeeb6b7a91439fac8de23db,"Based on rules and ontologies, this paper proposes a framework for predicting types of arrhythmia from electro-cardiogram (ECG) signals acquired using a BSN node. Using terms in an ECG signal ontology, ECG signals are annotated by locating the positions of elementary waves, including their onset, offset, and peak positions. Rules are used for extracting features, e.g., heart rate, PR intervals, RR intervals, and QRS intervals, from annotated signals. An arrhythmia indicator ontology is constructed in order to define concepts representing different characteristics of ECG waveforms, which are then used for defining necessary and sufficient conditions for arrhythmia classification of signal portions. Using standard semantic web ontology and rule languages, i.e., OWL and SWRL, for rule and ontology representation, knowledge content in this framework can be integrated with other existing knowledge sources for retrieval of related information, e.g., recommended treatment. © 2011 IEEE.",arrhythmia classification; electrocardiogram; knowledge-based system; ontology,Arrhythmia classification; ECG signals; ECG waveforms; electrocardiogram; Elementary waves; Extracting features; Heart rates; Knowledge content; Knowledge sources; Peak position; PR intervals; Predicting types; RR intervals; Semantic Web ontologies; Sufficient conditions; Bioinformatics; Diseases; Electrocardiography; Knowledge based systems; Knowledge representation; Semantic Web; Ontology,Conference Paper,2-s2.0-84155176955,
wjr,Enhancing decision patterns dicovered by process mining with semantic related data,2011,Proceedings of the International Conference on Electronic Business (ICEB),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436119&partnerID=40&md5=6286c25a4c5e2cdb888ef6b93987818e,"Business processes can be automatic, semiautomatic or manual processes. Semi-automatic and manual processes are involved in some parts by people. Understanding how people work or make judgments in processes can help management to evaluate their performance and suggest essential information to enhance their decision making. This paper describes a case study of using process mining to discover decision patterns of a worker in a semi-automatic business process. It was found that the discovered rules could be improved by enhancing the business execution log file with semantic related data. The experimental results before and after improvements were compared.",Decision making; Decision pattern; Log enhancing; Process mining; Semantic related attribute,Business Process; Decision pattern; In-process; Log enhancing; Log file; Manual process; Process mining; Semi-automatics; Automation; Data mining; Electronic commerce; Semantics; Decision making,Conference Paper,2-s2.0-84873436119,
lpp,Influence of age group differences on website cultural usability,2011,International Conference on ICT and Knowledge Engineering,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858775417&doi=10.1109%2fICTKE.2012.6152413&partnerID=40&md5=c6f765797c0c54f4b1b6f17e7facaa77,"Research on culture and web usability has highlighted influences of culture on web usability. However, the issue of age group differences has been neglected, as participants were mostly university students which represent only one age group. People of different age groups are different, not only in their physical attributes, but also in their cognitive and socioemotional attributes. This paper investigated the influence of age group differences on usability of culturally accommodated websites. Research participants were tested with three different web prototypes. Two of them represented two different cultures, while another one was designed to be neutral to the other two cultures. The results were evaluated both subjectively and objectively. The results indicate that there were interaction effects between age group and culture, as represented in each prototype, on web usability was significantly different. In conclusion, interpretations of interaction effects are discussed. © 2011 IEEE.",age group differences; Cultural usability; culture; human factors; individual differences; usability; website localization; website usability,Age groups; Cultural usability; Individual Differences; usability; website usability; Cell culture; Human engineering; Knowledge engineering; Websites; Usability engineering,Conference Paper,2-s2.0-84858775417,
ojs,Preface of work-in-progress posters,2011,"Proceedings of the 19th International Conference on Computers in Education, ICCE 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860480145&partnerID=40&md5=63820b333380773f2aaeb7d460ab7292,,,,Editorial,2-s2.0-84860480145,
ppr,Mode transition for online scheduling of adaptive real-time systems on multiprocessors,2011,"Proceedings - 17th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications, RTCSA 2011",7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855530400&doi=10.1109%2fRTCSA.2011.71&partnerID=40&md5=3a80b83f2ea8b97bf47198cc4ac49c81,"This paper presents a novel online scheduling algorithm for scheduling real-time adaptive systems in which tasks may have distinct resource requirements for each of the systems' operating modes. Apart from prior work that considers only step-wise adaptation of tasks' resource utilization during mode transition, the proposed algorithm (named EAGLE-T) enables tasks to adapt their resource utilization progressively from one mode to another in a timely manner without causing any deadline miss. The upper bound of the delay and the drift between resource utilization achieved by EAGLE-T and the ideal scheduler during mode transition are provided. Performance evaluation shows that the progressive adaptation of EAGLE-T offers improved performance over a step-wise approach (average maximal-utilization drift and mode-transition delay are reduced by up to 68.75% and 32.16%, respectively). As the probability of a mode change or the number of tasks vary, empirical results show that the resource utilization achieved by tasks scheduled using EAGLE-T is within 56% to 90% of the desired utilization (compared to 11%-81% when the step-wise scheme is used). © 2011 IEEE.",Adaptive task systems; Mode transition; Multi-mode applications; Real-time scheduling,Empirical results; Mode transitions; Multimodes; Online scheduling; Online scheduling algorithm; Operating modes; Performance evaluation; Real time scheduling; Resource requirements; Resource utilizations; Upper Bound; Adaptive systems; Embedded systems; Online systems; Scheduling algorithms; Real time systems,Conference Paper,2-s2.0-84855530400,
ssr,Identifying refactoring through formal model based on data flow graph,2011,"2011 5th Malaysian Conference in Software Engineering, MySEC 2011",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857321424&doi=10.1109%2fMySEC.2011.6140653&partnerID=40&md5=50f02716e5b9217f73c734184e55ee55,"Identifying refactoring is an important step of bad smell removal process. Many developers have problem identifying refactoring, e.g., novice developers are not sure which refactorings should be applied. Even skilled developers may need to spend a lot of time doing this manually. Identifying refactoring techniques can alleviate these problems for developers. In our previous work, we proposed refactoring filtering conditions (RFC) that identify candidate refactorings for removing long method bad smell. These conditions analyze internal structure inside a method with data flow analysis. Our research improves these RFC based on more refined data flow analysis and presents its formal description. This research also demonstrates an approach of using the improved RFC of three refactorings. Moreover, we conducted an experiment to compare the efficiency of the results of our approach with that of our previous work and that of developer suggestion. © 2011 IEEE.",bad smell; maintainability; refactoring; software maintenance,bad smell; Formal Description; Formal model; Internal structure; Refactorings; Removal process; Computer software maintenance; Data flow analysis; Data flow graphs; Maintainability; Software engineering,Conference Paper,2-s2.0-84857321424,
ojs,Cognitive assessment applying with item response theory,2011,"Proceedings of the 19th International Conference on Computers in Education, ICCE 2011",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860487978&partnerID=40&md5=b44c31644adf428705906d892e3da7ca,Using item response theory (IRT) in computer adaptive test (CAT) is critical to assess learners' readiness for further learning. This theory has assumptions concerning the mathematical relationship between learner's abilities and item responses. A numerical value from the theory has potential to decide who the best learner is. The IRT score only provides the learner's result and lack of the assessment of learner's ability level in cognitive skills. This paper proposes an approach of using Bloom's cognitive skills for evaluating learned ability level by considering achieved the level of difficulty and the IRT score. Our starting point in the adaptive test is at a question of learners' knowledge level difficulty that we got from their pre-test scores instead of starting with a question of a medium difficulty. This approach provides the effectiveness of CAT to test cognitive skills and offers some theoretical considerations on linking learning outcomes and assessments.,Cognitive skill; Computer adaptive test; Item response theory,Cognitive skill; Computer-adaptive tests; Item response theory; Knowledge level; Learning outcome; Level of difficulties; Mathematical relationship; Numerical values,Conference Paper,2-s2.0-84860487978,
pps,Detecting redundant items in construction of multiple equivalent test forms using latent Dirichlet allocation,2011,"Proceedings of the 19th International Conference on Computers in Education, ICCE 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860437637&partnerID=40&md5=0e2166fc60fd40b50bd42f3515966129,"We propose an automatic construction method of multiple equivalent test forms indicated by test information function, and the method reduces the probabilities of selecting redundant items to the same test form. In previous studies, although their methods minimized the different between the test information functions of the constructed test forms, they neglected the content similarities of selected items in the same test form. Therefore, the content similar items have probabilities to be selected into the same test form. This affects the test reliability. The main idea of this paper is to reduce the probabilities by applying a latent Dirichlet allocation in the test construction method to detect the content similarities between the selected items and the remaining items in the item banks.",E-testing; Latent Dirichlet allocation; Multiple test forms; Test construction,Automatic construction methods; Construction method; Content similarity; E-testing; Item bank; Latent Dirichlet allocation; Multiple test; Test information; Test reliability; Probability; Statistics,Conference Paper,2-s2.0-84860437637,
ksc,Thread-based live checkpointing of virtual machines,2011,"Proceedings - 2011 IEEE International Symposium on Network Computing and Applications, NCA 2011",7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055012851&doi=10.1109%2fNCA.2011.28&partnerID=40&md5=462cc436fbba08ad07c77333dd3f9575,"Virtual machine checkpointing is the mechanism to save virtual machine state to a file for later recovery. Traditional checkpointing mechanisms can suffer a long delay and cause a long disruption of services since they have to stop virtual machines to save state, which could be large. In this study, a novel Thread-based Live Checkpointing (TLC) mechanism is proposed. This mechanism leverages the pre-copy live migration mechanism introducing a checkpoint thread, which is responsible for the majority of the checkpointing activities. While the checkpoint thread is saving the virtual machine state to persistent storage, the virtual machine thread is allowed to progress with normal execution. However, the virtual machine thread will be periodically interrupted to incrementally copy dirty memory pages to a hash table. The interruptions will occur until the final stage of checkpointing is reached. This approach is implemented in KVM and its performance evaluations are conducted using NAS parallel benchmarks. Experiments show that this approach can provide high levels of virtual machine responsiveness during checkpointing. It can also reduce the checkpointing overheads to as low as 0.53 times of that of the traditional approach, when operating on a virtual machine running memory intensive workloads. © 2011 IEEE.",Checkpointing; Virtual machine; Virtualization,Check pointing; Hash table; Live migrations; Memory pages; NAS parallel benchmarks; Performance evaluation; Persistent storage; Virtual machines; Virtualizations; Computer simulation,Conference Paper,2-s2.0-80055012851,
pkl,Multi-objective traffic grooming for survivable network design,2011,"2011 IEEE International Conference on Quality and Reliability, ICQR 2011",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054763195&doi=10.1109%2fICQR.2011.6031685&partnerID=40&md5=7ecbd8589d8033c4004cc557d9ffe53a,"In this paper, we consider reliable traffic grooming in optical network design with multiple objectives. The objectives are to maximize average network reliability/availability and to minimize the number of switching ports required in optical networks. Maximizing the average connection availability of all traffic demands normally requires a large amount of resources (i.e., switching ports). In contrast, minimizing the number of switching ports could block some traffic demands from having backup paths. Without backup paths and spare resources for user traffics, the network reliability/availability cannot be improved. In our simulation experiments, we implement and compare two traffic grooming techniques (MST and MRU). We also apply a path restoration technique to protect traffic demands against network failures. To solve the multi-objective optimization problem, we apply a multi-objective evolutionary algorithm (i.e., NSGA-II) for searching a set of non-dominated solutions. Our experiment results show that the non-dominate solutions from MRU provide superior results than those of MST grooming approach. © 2011 IEEE.",evolutionary algorithm; Multi-objective optimization; reliable network design; traffic grooming,Backup path; Connection availability; Multi objective; Multi objective evolutionary algorithms; Multi-objective optimization problem; Multiple objectives; Network failure; Network reliability; Nondominated solutions; NSGA-II; Optical network design; Path restoration; reliable network design; Simulation experiments; Survivable network design; Switching ports; Traffic demands; traffic grooming; User traffics; Design; Evolutionary algorithms; Experiments; Fiber optic networks; Multiobjective optimization,Conference Paper,2-s2.0-80054763195,
wdc,Griffon-GPU programming apis for scientific and general purpose computing,2011,Advances in Intelligent and Soft Computing,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052974536&doi=10.1007%2f978-3-642-19934-9_22&partnerID=40&md5=d9b49282273fc90cef8593359e05777f,"Applications can accelerates up to hundreds of times faster by offloading some computation from CPU to execute at graphical processing units (GPUs). This technique is so called the general-purpose computation on graphic processing units (GPGPUs). Recent research on accelerating various applications by GPGPUs using a programming model from NVIDIA, called Compute Unified Device Architecture (CUDA), have shown significant improvement on performance results. However, writing an efficient CUDA program requires in-depth understanding of GPU architecture in order to develop a suitable data-parallel strategy, and to express it in a low-level style of code. Thus, CUDA programming is still considered complex and error-prone. This paper proposes a new set of application program interfaces (APIs), called Griffon, and its compiler framework for automatic translation of C programs to CUDA-based programs. The Griffon APIs allow programmers to exploit the performance of multicore machines using OpenMP and offloads computations to GPUs using Griffon directives. The Griffon compiler framework uses a new graph algorithm for efficiently exploiting data locality. Experimental results on a 16-core NVIDIA Geforce 8400M GS using four workloads show that Griffon-based programs can accelerate up to 89 times faster than their sequential implementation. © 2011 Springer-Verlag Berlin Heidelberg.",Accelerating Computing; Automatic translation; CUDA; GPU; Parallel Programming,Accelerating Computing; Application program interfaces; Automatic translation; C programs; Compute unified device architectures; CUDA; Data locality; Data parallel; Error prones; General-purpose computing; GPU; Graph algorithms; Graphic processing units; Graphical processing units; In-depth understanding; Multi-core machines; Programming models; Sequential implementation; Application programming interfaces (API); Artificial intelligence; C (programming language); Computer graphics equipment; Distributed computer systems; Parallel architectures; Parallel programming; Program translators; Program compilers,Conference Paper,2-s2.0-80052974536,
ppr,Improved real-time scheduling for periodic tasks on multiprocessors,2011,"Proceedings of the 2011 International Conference on High Performance Computing and Simulation, HPCS 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053042981&doi=10.1109%2fHPCSim.2011.5999810&partnerID=40&md5=307842fdf895ffc0214aae7b9718ab23,"Due to increasing numbers of real-time high-performance applications like control systems, autonomous robots, financial systems, scheduling these real-time applications on HPC resources has become an important problem. This paper presents a novel real-time multiprocessor scheduling algorithm, called Notional Approximation for Balancing Load Residues (NABLR), which heuristically selects tasks for execution by taking into account their residual loads and laxities. The NABLR schedule is created by considering a sequence of inter-arrival intervals (IAI) between two consecutive job arrivals of any task and using a heuristic to carefully plan task execution to fully utilize available resources in each of these intervals and avoid deadline misses as much as possible. Performance evaluation shows that NABLR outperforms previously known efficient algorithms (i.e. EDF and EDZL) in successfully scheduling sets of tasks in which total utilization of each task set equals available resource capacity, performing the closest to an optimal algorithm such as LLREF and Pfair. Out of 2500 randomly selected high-utilization task sets, NABLR can schedule up to 97.9% of the sets versus 63.2% by the best known efficient algorithm. In addition, the overheads of NABLR schedule are significantly smaller than those of optimal schedules (on average 80.57% fewer preemptions, migrations and 75.52% fewer scheduler invocations than those of LLREF) and comparably efficient to those of suboptimal schedules (fewer or nearly the same number of invocations as EDZL and ASEDZL, but within only 0.12% more preemptions/migrations than ASEDZL). NABLR has the same O(NlogN) time complexity as other previously proposed efficient. © 2011 IEEE.",Global Multiprocessor Scheduling; Real-time Applications; Resource Allocation; Sharing and Management,Efficient algorithm; Financial system; High performance applications; Multi processor scheduling; Optimal algorithm; Optimal schedule; Performance evaluation; Periodic tasks; Real time scheduling; Real-time application; Residual load; Resource capacity; Task executions; Time complexity; Approximation algorithms; Computer software selection and evaluation; Multiprocessing systems; Optimization; Resource allocation; Response time (computer systems); Scheduling algorithms; Real time systems,Conference Paper,2-s2.0-80053042981,
ssr,Using software metrics to select refactoring for long method bad smell,2011,"ECTI-CON 2011 - 8th Electrical Engineering/ Electronics, Computer, Telecommunications and Information Technology (ECTI) Association of Thailand - Conference 2011",15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961238598&doi=10.1109%2fECTICON.2011.5947882&partnerID=40&md5=be0840156e5691c2601e798ef1cb43ec,"Refactoring is a technique for improving software structure without changing its behavior which can be used to remove bad smells and increases software maintainability. But only few approaches have been proposed to address the identification of appropriate refactorings. Specifically, our research proposes a method to select refactoring based on software metrics which are defined in terms of data flow and control flow graphs. The method consist of 4 steps: 1) calculate metrics, 2) find candidate refactoring by using refactoring filtering condition (RFC), 3) apply a suite of candidate refactorings and compute maintainability, and 4) identify the refactoring that gives the highest maintainability. We demonstrate out approach by giving an example of removing a long method bad smell in a customer class in a movie rental system. Our approach proves to be able to suggest an appropriate set of refactoring techniques such as extract method, replace temp with query, and decompose condition, to solve the long method bad smell. © 2011 IEEE.",Bad Smell; Long Method; Refactoring Identification; Software Maintainability; Software Metrics,Bad Smell; Long Method; Refactorings; Software Maintainability; Software Metrics; Data flow analysis; Information technology; Maintainability; Telecommunication; Computer software,Conference Paper,2-s2.0-79961238598,
ppr,Towards closed-loop brain-machine experiments across wide-area networks,2011,"2011 5th International IEEE/EMBS Conference on Neural Engineering, NER 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960373532&doi=10.1109%2fNER.2011.5910584&partnerID=40&md5=bd339d50e4ea5f841d35cd0ddbb14b9a,"Experiments for the online closed-loop control of neural prosthetics require feedback within 100ms. In a typical neurophysiology laboratory with local computing machines, a majority of this time is spent on acquiring and analyzing the neural signals and a minority (i.e. less than a millisecond) is actual data transfer among machines on local- or campus-area networks. However, the local computing machines may not offer the computational resources necessary for running complex algorithms or scenarios that have been recently proposed. While scientists can take advantage of remote computing resource providers, wide-area networks present much larger latencies that can affect an online experiment. This work presents a split modeling approach that allows the execution of a controller on the neurophysiology resource and the execution of computationally intensive modeling and adaptation algorithms on a remote datacenter, even with the inevitable network latency. Simulation results are presented to quantify how the accuracy of the controller is affected by the split modeling approach in the presence of delays, and to demonstrate that scientists can take advantage of remotely available massive resources. © 2011 IEEE.",,Adaptation algorithms; Closed-loop; Closed-loop control; Complex algorithms; Computational resources; Computing machines; Modeling approach; Network latencies; Neural prosthetic; Neural signals; Online experiment; Remote computing; Simulation result; Algorithms; Computers; Data transfer; Experiments; Neural networks; Neurophysiology; Wide area networks; Local area networks,Conference Paper,2-s2.0-79960373532,
pps,Bees algorithm for construction of multiple test forms in E-testing,2011,IEEE Transactions on Learning Technologies,23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052643936&doi=10.1109%2fTLT.2010.29&partnerID=40&md5=b5f89d8cf7a2006add5226d6b185ab6f,"The purpose of this research is to automatically construct multiple equivalent test forms that have equivalent qualities indicated by test information functions based on item response theory. There has been a trade-off in previous studies between the computational costs and the equivalent qualities of test forms. To alleviate this problem, we propose an automated system of test construction based on the Bees Algorithm in parallel computing. We demonstrate the effectiveness of the proposed system through various experiments. © 2011 IEEE.",Bees algorithm; E-testing; multiple test forms; parallel computing.; test construction,Automated systems; Bees algorithms; Computational costs; E-testing; Item Response Theory; Multiple test; test construction; Test information; Algorithms; Automation; Parallel architectures; Testing,Article,2-s2.0-80052643936,
wlr,Neighbour discovery for transmit power adjustment in IEEE 802.15.4 using RSSI,2011,"2011 4th IFIP International Conference on New Technologies, Mobility and Security, NTMS 2011 - Proceedings",11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952852578&doi=10.1109%2fNTMS.2011.5720664&partnerID=40&md5=8fa9ee72ad41dedd28a37e74df802bcc,"Neighbour discovery (ND) is an important process for self-configuration as it provides useful information, such as neighbour nodes with some power saving parameters, for other processes. In wireless sensor networks, transmission power adjustment is one of the possible ways to minimise energy consumption and prolong network lifetime. This paper proposes the ND technique which provides the received signal strength indicator (RSSI) value for dynamic transmit power adjustment in IEEE 802.15.4. This work focuses on a single-hop network of wireless devices with different maximum transmission powers, where an asymmetric wireless link is very common. The energy consumption and packet loss rate are investigated through testbed experimentation. The proposed algorithm shows transmission efficiency which does not affect the packet loss rate. Moreover, it supports both environment change and the change of distance between nodes.",Energy consumption; IEEE 802.15.4; Neighbour discovery; Power adjustment; RSSI; Wireless sensor networks,Energy consumption; IEEE 802.15.4; Neighbour discovery; Power adjustment; RSSI; Algorithms; Energy utilization; Packet loss; Sensors; Wireless sensor networks,Conference Paper,2-s2.0-79952852578,
yao,Detecting and classifying mutations in genetic code with an application to β-thalassaemia,2011,ScienceAsia,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955548374&doi=10.2306%2fscienceasial513-1874.2011.37.051&partnerID=40&md5=70bf60581fb28f69593047be0189145a,"β-thalassaemia is a common disease in peoples of the Northern and Northeastern regions of Thailand. The general causes of the disease are mutations in the β-globin gene. In this paper we discuss methods based on finite-state automata theory, regular expressions, and partially ordered sets that can be used for detecting and classifying mutations in genes. The methods are applied to three main problems. The first problem is the reliable detection in a given β-globin gene of mutations that are known to cause β-thalassaemia. It is shown that for most known mutations in Thailand a 5-base pattern is an optimal size search pattern for reliably detecting if that mutation is or is not present in a given β-globin gene. The second problem is to find all differences between a standard normal β-globin gene and a suspected abnormal gene, to identify any differences as point mutations or frameshift mutations, and to list important biochemical effects of the mutations. The third problem is to list the types of bases in a given gene pattern as purines or pyrimidines and the types of codons as weak (4 hydrogen bonds), mixed (5 hydrogen bonds), or strong (6 hydrogen bonds). Fast and easy-to-use Matlab programs have been developed for each of these three problems. The programs could be useful when large commercial packages are not available.",Binary codes; Finite-state automata; Frameshifts; Point mutations; Regular expressions,,Article,2-s2.0-79955548374,
pkl,Solving multi-objective routing and wavelength assignment in WDM network using hybrid evolutionary computation approach,2010,Computer Communications,20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049385186&doi=10.1016%2fj.comcom.2010.07.029&partnerID=40&md5=39ef69fafb5e1c57b34d5877eeec6c03,"Routing and wavelength assignment (RWA) is a well-known issue in wavelength division multiplexing (WDM) optical networks. In this paper, we present RWA design for WDM networks by considering multiple design objectives which are maximizing the number of traffic demands to be served and minimizing the number of wavelength channels to be assigned. A hybrid evolutionary computation approach consisting of genetic algorithm for routing allocation with minimum degree first for wavelength assignment (GA-MDF) and the fast non-dominated sorting genetic algorithm (NSGA-II) to search for non-dominated solutions is applied for solving the multi-objective RWA network design problem. The hybrid evolutionary algorithm is used as a meta-heuristic technique for obtaining good solutions for various problem sizes. The obtained results are provided as candidate choices or non-dominated front. We compare the simulation results obtained from the NSGA-II with those obtained from the traditional Weighted-Sum approach. Numerical results show that our hybrid evolutionary computation approach is effective in solving the multi-objective RWA problem. The GA-MDF can outperform the FAR-FF method in both design objectives. In addition, the solutions from the NSGA-II are more diverse on the multi-objective space than those of the Weighted-Sum method. We also apply a Pruned mechanism to help cutting off numerous non-dominated solutions for making decision on the final solution. © 2010 Elsevier B.V. All rights reserved.",Multi-objective genetic algorithm; Routing and wavelength assignment; WDM network,Cutting-off; Design objectives; Evolutionary computations; Hybrid evolutionary algorithm; Making decision; Meta-heuristic techniques; Minimum degree; Multi objective; Multi-objective genetic algorithm; Network design problems; Non-dominated sorting genetic algorithms; Nondominated solutions; NSGA-II; Numerical results; Optical networks; Problem size; Routing and wavelength assignment; RWA problem; Simulation result; Traffic demands; Wavelength assignment; Wavelength channels; WDM networks; Weighted-sum; Calculations; Design; Fiber optic networks; Heuristic methods; Multiobjective optimization; Multiplexing equipment; Wavelength; Wavelength division multiplexing; Genetic algorithms,Article,2-s2.0-78049385186,
ojs,Interoperable assessment based on competency modelling,2010,Handbook of Research on E-Learning Standards and Interoperability: Frameworks and Issues,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901511697&doi=10.4018%2f978-1-61692-789-9.ch002&partnerID=40&md5=a52c0f001a2b27ea468e0b8aa415053f,"The aim of this chapter is to illustrate some affordances of machine-processable competency statements. Such competency statements are supported by ontologies and taxonomies of competency. Machine processing can offer interoperable and reusable resources and applications that are pedagogically effective for e-learning and assessment. A competency statement which can be read, processed, and interpreted by machine contributes to the automatic generation of questions and offers a semantic structure using the Web Ontology Language (OWL) to express competencies for further processing. The generated questions are expressed in the IMS Question and Test Interoperability specification (IMS QTI) to enable interoperability. © 2011, IGI Global.",,,Book Chapter,2-s2.0-84901511697,
ojs,An evaluation of generated question sequences based on competency modelling,2010,"Proceedings of the 18th International Conference on Computers in Education: Enhancing and Sustaining New Knowledge Through the Use of Digital Technology in Education, ICCE 2010",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856945754&partnerID=40&md5=6863c42dba94a7d586a437726d837129,"In order to support lifelong learning, assessment systems have to focus on representation and updating a variety of knowledge domains, rules, assessments and learner's competency profiles. Adaptive assessment provides efficient and personalised routes to establishing the proficiencies of learners. Existing adaptive assessment systems are faced the challenge of dealing with inconsistently measuring and representing student's knowledge. We can envisage a future in which learners are able to maintain and expose their competency profile to multiple services, throughout their life, which will use the competency information in the model to personalise assessment. This paper presents an adaptive assessment system based on a competency model. The system automatically generates questions from a competency framework and sequence the questions based on the taxonomies of subject matter or of capability, making it possible to guide learners in developing questions and testing knowledge for themselves. The questions and their sequencing are constructed from a given set of learning outcomes and the subject matter recorded in an ontological database. The architecture of the system and the mechanism of sequencing the questions are discussed.",Adaptive assessment; Competency; IMS QTI; Ontology,Adaptive assessment; Assessment system; Competency; Competency model; Guide learners; IMS QTI; Knowledge domains; Learning outcome; Life long learning; Multiple services; Ontological database; Subject matters; Testing knowledge; Education; Education computing; Knowledge representation; Ontology; Rating; Learning systems,Conference Paper,2-s2.0-84856945754,
pps,Multiple test forms construction based on bees algorithm,2010,Educational Data Mining 2010 - 3rd International Conference on Educational Data Mining,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857489537&partnerID=40&md5=169f36343f1df19c1af545180c68f7a8,This paper proposes a new construction method of multiple test forms that applies a Bees Algorithm and a parallel computation technique to improve the computational costs of the traditional methods.,,Bees algorithms; Computational costs; Construction method; Multiple test; Parallel Computation; Algorithms; Data mining,Conference Paper,2-s2.0-84857489537,
skn,A lazy processing approach to user relevance feedback for content-based image retrieval,2010,"Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010",8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951755954&doi=10.1109%2fISM.2010.58&partnerID=40&md5=e7738230fd02da9bd2fa204631c2dc52,"User Relevance feedback techniques based on learning methods such as Artificial Neural Networks and kernel machines have been widely used in content-based image retrieval. However, the traditional relevance feedback framework for existing techniques still suffers from: (1) high learning cost incurs substantial delay in responding to user relevance feedback; (2) the classifiers may be biased when the negative feedback samples out-number the positive feedback samples; and (3) The high feature dimensions compared to the size of the training set causes over fitting. We propose a new relevance feedback approach based on a lazy processing framework. This approach combines random sampling, data clustering, and ensembles of local classifiers to address the aforementioned problems. Our experimental studies show that the proposed framework provides a responsive user feedback environment that is capable of outperforming the traditional approach. © 2010 IEEE.",Content-based image retrieval; Machine learning; Relevance feedback,Artificial Neural Network; Content based image retrieval; Data clustering; Experimental studies; Feature dimensions; Kernel machine; Learning costs; Learning methods; Local classifier; Machine-learning; Negative feedback; Overfitting; Positive feedback; Processing approach; Random sampling; Relevance feedback; Substantial delays; Training sets; User feedback; User relevance feedbacks; Classification (of information); Classifiers; Clustering algorithms; Content based retrieval; Learning systems; Neural networks; Feedback,Conference Paper,2-s2.0-79951755954,
ppr,"Model development, testing and experimentation in a CyberWorkstation for Brain-Machine Interface research",2010,"2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650840278&doi=10.1109%2fIEMBS.2010.5626234&partnerID=40&md5=4bfd6e9af77dcfedbef3b169b69f9139,"The CyberWorkstation (CW) is an advanced cyber-infrastructure for Brain-Machine Interface (BMI) research. It allows the development, configuration and execution of BMI computational models using highperformance computing resources. The CW's concept is implemented using a software structure in which an ""experiment engine"" is used to coordinate all software modules needed to capture, communicate and process brain signals and motor-control commands. A generic BMI-model template, which specifies a common interface to the CW's experiment engine, and a common communication protocol enable easy addition, removal or replacement of models without disrupting system operation. This paper reviews the essential components of the CW and shows how templates can facilitate the processes of BMI model development, testing and incorporation into the CW. It also discusses the ongoing work towards making this process infrastructure independent. © 2010 IEEE.",,Brain machine interface; Brain signals; Communication protocols; Computational model; Cyber infrastructures; Essential component; High-performance computing resources; Model development; Model templates; Motor control; Software modules; Software structures; System operation; Communication; Experiments; brain; computer; computer program; computer simulation; human; man machine interaction; physiology; review; Brain; Computer Simulation; Computers; Humans; Man-Machine Systems; Software,Conference Paper,2-s2.0-78650840278,
pps,Computerized adaptive testing based on decision tree,2010,"Proceedings - 10th IEEE International Conference on Advanced Learning Technologies, ICALT 2010",10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049245905&doi=10.1109%2fICALT.2010.58&partnerID=40&md5=1f3c97da1f47631513dd19af11760b81,"This paper proposes a new computerized adaptive testing employing a decision tree model, instead of test theories. The attribute variable of the model is examinees' responses to each item and the output variable is examinees' test total scores. Some simulation experiments show better performances of the proposed method compared to the traditional methods and solve the problems. © 2010 IEEE.",Computerized adaptive testing; E-Testing; Educational assessment; Item response theory,Computerized adaptive testing; Decision tree models; E-testing; Educational assessment; Item Response Theory; Output variables; Simulation experiments; Education; Decision trees,Conference Paper,2-s2.0-78049245905,
pkl,Path level traffic grooming strategies for multi-objective design in WDM networks,2010,"ECTI-CON 2010 - The 2010 ECTI International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954898367&partnerID=40&md5=2747605afa0eafa8689ff5dfd5f90c47,"In this paper, we develop a multi-objective network design model for the traffic grooming, routing and wavelength assignment (GRWA) in WDM networks. We consider and evaluate four traffic grooming strategies which are Point to Point (P2P), Point to Multi-Point (P2MP), Multi-Point to Point (MP2P) and Multi-Point to Multi-Point (MP2MP). The design objectives are to maximize the number of accepted communication requests (source-destination pairs) as well as to minimize the number of wavelength channel requirement. Both the design objectives are conflicted to each other; maximizing the number of accepted commodities will require a large number of wavelength channels while minimizing the number of wavelength channels will limit the amount of accepted commodities. To solve the multi-objective network design problem, we apply a fast and efficient optimization technique called ""Fast Non-dominated Sorting Genetic Algorithm (NSGA-II)"". The simulation results show that traffic grooming with multiple sources/destinations are the most flexible and efficient grooming techniques that give wide-spread solutions on the objective space, than those solutions obtained from the P2P grooming technique in both objective values (i.e., number of accepted commodities and wavelength channels required).",,Design objectives; Multi objective; Multi-objective design; Network design; Network design problems; Non-dominated sorting genetic algorithms; NSGA-II; Objective space; Optimization techniques; Path level; Point to point; Routing and wavelength assignment; Simulation result; Source-destination pairs; Traffic Grooming; Wavelength channels; WDM networks; Design; Information technology; Wavelength division multiplexing; Multiobjective optimization,Conference Paper,2-s2.0-77954898367,
pkl,Multi-objective traffic grooming in WDM network using NSGA-II approach,2010,"INC2010 - The International Conference on Networked Computing, Proceeding",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954768238&partnerID=40&md5=98d802c7d486ceb604bbf8f43be509aa,"This paper considers a multi-objective network design problem for the traffic grooming, routing and wavelength assignment (GRWA) in WDM networks. The design objectives are to maximize the number of accepted communication requests (source-destination pairs) as well as to minimize the number of wavelength channel requirement. Both the design objectives are conflicted to each other; maximizing the number of accepted commodities will require a large number of wavelength channels while minimizing the number of wavelength channels will limit the amount of accepted commodities. To solve the multi-objective network design problem, we apply a fast and efficient optimization technique called ""Fast Non-dominated Sorting Genetic Algorithm (NSGA-II)"". In this paper, traffic grooming (GA-LMF) and non-traffic grooming (GA-MDF and FAR-FF) algorithms are compared and benchmarked for solving the multi-objective design problem. The results show that the GA-LMF is the most flexible and efficient grooming technique. The obtained solutions from the GA-LMF are spread on the objective space and are better than those from other non-grooming techniques in both objective values (i.e., number of accepted commodities and wavelength channels required).",,Design objectives; Multi objective; Multi-objective design; Network design problems; Non-dominated sorting genetic algorithms; NSGA-II; Objective space; Optimization techniques; Routing and wavelength assignment; Source-destination pairs; Traffic Grooming; Wavelength channels; WDM networks; Design; Genetic algorithms; Wavelength division multiplexing; Multiobjective optimization,Conference Paper,2-s2.0-77954768238,
ppr,Real-time scheduling of mixture-of-experts systems with limited resources,2010,HSCC'10 - Proceedings of the 13th ACM International Conference on Hybrid Systems: Computation and Control,7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953042422&doi=10.1145%2f1755952.1755964&partnerID=40&md5=f779d440b778d8d7e18b57a6c1b403b6,"Mixture-of-Experts (MoE) systems solve intricate problems by combining results generated independently by multiple computational models (the ""experts""). Given an instance of a problem, the responsibility of an expert measures the degree to which the expert's output contributes to the final solution. Brain Machine Interfaces are examples of applications where an MoE system needs to run periodically and expert responsibilities can vary across execution cycles. When resources are insufficient to run all experts in every cycle, it becomes necessary to execute the most responsible experts within each cycle. The problem of adaptively scheduling experts with dynamic responsibilities can be formulated as a succession of optimization problems. Each of these problems can be solved by a known technique called ""task compression"" using explicit mappings described in this paper to relate expert responsibilities to task elasticities. A novel heuristic is proposed to enable real-time execution rate adaptation in MoE systems with insufficient resources. In any given cycle, the heuristic uses sensitivity analysis to test whether one of two pre-computed schedules is the optimal solution of the optimization problem to avoid re-optimization when the test result is positive. These two candidate schedules are the schedule used in the previous cycle and the schedule pre-computed by the heuristic during the previous cycle, using future responsibilities predicted by the heuristic's responsibility predictor. Our heuristic significantly reduces the scheduling delay in the execution of experts when re-execution of the task-compression algorithm is not needed - from O(N2) time, where N denotes the number of experts, to O(N) time. Experimental evaluation of the heuristic on a test case in motor control shows that these time savings occur and scheduled experts' deadlines are met in up to 90% of all cycles. For the test scenario considered in the paper, the average output error of a real-time MoE system due to the use of limited resources is less than 7%. © 2010 ACM.",Adaptive scheduling; Ensemble computing; Mixture of experts; Prediction; Real-time scheduling; Sensitivity analysis,Adaptive scheduling; Ensemble computing; Mixture of experts; Prediction; Real time scheduling; Automobile exhibitions; Electric grounding; Hybrid computers; Hybrid systems; Mixtures; Optimization; Problem solving; Sensitivity analysis; Real time systems,Conference Paper,2-s2.0-77953042422,
ppr,Cyber-workstation for computational neuroscience,2010,Frontiers in Neuroengineering,6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355152271&doi=10.3389%2fneuro.16.011.2009&partnerID=40&md5=6dc25930ed2434749150e77664ef1176,"A Cyber-Workstation (CW) to study in vivo, real-time interactions between computational models and large-scale brain subsystems during behavioral experiments has been designed and implemented. The design philosophy seeks to directly link the in vivo neurophysiology laboratory with scalable computing resources to enable more sophisticated computational neuroscience investigation. The architecture designed here allows scientists to develop new models and integrate them with existing models (e.g. recursive least-squares regressor) by specifying appropriate connections in a block-diagram. Then, adaptive middleware transparently implements these user specifi cations using the full power of remote grid-computing hardware. In effect, the middleware deploys an on-demand and fl exible neuroscience research test-bed to provide the neurophysiology laboratory extensive computational power from an outside source. The CW consolidates distributed software and hardware resources to support time-critical and/or resource-demanding computing during data collection from behaving animals. This power and fl exibility is important as experimental and theoretical neuroscience evolves based on insights gained from data-intensive experiments, new technologies and engineering methodologies. This paper describes briefl y the computational infrastructure and its most relevant components. Each component is discussed within a systematic process of setting up an in vivo, neuroscience experiment. Furthermore, a co-adaptive brain machine interface is implemented on the CW to illustrate how this integrated computational and experimental platform can be used to study systems neurophysiology and learning in a behavior task. We believe this implementation is also the fi rst remote execution and adaptation of a brain-machine interface. © 2010 DiGiovanna, Rattanatamrong, Zhao, Mahmoudi, Hermer, Figueiredo, Principe, Fortes and Sanchez.",Brain-machine interface; Cyber-workstation; Distributed parallel processing; Real-time computational neuroscience,,Article,2-s2.0-81355152271,
wdc,Gasimo: A global address space simulation model,2010,SIMUTools 2010 - 3rd International ICST Conference on Simulation Tools and Techniques,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922879911&doi=10.4108%2fICST.SIMUTOOLS2010.8671&partnerID=40&md5=c3eb23d223c0e2279268e7ba9c821937,"The partitioned global address space (PGAS) programming model has gained attention as a robust model suitable for a diversity of emerging concurrent architectures. PGAS offers more scalability over the former distributed shared memory system (DSM) by supporting asynchronous execution based on message passing. Combining asynchronous communication with the facility to make the location of data transparent, applications written in PGAS languages have to trade off the benefits of concurrent architectures with the overhead caused by accessing distant memories. Here we present an effective simulation model to reflect the cost of distant memory accesses on a PGAS system. The model, called Gasimo, simulates a generic PGAS execution environment on top of a cluster of homogeneous dual-core machines. Gasimo is a parallel extension of a particular DSM simulator, called DSiMCluster, which has been implemented on top of a discrete event simulation (DES) engine known as HASE. © Copyright 2010 ICST.",Multi-core; OpenMP; Parallel discrete-event simulation (PDES); Partitioned global address space (PGAS); Simulation model,Application programming interfaces (API); Economic and social effects; Message passing; Multi core; OpenMP; Parallel discrete event simulations; Partitioned Global Address Space; Simulation model; Discrete event simulation,Conference Paper,2-s2.0-84922879911,
nth,Effects of program visualization (Jeliot3) on students' performance and attitudes towards Java programming,2010,"IMCIC 2010 - International Multi-Conference on Complexity, Informatics and Cybernetics, Proceedings",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880254676&partnerID=40&md5=409926673ec0c5f996f2be0cfd18909e,"This research aims to examine whether the use of computer program visualization software could help students to improve their performance and have long-term effects on their attitudes towards objected-oriented programming (OOP). The computer program visualization software, Jeliot3, was used in this research as a learning tool for Java programming. The experiment was conducted with 54 students, divided equally into two Sections. All of them are studying in the information technology program at a private university in Thailand. The experimental results showed that, after controlling for the demographics and backgrounds of the students, the use of Jeliot3 could help students improve their learning performance in OOP and Java programming and obtain higher scores than those who did not use Jeliot3. However, it was found that the use of Jeliot3 did not affect students' long-term attitudes toward OOP and Java programming.",Attitudes; Jeliot3; Object-oriented programming; Performance; Program visualization,Computer programming; Computer software; Cybernetics; Education; Java programming language; Students; Visualization; Attitudes; Information technology programs; Jeliot3; Learning performance; Long-term effects; Objected-oriented programming; Performance; Program visualization; Object oriented programming,Conference Paper,2-s2.0-84880254676,
wdc,Specification-based Verification in a Distributed Shared Memory Simulation Model,2010,SIMULATION,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952666300&doi=10.1177%2f0037549709349843&partnerID=40&md5=bef05245497d1198736f13bc4d612a54,"The emergence of chip multiprocessors is leading to rapid advances in hardware and software systems to provide distributed shared memory (DSM) programming models, so-called DSM systems. A DSM system provides programming advantages within a scalable and cost-effective hardware solution. This benefit derives from the fact that a DSM system creates a shared-memory abstraction on top of a distributed-memory machine by caching data replicas locally. In this respect, a coherence protocol is a vital component responsible for assuring data consistency across all replicas. The design of coherence protocols impacts a DSM system in terms of both performance and accuracy. Performance is often measured via simulation and various verification techniques have been proposed to deal with protocol accuracy. Nevertheless, integrating accuracy verification into a DSM cluster simulation to ensure correct simulation results is still an open issue. In this paper, we address three properties of a coherence protocol (safety, liveness, and inclusion) without which errors may occur in the simulation results. We propose a specification-based parameter—model interaction (SPMI) technique to detect these cases in a particular DSM cluster simulator called DSiMCluster. Our experimental results demonstrate that with SPMI, DSiMCluster can ensure the coherence protocol properties and provides a correct reflection of memory characteristics in shared-memory and DSM multiprocessors. © 2010, The Society for Modeling and Simulation International. All rights reserved.",coherence protocol; distributed shared memory; DSM cluster; verification technique,Chip Multiprocessor; Cluster simulations; Coherence protocol; Data consistency; Data replica; Distributed memory; Distributed shared memory; Hardware and software; Hardware solutions; Liveness; Model interaction; Programming models; Shared memories; Simulation model; Simulation result; Specification-based verification; Verification techniques; Computer software selection and evaluation; Electric load management; Hardware; Multiprocessing systems; Specifications; Computer simulation,Article,2-s2.0-77952666300,
ppr,BMI CyberWorkstation: A cyberinfrastructure for collaborative experimental research on brain-machine interfaces,2010,"Proceedings of the 6th International Conference on Collaborative Computing: Networking, Applications and Worksharing, CollaborateCom 2010",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957850814&doi=10.4108%2ficst.collaboratecom.2010.42&partnerID=40&md5=c346af3dd15f36612f2f59a0b4f9fc31,"This paper describes the design and implementation of an improved version (2.0) of a computational cyberinfrastructure for neuroscience research, called CyberWorkstation (CW). CW can provide to neurophysiology laboratories the following: (1) data storage for large volumes of neural signals, experimental parameters and computational results, (2) integration of necessary experimental equipment, powerful computational resources and robust software mechanisms that enable users to conduct online and offline BMI experiments, (3) a Web-based interface that permits users to conveniently setup, monitor and review their experiments and collaborate with others in analyzing and developing their research findings. The capabilities of the CW in enabling collaborative BMI research are demonstrated using forward models based on neural networks that predict positions of an agent in 2D movement control. © 2010 ICST.",Brain-Machine Interfaces; Collaborative Computing; Cyberinfrastructure; CyberWorkstation,Computer supported cooperative work; Digital storage; Multimedia systems; Neural networks; Brain machine interface; Computational resources; Computational results; Cyber infrastructures; CyberWorkstation; Design and implementations; Experimental equipments; Experimental parameters; Brain computer interface,Conference Paper,2-s2.0-79957850814,
ksc,"VCCP: A transparent, coordinated checkpointing system for virtualization-based cluster computing",2009,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72049109446&doi=10.1109%2fCLUSTR.2009.5289183&partnerID=40&md5=1aa29113cf06931391d6a5a1a73d8cfd,"Virtual machine, which typically consists of a guest operating system (OS) and its serial applications, can be checkpointed, migrated to another cluster node, and restarted later to its previous saved state. However, to date, it is nontrivial to provide checkpoint-restart mechanisms with the same level of transparency for distributed applications running on a cluster of virtual machines. To address this particular issue, we have created the Virtual Cluster CheckPointing (VCCP) system, a novel system for transparent coordinated checkpoint-restart of virtual machines and its distributed application on commodity clusters. In this paper, we detail the design and implementation of the VCCP system. Our VCCP prototype extends the open source QEMU system with kqemu module by implementing hypervisor-based Coordinated Checkpoint-Restart protocols. To verify and validate our prototype, we measured its performance using the NAS parallel benchmark. Our experimental results indicate that VCCP generates less than 1% of additional execution overhead for non-communication intensive parallel applications. Furthermore, our correctness analysis shows that VCCP does not cause message loss or reordering, which is a necessary property to ensure correctness of checkpoint-restart mechanism. Finally, we believe that VCCP is a promising checkpoint-restart alternative for legacy applications that have implemented traditional process-level checkpoint-restart. © 2009 IEEE.",Coordinated checkpointing; Fault tolerance; High performance computing; Virtualization,Check pointing; Cluster nodes; Commodity clusters; Communication-intensive parallel applications; Coordinated checkpointing; Coordinated checkpoints; Distributed applications; High performance computing; Hypervisor; Legacy applications; NAS parallel benchmarks; Open sources; Operating systems; Restart mechanism; Virtual clusters; Virtual machines; Virtualizations; Cluster computing; Computer operating systems; Computer science; Fault tolerance; Quality assurance; Technical presentations; Fault tolerant computer systems,Conference Paper,2-s2.0-72049109446,
ojs,Affordances of machine-processable competency modelling,2009,"IADIS International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883014028&partnerID=40&md5=1da5740da96eb75f8bf3dc49dbe5e316,"Existing e-learning competency standards such as the IMS Reusable Definition of Competency or Educational Objective (IMS RDCEO) specification and the HR-XML standard are not able to accommodate the level of a competency described separately from its narrative description; the grading scale of a competency; the success threshold of a competency; or the structure of competency trees or hierarchies. The proposed competency model addresses these problems and reflects all relevant features of the learner's behaviour and their knowledge, skills, and attitudes that affect their learning and performance. Statements of competency are machine-readable. Machine processing can offer interoperable and reusable resources and applications that are pedagogically effective for e-learning and assessment. A competency statement which can be read, processed, and interpreted by machine contributes to the automatic generation of questions, distractors, and question sequences, and offers a semantic structure for further processing.",Adaptive assessment; Competence; Distractor; IMS QTI; Ontology,Adaptive assessment; Automatic Generation; Competence; Distractor; Educational objectives; IMS QTI; Reusable resources; Semantic structures; Grading; Interoperability; Ontology; Semantics; E-learning,Conference Paper,2-s2.0-84883014028,
pkl,Multi-objective routing wavelength assignment in WDM network using SPEA2 approach,2009,"2009 9th International Symposium on Communications and Information Technology, ISCIT 2009",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549215675&doi=10.1109%2fISCIT.2009.5341008&partnerID=40&md5=a2bf74dec9bf39270f3b5d08bb14fcd5,"Network design problem is usually complicated. Network designers may have to optimize various design objectives simultaneously. This paper considers a multi-objective network design problem for routing wavelength assignment (RWA) in WDM networks. The design objectives are to maximize the number of accepted communication requests (source-destination pairs) as well as to minimize the number of wavelength channel requirement subject to a limited number of wavelength channels available on each network link and at least 80% of all commodities must be accepted. Both the design objectives are conflicted to each other; maximizing the number of accepted commodities will require a large number of wavelength channels while minimizing the number of wavelength channels will limit the amount of accepted commodities. To solve the problem, we apply a famous multiobjective optimization approach called ""SPEA2"". The obtained results are compared with those from the Weighted Sum Approach in various weighted cases. The result comparisons show that the SPEA2 is superior in term of providing various sets of feasible solutions as a front in the objective space. ©2009 IEEE.",,Design objectives; Feasible solution; Multi objective; Network design problems; Network designer; Network links; Objective space; Result comparison; Source-destination pairs; Wavelength assignment; Wavelength channels; WDM networks; Weighted Sum; Design; Information technology; Problem solving; Wavelength; Wavelength division multiplexing; Multiobjective optimization,Conference Paper,2-s2.0-74549215675,
ojs,A study of head instabilities in TMR sensors using in high density perpendicular magnetic recoding systems,2009,International Journal of Applied Electromagnetics and Mechanics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449115217&doi=10.3233%2fJAE-2009-1061&partnerID=40&md5=172b1c804a7b5fffe3ede456e42020f0,"We did an experimental study to examine head instabilities and noise of tunneling magnetoresistance (TMR) sensors using in high-density perpendicular recording systems. It found that the instability signatures on the transfer curves could be categorized into two groups; permanent magnet/free layer (PM/FL) failure and synthetic antiferromagnetic layers/antiferromagnetic layer (SAF/AFM) failure. The thermal-mechanical stress from the adaptive flying height (AFH) heater as well as head-disk interaction potentially degrades PM resulting in the reduction of the FL stabilizing field, therefore TMR ratio and signal increases. The non-uniformity of PM remnant magnetization and the switching of PM grains especially near the edge potentially degrade the biasing field. Scanning electron microscope images of these weak heads show rough surface and scratches close to the active area of read sensor. The deep scratches by particles in the head-disk interaction and shallow scratches by the slider lapping process is a potential cause to degrade PM. In another group, the thermal-mechanical stress from AFH heater potentially reduces the exchange-bias field and thermal stability of SAF/AFM layers allowing partial flipping of SAF edge magnetization, consequently it induces voltage fluctuations and hysteresis loops in the transfer curves. © 2009 - IOS Press and the authors. All rights reserved.",Adaptive flying height; Head instabilities; Head-disk interaction; Thermal-mechanical stress,Active area; Biasing field; Exchange-bias fields; Experimental studies; Flying heights; High-density; Mechanical stress; Nonuniformity; Perpendicular recording; Read sensor; Recoding; Remnant magnetization; Rough surfaces; Scanning Electron Microscope; Synthetic antiferromagnetic; Thermal stability; Transfer curves; Tunneling magnetoresistance; Voltage fluctuations; Antiferromagnetism; Disks (structural components); Electric resistance; Hysteresis; Hysteresis loops; Magnetic field effects; Magnetic materials; Magnetoresistance; Metal finishing; Scanning electron microscopy; Sensors; Thermodynamic stability; Stresses,Article,2-s2.0-75449115217,
pkl,Multi-objective design for routing wavelength assignment in WDM networks,2009,"Proceedings - 2009 International Conference on New Trends in Information and Service Science, NISS 2009",5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449553542&doi=10.1109%2fNISS.2009.220&partnerID=40&md5=07f8d0ef22b024b97e0358b5b9ac5348,"In this paper, we consider a multi-objective network design problem with static routing wavelength assignment (RWA) in WDM networks. The design objective is to maximize the number of accepted communication requests (sourcedestination pairs) and to minimize the number of wavelength channel required subject to a limited number of wavelength channels available on each network link and at least 80% of all commodities must be accepted. We consider our MultiObjective WDM network design problem in various cases. The network model is solved using a new and efficient approach called ""Genetic Algorithm for Routing and Minimum Degree First Wavelength Assignment (GA-MDF)"". We compare our simulation results with those of the traditional approach that is Fixed-Alternate Routing and First-Fit Wavelength Assignment (FAR-FF). The result comparisons show that GA-MDF is superior both in terms of the amount of accepted commodity requests and CPU computation time. © 2009 IEEE.",Multi-objective network design; Optimization algorithm; RWA; Wdm optical network,Alternate routings; Computation time; Design objectives; Minimum degree; Multi objective; Multi-objective design; Multi-objective network design; Network design problems; Network links; Network models; Optimization algorithm; Result comparison; RWA; Simulation result; Source-destination pairs; Wavelength assignment; Wavelength channels; WDM networks; Wdm optical network; Computational efficiency; Design; Genetic algorithms; Multiobjective optimization; Optical materials; Simulators; Wavelength; Wavelength division multiplexing; Routing algorithms,Conference Paper,2-s2.0-70449553542,
wdc,DSiMCluster: A Simulation Model for Efficient Memory Analysis Experiments of DSM Clusters,2009,SIMULATION,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149171515&doi=10.1177%2f0037549709104483&partnerID=40&md5=f50c61c391223bec7e648ab32b10d1d5,"The emergence of chip multiprocessors is leading to the rapid development of multicore clusters that provide distributed—shared memory (DSM) programming models, so called DSM systems. DSM systems allow applications to access data in a logically shared address space by abstracting away the distinction of physical memory location. However, a significant drawback of a DSM cluster is that it performs poorly on applications that require large shared memory because of the performance loss or overhead caused by memory locality. Consequently, an effective memory analysis methodology is mandatory to quantify the impact of such overhead. In this respect, a widely used technique is simulation. Nevertheless, both the intrinsically difficult design of cluster architectures and the rapid advance of DSM implementations impose a constraint on simulation model designs. In this paper we describe DSiMCluster, a simulation model for memory analysis experiments of a DSM cluster system. We capitalize on the observation that the components of a simulation model have to be highly reusable and provide automatic behavioral verification. That is, in order to maintain the accuracy of simulation results with the technological advances of target architectures, which grow extensively at the software level. We show that highly reusable simulation components can be created by implementing a system-oriented simulation on top of a well-founded and broadly applicable, discrete-event simulation engine. Our experimental results demonstrate that, with these operations, DSiMCluster provides an efficient framework for memory analysis experiments on a DSM cluster architecture within a reconfigurable, customizable and user-friendly environment. © 2009, SAGE Publications. All rights reserved.",distributed—shared memory; DSM cluster; memory analysis; memory simulation,Chip Multiprocessor; Cluster architecture; Cluster systems; Customizable; Distributed-shared memory; DSM cluster; Memory analysis; Memory locality; Memory simulation; Multi-core cluster; Performance loss; Physical memory; Programming models; Rapid development; Re-configurable; Reusable simulation; Shared address spaces; Shared memories; Simulation model; Simulation result; Target architectures; Technological advances; Architectural design; Computer software; Electric load management; Experiments; Multiprocessing systems; Security of data; Simulators,Article,2-s2.0-66149171515,
ojs,Transforming a competency model to parameterised questions in assessment,2009,Lecture Notes in Business Information Processing,4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249097988&doi=10.1007%2f978-3-642-01344-7_29&partnerID=40&md5=510a1d0d119648235eeb0c2b61d7767b,"The problem of comparing and matching different learners' knowledge arises when assessment systems use a one-dimensional numerical value to represent ""knowledge level"". Such assessment systems may measure inconsistently because they estimate this level differently and inadequately. The multi-dimensional competency model called COMpetence-Based learner knowledge for personalized Assessment (COMBA) is being developed to represent a learner's knowledge in a multi-dimensional vector space. The heart of this model is to treat knowledge, not as possession, but as a contextualized space of capability either actual or potential. The paper discusses a system for automatically generating questions from the COMBA competency model as a ""guide-on-the- side"". The system's novel design and implementation involves an ontological database that represents the intended learning outcome to be assessed across a number of dimensions, including level of cognitive ability and subject matter. The system generates all the questions that are possible from a given learning outcome, which may then be used to test for understanding, and so could determine the degree to which learners actually acquire the desired knowledge. © 2009 Springer Berlin Heidelberg.",Assessment; Competency; Knowledge level; Ontology,Education; Assessment; Assessment system; Cognitive ability; Competency; Competency model; Dimensional vectors; Knowledge level; Learning outcome; Novel design; Numerical values; Ontological database; Subject matters; Ontology,Conference Paper,2-s2.0-67249097988,
wdp,Modeling of end-to-end available bandwidth in wide area network,2008,"Proceedings of the 2008 International Symposium on Parallel and Distributed Processing with Applications, ISPA 2008",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649087530&doi=10.1109%2fISPA.2008.56&partnerID=40&md5=2e4636a363ba931812c83ed625a5abc1,"Modeling the available bandwidth of a path using a known stochastic process is one possible method for estimating future available bandwidth along the path without explicit support from network routers. Our two hypotheses for the stochastic process are as follows. First, an autoregressive integrated moving-average process (ARIMA) is a suitable model for the available bandwidth over time of a path. Second, the available bandwidth over time of a path can be modeled as a self-similar process. We verify both hypotheses using R statistical software and available bandwidth data sets published by Stanford Linear Accelerator Center (SLAC). Our results indicate that the available bandwidth over time of an end-to-end path can be modeled as fractional Gaussian Noise (FGN) and seasonal fractional ARIMA (SFARIMA) processes. On the other hand, we found that an ARIMA process is not a good model for available bandwidth over time of an end-to-end path. © 2008 IEEE.",,Distributed parameter networks; Random processes; Routers; Telecommunication systems; Autoregressive integrated moving-average process; Available bandwidths; Data sets; End-to-end paths; Fractional Gaussian noise (fGn); Network routers; Self-similar process; Stanford linear accelerator centers; Statistical softwares; Stochastic process; Bandwidth,Conference Paper,2-s2.0-60649087530,
pkl,Efficient design techniques for reliable wireless backhaul networks,2008,"2008 International Symposium on Communications and Information Technologies, ISCIT 2008",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549133145&doi=10.1109%2fISCIT.2008.4700148&partnerID=40&md5=c4c27a6a49af7842d4625b482b65ab24,"The issues of network performance and reliability in wireless access networks have become indispensable due to the increasing needs for communication services anywhere and anytime. Traditional design of wireless backhaul networks has a simple tree-like structure which is vulnerable to network failures. To protect services against network component/facility breakdown, reliable network design is required. However, the complexity of reliable network design is a well-known problem. Various techniques have been proposed to find a minimum-cost solution in a reliable network design. In this paper, we propose and compare two different approaches which are network link restoration and network path restoration for reliable wireless backhaul network design. These restoration techniques provide fault tolerance to tree topology in traditional cellular network design, so that any breakdown of single link or multiple links within a network path can no longer interrupt the network services. As a result, the network design with backup path using restoration approaches can greatly enhance the network reliability and quality of services. We also show that a well-known shortest path algorithm can be efficiently applied as a meta- heuristic technique for both link restoration and path restoration approaches in a reliable wireless backhaul network design. © 2008 IEEE.",,Backup path; Cellular network designs; Communication service; Cost solutions; Efficient designs; Link restoration; Meta-heuristic techniques; Multiple links; Network design; Network failure; Network links; Network path restoration; Network paths; Network reliability; Network services; Path restoration; Restoration techniques; Shortest path algorithms; Single link; Tree topology; Tree-like structures; Wireless access networks; Wireless backhaul networks; Cellular neural networks; Cellular telephone systems; Design; Fault tolerance; Heuristic algorithms; Heuristic methods; Network performance; Quality assurance; Restoration; Systems engineering; Telecommunication networks; Wireless telecommunication systems; Wireless networks,Conference Paper,2-s2.0-67549133145,
ojs,Transforming a competency model to assessment items,2008,"WEBIST 2008 - 4th International Conference on Web Information Systems and Technologies, Proceedings",5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049138588&partnerID=40&md5=2aba9d257e9bb8002a6200b5f54fd361,"The problem of comparing and matching different learners' knowledge arises when assessment systems use a one-dimensional numerical value to represent ""knowledge level"". Such assessment systems may measure inconsistently because they estimate this level differently and inadequately. The multi-dimensional competency model called COMpetence-Based learner knowledge for personalized Assessment (COMBA) is being developed to represent a learner's knowledge in a multi-dimensional vector space. The heart of this model is to treat knowledge, not as possession, but as a contextualized space of capability either actual or potential. The paper discusses the automatic generation of an assessment from the COMBA competency model as a ""guide-on-the-side"".",Adaptive assessment; Competency; Knowledge level,Digital signal processing; E-learning; Information systems; Internet; Multimedia systems; World Wide Web; Adaptive assessment; Assessment systems; Automatic generations; Competency; Competency models; Knowledge level; Multi-dimensional; Numerical values; Mathematical models,Conference Paper,2-s2.0-58049138588,
mvp,Comparative experiences with software process modeling tools for the incremental commitment model,2008,Intech'08 - Proceedings of the 9th International Conference on Intelligent Technologies,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62549152268&partnerID=40&md5=5811d9e9e3f604f96feeb210df4f051a,"The Incremental Commitment Model (ICM) is a new generation process model that focuses on the incremental growth of success critical stakeholder satisfaction, system definition and stakeholder commitment. ICM has been introduced in system engineering, but not software engineering. In the Fall 2008, ICM will be used as a process model to develop software system in University of Southern California (USC) Software Engineering graduate course. Hence, two significant different software process modeling tools are selected to create the electronic process guidelines for this course. This paper reports our comparative experiences between an adaptability tolerance framework, Eclipse Process Framework Composer (EPFC) and a precision oriented process definition language, Little-JIL in order to create ICM electronic guide. In addition, the paper provides a tool comparison analysis by using Humphrey and Kellner's criteria and a target group experimental result. The evaluation identifies some research challenges and areas for future research work.","Process modeling tools, Electronic process guide generator tools, Little-JIL, EPF Composer, Incremental Commitment Model","Comparison analysis; Eclipse process frameworks; Electronic process; Engineering graduates; Generation process; Incremental growths; Oriented process; Process modeling tools, Electronic process guide generator tools, Little-JIL, EPF Composer, Incremental Commitment Model; Process models; Research challenges; Software process modeling; Software systems; System engineerings; Target groups; University of Southern California; Process monitoring; Systems engineering; Software engineering",Conference Paper,2-s2.0-62549152268,
ojs,Deriving e-assessment from a competency model,2008,"Proceedings - The 8th IEEE International Conference on Advanced Learning Technologies, ICALT 2008",17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849102458&doi=10.1109%2fICALT.2008.255&partnerID=40&md5=f99b031b2a362ccfb79faa044e74d0c8,"Self-assessment is a crucial component of learning. Creating effective questions is time-consuming, however, because it may require considerable resources and the skill of critical thinking. Questions need careful construction to accurately represent the intended learning outcome and the subject matter involved. There are very few systems currently available which generate questions automatically, and these are confined to specific domains. This paper presents a system for automatically generating questions from a competency framework, based on question templates, criteria for effective questions, and the instructional content and ability matrix. This makes it possible to guide learners in developing questions for themselves, and to provide authoring templates which speed the creation of new questions for selfassessment. © 2008 IEEE.",,Technology; Competency model; Considerable resources; Critical thinking; Guide learners; Instructional content; International conferences; Learning outcomes; Learning technologies; Self-assessment; Education,Conference Paper,2-s2.0-51849102458,
scw,A fuzzy approach for personalized product clustering with flexible discriminating power,2008,"Proceedings - International Conference on Advanced Information Networking and Applications, AINA",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249182318&doi=10.1109%2fWAINA.2008.197&partnerID=40&md5=609477ee87465dbc32cc3236b877edc0,"This paper proposes an improved linguistic quantifier that operates with penalty function so that a set of products can be clustered into hierarchical levels. The method is based on fuzzy approach with an aim at personalizing the display of products in an order of personal preference on a set of product attributes. The discriminating power of the system can be flexibly tuned via the set up of quantifier's parameters and the penalty function. Numerical example is given for illustrating the computation, providing comparative results to the existing method and giving some technical insights. © 2008 IEEE.",,Fuzzy clustering; Discriminating power; Existing method; Fuzzy approach; Hierarchical levels; Information networking; International conferences; Numerical examples; Penalty functions; Product attributes; Set theory,Conference Paper,2-s2.0-50249182318,
ojs,An evaluation of pedagogically informed parameterised questions for self-assessment,2008,"Learning, Media and Technology",11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149083005&doi=10.1080%2f17439880802324210&partnerID=40&md5=af1bcae6699ea0a82e5b48034a114501,"Self-assessment is a crucial component of learning. Learners can learn by asking themselves questions and attempting to answer them. However, creating effective questions is time-consuming because it may require considerable resources and the skill of critical thinking. Questions need careful construction to accurately represent the intended learning outcome and the subject matter involved. There are very few systems currently available which generate questions automatically, and these are confined to specific domains. This paper presents a system for automatically generating questions from a competency framework, based on a sound pedagogical and technological approach. This makes it possible to guide learners in developing questions for themselves, and to provide authoring templates which speed the creation of new questions for self-assessment. This novel design and implementation involves an ontological database that represents the intended learning outcome to be assessed across a number of dimensions, including level of cognitive ability and subject matter. The system generates a list of all the questions that are possible from a given learning outcome, which may then be used to test for understanding, and so could determine the degree to which learners actually acquire the desired knowledge. The way in which the system has been designed and evaluated is discussed, along with its educational benefits.",Competency; IMS QTI; Ontology; Self-assessment,,Article,2-s2.0-52149083005,
pkl,Providing fault tolerance in wireless backhaul network design with path restoration,2008,"ARES 2008 - 3rd International Conference on Availability, Security, and Reliability, Proceedings",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49049100495&doi=10.1109%2fARES.2008.176&partnerID=40&md5=5a0db96efb926a60e79215098daf1f68,"This paper presents a survivable network design using network path restoration approach to provide fault tolerance in cellular backhaul network design, so that any breakdown of links within a network path can no longer interrupt the network services. As a consequence, the network design can greatly enhance the network reliability and quality of services. We adopt a two-phase network design approach to reduce the complexity of network design and to provide a design mechanism for enhancing reliability in existing networks. The first phase provides a minimum-cost initial network design. The problem is to find a minimum-cost network topology which includes selecting the location and type of base station controllers and mobile switching controllers as well as their link types. The second phase provides backup paths and spare capacity to the network topology from phase one to improve network reliability. Due to the complexity of network design problem, a genetic algorithm is applied as a meta-heuristic technique for obtaining good solutions. Various problem sizes of example networks are considered and discussed. © 2008 IEEE.",Genetic algorithm; Network path restoration; Optimization algorithm; Survivable network design,Chlorine compounds; Diesel engines; Electric network topology; Errors; Fault tolerance; Genetic algorithms; Heuristic algorithms; Heuristic methods; Machine design; Quality assurance; Quality of service; Reliability; Repair; Restoration; Topology; Wireless networks; Base station controllers; Genetic algorithm; Heuristic techniques; International conferences; Network design problem; Network designs; Network path restoration; Network paths; Network reliability; Network Services; Network topologies; Optimization algorithm; Path restoration; Problem sizes; Second phase; Spare capacity; Survivable network design; Switching controllers; Wireless backhaul networks; Heuristic programming,Conference Paper,2-s2.0-49049100495,
scw,A sieving ANN for emotion-based movie clip classification,2008,IEICE Transactions on Information and Systems,14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149100415&doi=10.1093%2fietisy%2fe91-d.5.1562&partnerID=40&md5=eca777aa2d3a3fda26e1117e7f195bd6,"Effective classification and analysis of semantic contents are very important for the content-based indexing and retrieval of video database. Our research attempts to classify movie clips into three groups of commonly elicited emotions, namely excitement, joy and sadness, based on a set of abstract-level semantic features extracted from the film sequence. In particular, these features consist of six visual and audio measures grounded on the artistic film theories. A unique sieving-structured neural network is proposed to be the classifying model due to its robustness. The performance of the proposed model is tested with 101 movie clips excerpted from 24 award-winning and well-known Hollywood feature films. The experimental result of 97.8% correct classification rate, measured against the collected human-judges, indicates the great potential of using abstract-level semantic features as an engineered tool for the application of video-content retrieval/indexing. Copyright © 2008 The Institute of Electronics, Information and Communication Engineers.",Emotion-based classification; Movie clip classification; Multimedia content analysis; Semantic content analysis; Video analysis,Deep neural networks; Motion pictures; Semantics; Video recording; Classification rates; Content-based indexing and retrieval; Movie clips; Multimedia content analysis; Semantic content analysis; Semantic features; Structured neural networks; Video analysis; Classification (of information),Article,2-s2.0-68149100415,
pkl,Cellular wireless network design with reliability consideration,2008,ICQR 2007 - Proceedings of the 5th International Conference on Quality and Reliability,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906996739&partnerID=40&md5=47defdead7b4129ae89c703abdcd190a,"In this paper, we present a cellular wireless network topology design for reliability by considering network link restoration. This is to provide a back up path for any single link failure in a traditional network design with tree topology, so that a network link failure can no longer interrupt the network services and causes the network to fail. As a consequence, the network link restoration approach can greatly enhance the network reliability and quality of services. We adopt a two-phase network design approach. The first phase provides a minimumcost initial network design. The problem is to find a minimumcost network topology which includes selecting the location and type of base station controllers and mobile switching controllers as well as their link types. In the second phase, we provide network link restoration to the minimum cost network topology from phase one in order to improve network reliability. We apply a genetic algorithm (GA) to minimize the cost of the design problem, including the cost of network links, equipment and their installation. GA is used as a meta-heuristic technique for obtaining good solution for the design problem. Various sizes of network design problem are considered and discussed. The GA results are evaluated with those from a brute force search algorithm. © 2008 ICQR.",Cellular network design; Genetic algorithm; Network link restoration; Optimization algorithm,Algorithms; Costs; Design; Electric network topology; Genetic algorithms; Heuristic methods; Reliability; Restoration; Topology; Wireless networks; Base station controllers; Cellular network designs; Cellular wireless networks; Meta-heuristic techniques; Network design problems; Network links; Optimization algorithms; Switching controllers; Telecommunication links,Conference Paper,2-s2.0-84906996739,
ksc,"The design and prototype of RUDA, a distributed grid accounting system",2008,International Journal of Critical Infrastructures,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549125277&doi=10.1504%2fIJCIS.2008.017442&partnerID=40&md5=18dc055ce5a5b214aefbfd20e286c719,"The grid environment contains a large and growing number of widely distributed sites with heterogeneous resources. It is a great challenge to dynamically manage and account for usage data of grid resources, such as computational, network, and storage resources. A distributed Resource Usage Data management and Accounting (RUDA) system is designed to perform accounting in the grid environment. RUDA utilises fully decentralised design to enhance scalability and supports heterogeneous resources with no significant impact on local systems. It can easily be integrated into grid infrastructures and maintains the integrity of the grid security features. Copyright © 2008 Inderscience Enterprises Ltd.",Distributed accounting; Grid accounting; Grid resource usage; Resource usage data management and accounting; RUDA,Data storage equipment; Resource allocation; Scalability; Security of data; Distributed accounting; Grid accounting; Grid resource usage; Resource usage data management and accounting; Distributed computer systems,Article,2-s2.0-40549125277,
ppr,BMI cyberworkstation: Enabling dynamic data-driven brain-machine interface research through cyberinfrastructure,2008,"Proceedings of the 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS'08 - ""Personalized Healthcare through Technology""",5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61849175213&doi=10.1109%2fiembs.2008.4649235&partnerID=40&md5=56e43df5d48fef6b037069de04f563ec,"Dynamic data-driven brain-machine interfaces (DDDBMI) have great potential to advance the understanding of neural systems and improve the design of brain-inspired rehabilitative systems. This paper presents a novel cyberinfrastructure that couples in vivo neurophysiology experimentation with massive computational resources to provide seamless and efficient support of DDDBMI research. Closed-loop experiments can be conducted with in vivo data acquisition, reliable network transfer, parallel model computation, and real-time robot control. Behavioral experiments with live animals are supported with real-time guarantees. Offline studies can be performed with various configurations for extensive analysis and training. A Web-based portal is also provided to allow users to conveniently interact with the cyberinfrastructure, conducting both experimentation and analysis. New motor control models are developed based on this approach, which include recursive least square based (RLS) and reinforcement learning based (RLBMI) algorithms. The results from an online RLBMI experiment shows that the cyberinfrastructure can successfully support DDDBMI experiments and meet the desired real-time requirements. © 2008 IEEE.",,Data acquisition; Reinforcement learning; Behavioral experiment; Brain machine interface; Closed-loop experiments; Computational resources; Cyber infrastructures; Real time requirement; Real-time robot controls; Recursive least square (RLS); Brain computer interface; article; artificial intelligence; brain; computer; computer interface; computer program; cybernetics; electroencephalography; equipment; equipment design; equipment failure; evoked response; human; information retrieval; man machine interaction; methodology; physiology; Artificial Intelligence; Brain; Computers; Cybernetics; Electroencephalography; Equipment Design; Equipment Failure Analysis; Evoked Potentials; Humans; Information Storage and Retrieval; Man-Machine Systems; Software; User-Computer Interface,Conference Paper,2-s2.0-61849175213,
mvp,A replicate empirical comparison between pair development and software development with inspection,2007,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949089815&doi=10.1109%2fESEM.2007.9&partnerID=40&md5=6a2ae09f695970972bec340ec19272ef,"In 2005, we studied the development effort and effect of quality comparisons between software development with Fagan's inspection and pair development. Three experiments were conducted in Thailand: two classroom experiments and one industry experiment. We found that in the classroom experiments, the pair development group had less average development effort than the inspection group with the same or higher level of quality. The industry experiment's result showed pair development to have a bit more effort but about 40% fewer major defects. However, since this set of experiments was conducted in Thailand, the results may be different if we conducted the experiment in other countries due to the impact of cultural differences. To investigate this we conducted another experiment with Computer Science graduate students at USC in Fall 2006. Unfortunately, the majority of the graduate students who participated in the experiment were from India, a country in which the culture is not much different from Thailand [18], [19]. As a result, we cannot compare the impact of cultural differences in this paper. However, the results showed that the experiment can be replicated in other countries where the cultures are similar. © 2007 IEEE.",,Classroom experiments; Cultural differences; Empirical comparison; Empirical software engineering; Graduate students; International symposium; Software development; Thailand; Computer science; Computer software selection and evaluation; Inspection; Learning systems; School buildings; Software design; Software engineering; Students; Technology; Experiments,Conference Paper,2-s2.0-47949089815,
ojs,Interactivity within IMS learning design and question and test interoperability,2007,"Webist 2007 - 3rd International Conference on Web Information Systems and Technologies, Proceedings",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649950616&partnerID=40&md5=f7ea770023997914586eaa57768ab888,"We examine the integration of IMS Question and Test Interoperability (QTI) and IMS Learning Design (LD) in implementations of E-learning from both pedagogical and technological points of view. We propose the use of interactivity as a parameter to evaluate the quality of assessment and E-learning, and assess various cases of individual and group study for their interactivity, ease of coding, flexibility, and reusability. We conclude that presenting assessments using IMS QTI provides flexibility and reusability within an IMS LD Unit Of Learning (UOL) for individual study. For group study, however, the use of QTI items may involve coding difficulties if group members need to wait for their feedback until all students have attempted a question, and QTI items may not be able to be used at all if the QTI services are implemented within a service-oriented architecture.",Formative assessment; IMS; Interactivity; Learning design; QTI,Formative assessment; IMS; Interactivity; Learning design; QTI; Design; E-learning; Information services; Information systems; Insulating materials; Interoperability; Ladder networks; Multimedia systems; Reusability; Semantic Web; World Wide Web; Education,Conference Paper,2-s2.0-67649950616,
ojs,Adapting health care competencies to a formal competency model,2007,"Proceedings - The 7th IEEE International Conference on Advanced Learning Technologies, ICALT 2007",14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649088484&doi=10.1109%2fICALT.2007.49&partnerID=40&md5=b329c96433faa9e8487704c32847aa79,"Health professions education has moved away from process-based curricula to competency-based curricula. Machine readable and processable health care competencies are still embryonic, pending the emergence of appropriate standards. The IMS Reusable Definition of Competency or Educational Objective specification and the HR-XML competency standard are introduced, compared, and their problems identified in the implementation of exemplar competencies from the UK Royal College of Nursing. An improved competency model is proposed. © 2007 IEEE.",,Competency models; Processable; Curricula; Health; Health care; Markup languages; Education,Conference Paper,2-s2.0-47649088484,
pps,An analysis using eye-mark recorder of the effectiveness of presentation methods for e-learning,2007,"Proceedings - The 7th IEEE International Conference on Advanced Learning Technologies, ICALT 2007",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649128422&doi=10.1109%2fICALT.2007.52&partnerID=40&md5=7debfeb9fea1e5e105dfde6dcc5d0ae0,"In the context of e-learning contents, if we take an approach based on the ""Dual Channel"" model, which is well known as a processing model for human perceptual judgments, cognitive resources can be used most effectively by methods that synchronize narrations with images and video contents. In general, however, the learner must search for a visual fixation point in the image or video while simultaneously listening to the narration, and if it is difficult to locate that fixation point, then the cognitive burden is increased, and the efficiency of understanding the learning content decreases. In cases such as these, it is considered effective to introduce a presentation method that uses a pointer to visualize the fixation point. In this research, we used an eye mark recorder (a device for measuring the subject's point of visual focus and pupil diameter) to measure the point of fixation for e-learning students, in order to measure the effects of leading the subject's fixation point with a pointer. The results indicated that in more than 80% of presentation material displays, the subjects followed the lead of the pointer. Tests following the experiment also showed that the subjects demonstrated higher percentages of correct answers for contents that displayed the pointer. © 2007 IEEE.",,Cognitive resources; E - learnings; Learning contents; Point of fixations; Pupil diameters; Video contents; E-learning; Internet; Multimedia systems; Visual communication; Education,Conference Paper,2-s2.0-47649128422,
pps,E-testing construction support system with some prediction tools,2007,"Proceedings - The 7th IEEE International Conference on Advanced Learning Technologies, ICALT 2007",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649115403&doi=10.1109%2fICALT.2007.132&partnerID=40&md5=11e2f7db18f597450cbab1ee9f29883e,"This paper proposes an e-testing construction support system (eTCSS) with the prediction tools for the constructed test. This paper performs some comparison experiments to find the best predictive performances models for the Predictive response-time distribution and the Predictive response-time distribution. Furthermore, the amount of test information based on the Item Response Theory, which is important to improve the measurement efficiency of the constructed test, is applied to be the prediction tool of the eTCSS. Finally, this paper evaluates the system by using actual data. The results show the effectiveness of the system. © 2007 IEEE.",,E-testing; Measurement efficiencies; Prediction tools; Predictive performances; Support systems; Test informations; Time distributions; Paper,Conference Paper,2-s2.0-47649115403,
pps,Collaborative e-learning among teachers using a web database in special support education,2007,"Proceedings - The 7th IEEE International Conference on Advanced Learning Technologies, ICALT 2007",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649092559&doi=10.1109%2fICALT.2007.96&partnerID=40&md5=e8e08ade0fa70df9162734a494dd45ce,"We utilized e-Learning using example data from special support education stored in a web database as e-Learning contents. The e-Learning was conducted with teachers from various educational institutes: from primary schools to universities, in addition to special support schools. Through a collective problem solving process we were able to collect in a single web database disparate knowledge from a variety of teachers. © 2007 IEEE.",,E - learnings; Educational institutes; Learning contents; Primary schools; Problem solving processes; Web databases; Database systems; E-learning; Internet; Multimedia systems; Societies and institutions; Teaching; Education,Conference Paper,2-s2.0-47649092559,
wdp,OCS: An effective caching scheme for video streaming on overlay networks,2007,Multimedia Tools and Applications,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249085328&doi=10.1007%2fs11042-006-0071-x&partnerID=40&md5=7f70081fb8b1bd25c6e263859d4d91c5,"Video streaming is vital for many important applications such as distance learning, digital video libraries, and movie-on-demand. Since video streaming requires significant server and networking resources, caching has been used to reduce the demand on these resources. In this paper, we propose a novel collaboration scheme for video caching on overlay networks, called Overlay Caching Scheme (OCS), to further minimize service delays and loads placed on an overlay network for video streaming applications. OCS is not a centralized nor a hierarchical collaborative scheme. Despite its design simplicity, OCS effectively uses an aggregate storage space and capability of distributed overlay nodes to cache popular videos and serve nearby clients. Moreover, OCS is light-weight and adaptive to clients' locations and request patterns. We also investigate other video caching techniques for overlay networks including both collaborative and non-collaborative ones. Compared with these techniques on topologies inspired from actual networks, OCS offers extremely low average service delays and approximately half the server load. OCS also offers smaller network load in most cases in our study. © Springer Science+Business Media, LLC 2007.",Cache collaboration; Overlay networks; Video caching; Video streaming,Digital libraries; Distance education; Multimedia services; Multimedia systems; Optimization; Servers; Digital video libraries; Overlay Caching Scheme (OCS); Server load; Video caching; Video streaming,Article,2-s2.0-34249085328,
pps,Collaborative e-test construction: Using predicted response-time and score distributions to improve reliability,2007,IFIP International Federation for Information Processing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247525501&doi=10.1007%2f978-0-387-69312-5_25&partnerID=40&md5=5d824157ac85810a0c9e7dd64967e71b,"Analysis of collaborative e-test construction identified the number of test-authors as the most important factor in test validity, while test reliability depends more on participation of an expert. Based on these findings, a collaborative e-test construction system was developed that uses predicted response-time and score distributions to improve the reliability of tests constructed by novice test-authors. A gamma distribution is used as the predicted response-time distribution, and a mixed model of binomial distributions is used as the predicted score distribution. An experiment in which a novice and an expert test-author each constructed tests by using and not using these predicted distributions showed that those constructed using them were more reliable, although those constructed by the expert had even higher reliability. © 2007 International Federation for Information Processing.",Collaborative e-test construction; Predicted response-time distribution; Predicted score distribution; Reliability,Information technology; Knowledge management; Reliability; Reliability analysis; Binomial distribution; Construction systems; Gamma distribution; Mixed modeling; Predicted score distribution; Response time distribution; Test reliability; Test validity; Testing,Article,2-s2.0-34247525501,
nng,A novel framework for test domain reduction using extended finite state machine,2007,2nd International Conference on Software Engineering Advances - ICSEA 2007,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47849084554&doi=10.1109%2fICSEA.2007.9&partnerID=40&md5=1692520d87145d0f764de9912c03faa8,"Test case generation is an expensive, tedious, and error-prone process in software testing. In this paper, test case generation is accomplished using an Extended Finite State Machine (EFSM). The proper domain representative along the specified path is selected based on fundamental calculus approximation. The pre/post-conditions of class behavior is derived from a continuous or piece-wise continuous function whose values are chosen from partitioned subdomains. Subsequent test data for the designated class can be generated from the selected test frames. In so doing, the domain is partitioned wherein reduced test cases are generated, yet insuring complete test coverage of the designated test plan. The proposed modeling technique will be conducive toward a new realm of test domain analysis. Its validity can also be procedurally proved by straightforward mathematical principles. © 2007 IEEE.",Domain analysis; Path testing; Software testing; Test partitioning,Calculations; Computer aided software engineering; Testing; Continuous functions; Domain analysis; Error-prone process; Extended finite state machine; Mathematical principle; Modeling technique; Path testing; Test case generation; Software testing,Conference Paper,2-s2.0-47849084554,
mvp,Comparative experiences with electronic process guide generator tools,2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149009938&doi=10.1007%2f978-3-540-72426-1_6&partnerID=40&md5=75f044f54ada784ad46ab4cd27968674,"The primary objective of all software engineering courses is to help students learn how to develop successful software systems with good software engineering practices. Various tools and guidelines are used to assist students to gain the knowledge as much as possible. USCs Center for Systems and Software Engineering (CSSE) has found that the keystone course in learning software engineering is a year-long real-client team project course. Over the last ten years, CSSE has evolved a set of guidelines for the course, and has experimented with early tests for creating electronic process guides for MBASE (Model-Based (Systems) Architecting and Software Engineering) Guidelines using Spearmint/EPG. Currently, CSSE has been developing and experimenting with Eclipse Process Framework's (EPF) to situate the LeanMBASE Guidelines. This paper reports our comparative experiences of using the earlier and current tools to generate the electronic process guidelines. In our analysis, we used the objectives defined by Humphrey and Kellner[17] to compare the process tools. The evaluation identifies some research challenges and areas for future research work. © Springer-Verlag Berlin Heidelberg 2007.",Electronic process guide generator tools; Process modeling tools,Computer systems; Curricula; Knowledge acquisition; Learning systems; Process engineering; Project management; Students; Eclipse Process Framework's (EPF); Electronic process guide generator tools; Process modeling tools; Software systems; Software engineering,Conference Paper,2-s2.0-37149009938,
ppr,Towards real-time distributed signal modeling for brain-machine interfaces,2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249069101&doi=10.1007%2f978-3-540-72584-8_127&partnerID=40&md5=531eb75585055ac5ad457bc6c74b0d99,"New architectures for Brain-Machine Interface communication and control use mixture models for expanding rehabilitation capabilities of disabled patients. Here we present and test a dynamic data-driven (BMI) Brain-Machine Interface architecture that relies on multiple pairs of forward-inverse models to predict, control, and learn the trajectories of a robotic arm in a real-time closed-loop system. A method of window-RLS was used to compute the forward-inverse model pairs in real-time and a model switching mechanism based on reinforcement learning was used to test the ability to map neural activity to elementary behaviors. The architectures were tested with in vivo data and implemented using remote computing resources. © Springer-Verlag Berlin Heidelberg 2007.",Brain-machine interface; Forward-inverse models,Closed loop systems; Distributed computer systems; Handicapped persons; Human computer interaction; Patient rehabilitation; Reinforcement learning; Brain-machine interface; Forward-inverse models; Neural activity; Real time systems,Conference Paper,2-s2.0-37249069101,
mvp,An empirical comparison between pair development and software inspection in Thailand,2006,ISESE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering,14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247351635&doi=10.1145%2f1159733.1159749&partnerID=40&md5=884016f375e54b8441e72fa294511485,"Although pair programming and software inspection have the common aim of minimizing the defects of the software product, each practice has its strengths and weaknesses. We need to understand their costs and benefits under given conditions to be able to select a practice to execute in a development project. The objective of this study is to compare the commonalities and differences between pair development and software inspection as verification techniques in Thailand. One classroom experiment and one industry experiment were conducted. The development effort and effect of quality were investigated with some additional calendar time comparisons. The classroom results showed that average development effort of the pair development group was 24% less than inspection group with the improved product quality. The industry experiment showed pair development to have about 4% more effort but about 40% fewer major defects. In addition, the impacts of cultural differences to the adoption of pair programming or software inspection in Thailand are discussed. Copyright 2006 ACM.",Empirical study; Pair programming; Peer review; Software inspection; Software process model; Software verification,Empirical study; Pair programming; Peer review; Software inspection; Software process model; Software verification; Computer programming; Optimization; Project management; Software engineering,Conference Paper,2-s2.0-34247351635,
pps,Effectiveness of collaborative e-test construction,2006,"Proceedings - Sixth International Conference on Advanced Learning Technologies, ICALT 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247112402&partnerID=40&md5=b68dd2a2bf84bfb6e733d09158b465f1,"This paper focuses on one of the features of e-testing, which has not been discussed until now, collaborative test construction by several test-authors in distant places. It is well known that collaborative work has many advantages. The main idea of this paper is to apply the unique advantages of collaborative work to e-test construction. The purpose of this paper is to identify the important factors in constructing high-quality tests. It does this by comparing test reliabilities and validities based on test theory for tests constructed by different numbers of test-authors (one, three, and five) with and without the experts of the test domain. The results show that the number of test-authors is the most important factor in improving test validity. They also show that test reliability does not depend on the number of test-authors but rather on the participation of an expert. © 2006 IEEE.",,Computer supported cooperative work; Expert systems; Reliability; E-testing; Test authors; Test reliability; E-learning,Conference Paper,2-s2.0-34247112402,
ksc,Grid aware HA-OSCAR,2005,Proceedings - International Symposium on High Performance Computing Systems and Applications,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-28744453426&doi=10.1109%2fHPCS.2005.28&partnerID=40&md5=a66d060feb88f9ea063ef6af463cf032,"Physicists today have employed grid technology to overcome various resource level hurdles. The collective resource utilization achieved through grid computing is critical to the overall computing capacity of the community and should be guaranteed. In an environment where job sites are cluster systems, a service node failure renders a whole system outage. Our grid-aware HA-OSCAR effort was motivated by the popularity of the cluster architecture in the Grid environment. We propose the high-availability architecture, HA-OSCAR, for cluster-based job sites in the grid environment. This architecture deals with fault tolerance at the service level complementing task-based solutions such as checkpoint/restart. We discuss various service availability issues related to the grid, some issues and preliminary results obtained while implementing the smart failover feature and the automated grid installation package. Our report entails the performance benefits achieved after applying the HA-OSCAR solution to the cluster components of the grid compared to regular Beowulf style cluster solutions. © 2005 IEEE.",,Beowulf style cluster solutions; Collective resource utilization; Grid technology; Task-based solutions; Computer architecture; Computer system recovery; Database systems; Problem solving; Resource allocation; Distributed computer systems,Conference Paper,2-s2.0-28744453426,
mvp,Improving quality through software process improvement in thailand: Initial analysis,2005,Proceedings - International Conference on Software Engineering,13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885938005&doi=10.1145%2f1083292.1083299&partnerID=40&md5=1f6110948c71a79e202df3491897aa5c,"For almost 10 years there have been attempts in Thailand to improve software quality by adopting western software process improvement models. Only 17 of the 380 companies in Thailand were able to implement software process models that we see here in the US. 14 out of the 17 companies were not able to improve their process to a higher level. Why were companies not successful in implementing these software process models? Did they find other ways to improve quality? The objective of this paper is to analyze the experiences of software developers, project managers and executive managers in implementing these software process models in Thailand. The results will show that cultural differences are a key factor to this problem. Thai people have different cultural values, which we will explore further in this paper. © 2005 ACM.",Cultural Difference; Software Process Improvement Model; Software Process Model,Cultural difference; Project managers; Software developer; Software Process Improvement; Software process improvement model; Software process modeling; Software process models; Software Quality; Computer software selection and evaluation; Industry; Management; Managers; Software engineering,Conference Paper,2-s2.0-84885938005,
wdc,Specification-based parameter-model interaction: Towards a correct reflection of memory characteristics in a DSM cluster simulation,2005,"Summer Computer Simulation Conference 2005, SCSC 2005, Part of the 2005 Summer Simulation Multiconference, SummerSim 2005",1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952587774&partnerID=40&md5=1d0e8f815e5922563fcfbce6e029feef,"A DSM (Distributed-Shared Memory) cluster is an attractive parallel computing platform for scientific research as it provides programming advantages within a scalable and cost-effective hardware solution. This benefit derives from the fact that a DSM system provides a shared-memory abstraction on top of a distributed-memory machine by caching data replicas locally. In this respect, a coherence protocol is a vital component responsible for assuring data consistency across all replicas. The design of coherence protocols impacts a DSM system in terms of both performance and accuracy. Performance is often measured via simulation and various verification techniques have been proposed to deal with protocol accuracy. Nevertheless, integrating accuracy verification into a DSM cluster simulation to ensure correct simulation results is still an open issue. In this paper, we address three properties of a coherence protocol (safety, liveness, and inclusion) without which errors may occur in the simulation results. We propose a Specification-based Parameter-Model Interaction (SPMI) technique to detect these cases in a particular DSM cluster simulation called DSIMCLUSTER. Our experimental results demonstrate that with SPMI, DSIMCLUSTER can ensure the coherence protocol properties and provides a correct reflection in the simulation model of the memory characteristics of real shared-memory and distributed-shared memory multiprocessors.",Coherence protocol; Distributed-Shared Memory; DSM cluster; Verification technique,Cluster simulations; Coherence protocol; Data consistency; Data replica; Distributed Memory; DSM cluster; Hardware solutions; Liveness; Memory multiprocessors; Parallel computing platform; Scientific researches; Shared memories; Simulation model; Verification techniques; Distributed computer systems; Hardware; Parallel architectures; Specifications; Computer simulation,Conference Paper,2-s2.0-77952587774,
scw,Distinguishing excitement movie clips using movementand pace features,2005,"Proceedings of the 5th IASTED International Conference on Visualization, Imaging, and Image Processing, VIIP 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887238756&partnerID=40&md5=ef17aed1f7a1e594061b25025da8b942,"Classifying multimedia data into semantic, such as emotional, categories is challenging and useful. Using two film artistic components: movement and pace, this paper proposes a method that can distinguish movie clips belonging to the excitement class from the others. Movement is represented by the Average Squared Motion Vector Magnitude feature, and pace is represented by the Average Shot Duration feature. Classification experiments with 101 data clips, excerpted from 24 Hollywood movies, are conducted, employing minimum distance and k-NN classifiers. The results show that the selected features can potentially separate the excitement movie clips from the others with above 90% accuracy.",Emotion-based classification; Image processing and analysis; Semantic classification; Video content analysis,Image processing and analysis; k-NN classifier; Minimum distance; Motion Vectors; Movie clips; Multimedia data; Semantic classification; Video-content analysis; Image processing; Motion pictures; Semantics; Visualization; Classification (of information),Conference Paper,2-s2.0-84887238756,
skn,Recognition of enhanced images,2005,"Proceedings of the 11th International Multimedia Modelling Conference, MMM 2005",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047245307&doi=10.1109%2fMMMC.2005.61&partnerID=40&md5=52ca7edaed8f0801878122364efe1008,"Image enhancement such as adjusting brightness and contrast is central to improving human visualization of images content. Images in desired enhanced quality facilitate analysis, interpretation, classification, information exchange, indexing and retrieval. The adjustment process, guided by diverse enhancement objectives and subjective human judgment, often produces various versions of the same image. Despite the preservation of content under these operations, enhanced images are treated as new in most existing techniques via their widely different features. This leads to difficulties in recognition and retrieval of images across application domains and user interest. To allow unrestricted enhancement flexibility, accurate identification of images and their enhanced versions is therefore essential. In this paper, we introduce a measure that theoretically guarantees the identification of all enhanced images originated from one. In our approach, images are represented by points in multidimensional intensity-based space. We show that points representing images of the same content are confined in a well-defined area that can be identified by a so-devised formula. We evaluated our technique on large sets of images from various categories, including medical, satellite, texture, color images and scanned documents. The proposed measure yields an actual recognition rate approaching 100% in all image categories, outperforming other well-known techniques by a wide margin. Our analysis at the same time can serve as a basis for determining the minimum criterion a similarity measure should satisfy. We discuss also how to apply the formula as a similarity measure in existing systems to support general image retrieval. © 2005 IEEE.",,Color images; Existing systems; Human judgments; Indexing and retrieval; Information exchanges; Similarity measure; User interests; Copying; Image processing; Medical imaging; Quality control; Image retrieval,Conference Paper,2-s2.0-34047245307,
ksc,Job-site level fault tolerance for cluster and grid Environments,2005,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249144002&doi=10.1109%2fCLUSTR.2005.347043&partnerID=40&md5=9cfbf1f125405ad2a8151ae2cb926ea7,"In order to adopt high performance clusters and grid computing for mission critical applications, fault tolerance is a necessity. Common fault tolerance techniques in distributed systems are normally achieved with checkpoint-recovery and job replication on alternative resources, in cases of a system outage. The first approach depends on the system's MTTR while the latter approach depends on the availability of alternative sites to run replicas. There is a need for complementing these approaches by proactively handling failures at a job-site level, ensuring the system high availability with no loss of user submitted jobs. This paper discusses a novel fault tolerance technique that enables the job-site recovery in Beowulf cluster-based grid environments, whereas existing techniques give up a failed system by seeking alternative resources. Our results suggest sizable aggregate performance improvement during an implementation of our method in Globus-enabled HA-OSCAR. The technique called ""Smart Failover"" provides a transparent and graceful recovery mechanism that saves job states in a local job-manager queue and transfers those states to the backup server periodically, and in critical system events. Thus whenever a failover occurs, the backup server is able to restart the jobs from their last saved state.",,Aggregates; Chlorine compounds; Computer systems; Errors; Fault tolerance; Fault tolerant computer systems; Grid computing; Normal distribution; Reliability; Telecommunication networks; Aggregate performance; Beowulf clusters; Cluster computing; Distributed systems; Fail over; Failed system; Fault tolerance techniques; GRID environments; High Availability; High-performance clusters; International conferences; Mission-critical applications; Recovery mechanisms; System outages; Quality assurance,Conference Paper,2-s2.0-50249144002,
ksc,Reliability-aware resource management for computational grid/cluster environments,2005,Proceedings - IEEE/ACM International Workshop on Grid Computing,10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749661027&doi=10.1109%2fGRID.2005.1542744&partnerID=40&md5=a211a5fdbe39c3629529847d5d22ff07,"The collective resource utilization achieved through grid computing is critical to the overall computing capacity of the collaborative community and should be guaranteed. Especially, in an existing environment where job sites are Beowulf cluster systems, a service node failure may render the whole system outage. Current grid fault tolerance techniques only address these issues in an opportunistic fashion. Thus, there is a need for complementing these approaches by proactively handling failures at a job-site level, ensuring the system high availability with no loss of user submitted jobs. Our grid-aware cluster resource management effort was motivated by the fact that a cluster turns into a popular job site in the computational rid environment. We propose a solution dealing with fault tolerance at the service level complementing the task-based solutions as being done in some recent studies. We discuss various service availability issues related to the grid, and preliminary results obtained while implementing the smart failover and transparent job-queue replication mechanism and the automated grid installation package. Our report entails the benefits outweighing acceptable overhead after implementing our proof-of-concept framework. © 2005 IEEE.",,Collective resource utilization; Computational rid environments; Grid computing; Proof-of-concept framework; Automation; Computer supported cooperative work; Failure analysis; Fault tolerant computer systems; Reliability; Distributed computer systems,Conference Paper,2-s2.0-33749661027,
ddp,Partial order reduction for detecting safety and timing failures of timed circuits,2005,IEICE Transactions on Information and Systems,9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26044463780&doi=10.1093%2fietisy%2fe88-d.7.1646&partnerID=40&md5=9c61d87e9efb734c86859a0c36386b10,"This paper proposes a partial order reduction algorithm for timed trace theoretic verification in order to detect both safety failures and timing failures of timed circuits efficiently. This algorithm is based on the framework of timed trace theoretic verification according to the original untimed trace theory. Consequently, its conformance checking supports hierarchical structure when verifying timed circuits. Experimenting with the STARI and DME circuits, the proposed approach shows its effectiveness. Copyright © 2005 The Institute of Electronics, Information and Communication Engineers.",Formal verification; Safety/timing failures; Timed circuits; Timed trace theory,Algorithms; Formal verification; Safety/timing failures; Timed circuits; Timed trace theory; Timing circuits,Article,2-s2.0-26044463780,
ksc,"The design and prototype of RUDA, a distributed Grid accounting system",2005,Lecture Notes in Computer Science,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944555857&doi=10.1007%2f11424857_4&partnerID=40&md5=27e0e7452a08f2ec3df58a22124ccdf2,"The Grid environment contains a large and growing number of widely distributed sites with heterogeneous resources. It is a great challenge to dynamically manage and account for usage data of Grid resources, such as computational, network, and storage resources. A distributed Resource Usage Data management and Accounting system (RUDA) is designed to perform accounting in the Grid environment. RUDA utilizes fully decentralized design to enhance scalability and supports heterogeneous resources with no significant impact on local systems. It can easily be integrated into Grid infrastructures and maintains the integrity of the Grid security features. © Springer-Verlag Berlin Heidelberg 2005.",,Computational methods; Data reduction; Data storage equipment; Distributed computer systems; Logic design; Resource allocation; Disitributed Grid accounting system; Grid resources; Heterogeneous resources; Resource Usage Data management and Accounting system (RUDA); Software prototyping,Conference Paper,2-s2.0-24944555857,
ksc,The Earth System Grid: Supporting the next generation of climate modeling research,2005,Proceedings of the IEEE,130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18244408069&doi=10.1109%2fJPROC.2004.842745&partnerID=40&md5=3a6ed116d960eee3e2fbc0756080a434,"Understanding the earth's climate system and how it might be changing is a preeminent scientific challenge. Global climate models are used to simulate past, present, and future climates, and experiments are executed continuously on an array of distributed supercomputers. The resulting data archive, spread over several sites, currently contains upwards of 100 TB of simulation data and is growing rapidly. Looking toward mid-decade and beyond, we must anticipate and prepare for distributed climate research data holdings of many petabytes. The Earth System Grid (ESG) is a collaborative interdisciplinary project aimed at addressing the challenge of enabling management, discovery, access, and analysis of these critically important datasets in a distributed and heterogeneous computational environment. The problem is fundamentally a Grid problem. Building upon the Globus toolkit and a variety of other technologies, ESG is developing an environment that addresses authentication, authorization for data access, large-scale data transport and management, services and abstractions for high-performance remote data access, mechanisms for scalable data replication, cataloging with rich semantic and syntactic information, data discovery, distributed monitoring, and Web-based portals for using the system. © 2005 IEEE.",Climate modeling; Data management; Earth System Grid (ESG); Grid computing,Computational complexity; Computer simulation; Data reduction; Database systems; Distributed computer systems; Earth (planet); Mathematical models; Metadata; Monitoring; Supercomputers; Climate modeling; Data management; Earth System Grid (ESG); Grid computing; Climatology,Conference Paper,2-s2.0-18244408069,
ksc,Communication state transfer for the mobility of concurrent heterogeneous computing,2004,IEEE Transactions on Computers,6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11944275313&doi=10.1109%2fTC.2004.73&partnerID=40&md5=d3448892740666c0ceb049574f514355,"In a dynamic environment where a process can migrate from one host to another host, communication state transfer is a key issue of process coordination. This paper presents a set of data communication and process migration protocols to support communication state transfer in a dynamic, distributed parallel environment. The protocols preserve the semantics of point-to-point communication; they guarantee message delivery, maintain message ordering, and do not introduce deadlock when blocking send or receive operations are performed during process migration. Analytical proofs and prototype implementation are conducted to confirm the correctness of the protocols. Analytical and experimental results show the proposed design is valid and has a true potential in network computing. © 2004 IEEE.",Communication protocol; Distributed and parallel processing; Point-to-point communication; Process migration,Algorithms; Communication channels (information theory); Distributed computer systems; Network protocols; Parallel processing systems; Communication state transfer; Point-to-point communication; Process migration; Data communication systems,Article,2-s2.0-11944275313,
wdp,"A case for a generalized periodic broadcast server: Design, analysis, and implementation",2004,Proceedings - International Symposium on Applications and the Internet,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2642586487&doi=10.1109%2fSAINT.2004.1266107&partnerID=40&md5=7abf3acde2767000b770ce5cd3fe98f8,"Periodic broadcast is an effective paradigm for large-scale dissemination of popular videos. Considerable research efforts have gone into designing many excellent periodic broadcast protocols in terms of minimizing the server network bandwidth and the client resources. However, there are only a few implementations of periodic broadcast protocols available. This is probably because little has been documented on how the memory and disk bandwidth resources of a periodic broadcast server should be allocated. In this paper, we present a Generalized Periodic Broadcast Server (GPBS) model that supports any periodic broadcast protocol. Based on this model, we formulate and solve a new optimization problem whose solution provides insights into the server's memory and disk resources allocation. We also discuss our prototype implementation of GPBS. Our work facilitates future implementation and deployment of many existing periodic broadcast protocols.",,Bandwidth resources; Broadcast protocols; Generalized periodic broadcast server (GPBS); Optimization problems; Computation theory; Data reduction; Data storage equipment; Network protocols; Problem solving; Software prototyping; Servers,Conference Paper,2-s2.0-2642586487,
ddp,Partial order reduction for detecting safety and timing failures of timed circuits,2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048832522&doi=10.1007%2f978-3-540-30476-0_28&partnerID=40&md5=abf756915e57bf5796956c9bb72715a1,"This paper proposes a partial order reduction algorithm for timed trace theoretic verification in order to detect both safety failures and timing failures of timed circuits efficiently. This algorithm is based on the framework of timed trace theoretic verification according to the original untimed trace theory. Consequently, its conformance checking supports hierarchical verification. Experimenting with the STARI circuits, the proposed approach shows its effectiveness. © Springer-Verlag 2004.",,Algorithms; Model checking; State space methods; Conformance checking; Hierarchical verification; Partial order reductions; Safety failures; Timed circuits; Timing failures; Trace theoretic verification; Trace theory; Timing circuits,Article,2-s2.0-35048832522,
wdp,Distributed core selection with QoS support,2004,IEEE International Conference on Communications,6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4143146419&doi=10.1109%2ficc.2004.1312895&partnerID=40&md5=eb371ca3058834afcd3eaeb9f8283955,"Core-based routing with Quality of Service (QoS) support is essential to facilitate multi-sender multimedia multicast applications such as video conferencing and virtual collaboration applications. In this paper, we introduce a new distributed core selection protocol that has the following desirable properties. First, the protocol utilizes a new distributed primary core selection algorithm that selects as many primary cores per multicast group as necessary to maximize the number of group members with satisfied QoS requirements. Second, the protocol is distributed, preventing a single router from becoming a hot spot and a single point of failure during core selection. Last, the protocol employs a distributed backup core selection algorithm to provide quick recovery should some primary cores fail. Our analytical experiments show that the proposed protocol significantly satisfies more group members with noticeably less communication overhead than a recent core selection algorithm with QoS support using a single core.",Core-based Routing; Multicast; Quality of Service,Algorithms; Bandwidth; Computer system recovery; Data transfer; Electric power distribution; Multicasting; Multimedia systems; Network protocols; Routers; Video conferencing; Backup; Bottlenecks; Core-based routing; Distributed core selection protocol (DCSP); Quality of service,Conference Paper,2-s2.0-4143146419,
wdp,Core selection with end-to-end QoS support,2004,Proceedings of the ACM Symposium on Applied Computing,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442630496&doi=10.1145%2f967900.967972&partnerID=40&md5=c59888ebafed71c224b21ec7fa7746ac,"Core-based routing with Quality of Service (QoS) support is essential to facilitate multi-sender multimedia multicast applications such as video conferencing and virtual collaboration applications. In this paper, we introduce (i) a new application-level service class framework that allows group members to easily indicate their desired service quality and (ii) the use of as many cores per group as necessary in core-based routing to maximize the number of group members with satisfied QoS requirements. Under the service class framework, we formulate the novel core selection problem that selects as many cores as necessary while maximizing the number of satisfied group members. We propose a new core selection algorithm to address the problem and provide a complete core selection protocol using the algorithm. Experimental results show that our core selection algorithm performs as well as the optimal algorithm and significantly outperforms a recent core selection algorithm with QoS support using a single core.",Core-based Routing; Multicast; Quality of Service,Algorithms; Multicasting; Multimedia systems; Network protocols; Optimization; Problem solving; Quality of service; Routers; Video conferencing; Core-based routing; Multicast; Virtual collaboration; Computer science,Conference Paper,2-s2.0-2442630496,
ksc,An ontology for scientific information in a Grid environment: The earth system Grid,2003,Proceedings - CCGrid 2003: 3rd IEEE/ACM International Symposium on Cluster Computing and the Grid,23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-15944408322&doi=10.1109%2fCCGRID.2003.1199424&partnerID=40&md5=c22f278c6ea345ed108bf05e82935dfc,"In the emerging world of Grid Computing, shared computational, data, other distributed resources are becoming available to enable scientific advancement through collaborative research and collaboratories. This paper describes the increasing role of ontologies in the context of Grid Computing for obtaining, comparing and analyzing data. We present ontology entities and a declarative model that provide the outline for an ontology of scientific information. Relationships between concepts are also given. The implementation of some concepts described in this ontology is discussed within the context of the Earth System Grid II (ESG)[1]. © 2003 IEEE.",climate; earth sciences; Grid; ontologies; Ontology,climate; Collaborative research; Declarative models; Distributed resources; Earth system grids; Grid; Grid environments; Scientific information; Cluster computing; Earth sciences; Ontology; Grid computing,Conference Paper,2-s2.0-15944408322,
ksc,HPCM: A pre-compiler aided middleware for the mobility of legacy Code,2003,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944901368&doi=10.1109%2fCLUSTR.2003.1253314&partnerID=40&md5=a285bf5375010cc867ef45adddcdc5b5,"Mobility is a fundamental functionality of the next generation internet computing. How to support mobility for legacy codes, however, is still an issue of research. The key to solve this outstanding issue is the support of heterogeneous process migration. During the last few years, we have successfully developed mechanisms to support heterogeneous process migration of legacy codes written in C, C++, and Fortran. We present in this paper the design of the High Performance Computing Mobility (HPCM) middleware, the development and implementation of its key components, pre-compiler and its static libraries. Due to the similarity between process migration and checkpointing, the pre-compiler not only makes automatic process migration of legacy codes feasible, but also supports dynamic heterogeneous checkpointing. We perform a set of tests and compare experimental results with Porch, a well-known portable heterogeneous checkpointing system. The experimental results show that our methods are feasible, efficient and very promising. © 2003 IEEE.",,Cluster computing; Codes (symbols); Computer architecture; Middleware; Program compilers; Check pointing; Compiler-aided; High performance computing; Legacy code; Next generation Internet; Process migration; C++ (programming language),Conference Paper,2-s2.0-84944901368,
yao,Job Application on the Web with Personality-Job Fit Model,2002,Proceedings of the Joint Conference on Information Sciences,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642298952&partnerID=40&md5=991be36f6a614cfc718ebec57e52988e,"This research is to model the prototype of Job Application on the web that is related to personality characteristics. The model results will be generated by the innovation method which is shown in a different style from the existing job search web sites. In the results of the majority of the existing systems the records displayed are not prioritized or weighted by information, but are sorted by ID, alphabet, date, etc. So, it gives too many results which are difficult to compare within a short period of time. The results of this project model are derived from querying, which is composed of the personality characteristics and general requirements and will be displayed as weighted results as a percentage sorted in descending order. So that the most matched results will be a percentage at the top of the list and the rest follow. We have tested our model by using real data from applicants who apply for jobs with a Thai company. The result shows that we are able to give a better match than just comparing basic qualifications.",Employment; Internet Job Search; Personality Characteristics; Personality-Job Fit; Recruitment,Artificial intelligence; Employment; Fuzzy sets; Information analysis; Websites; Internet job search; Personality characteristics; Personality-job fit; Recruitment; Job analysis,Conference Paper,2-s2.0-1642298952,
ksc,Data collection and restoration for heterogeneous process migration,2002,Software-Practice and Experience,16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037173433&doi=10.1002%2fspe.464&partnerID=40&md5=6c403f53ab66e56d9c9596021a0f15a9,"This study presents a practical solution for data collection and restoration to migrate a process written in high-level stack-based languages such as C and Fortran over a network of heterogeneous computers. We first introduce a logical data model, namely the Memory Space Representation (MSR) model, to recognize complex data structures in process address space. Then, novel methods are developed to incorporate the MSR model into a process, and to collect and restore data efficiently. We have implemented prototype software and performed experiments on different programs. Experimental and analytical results show that: (1) a user-level process can be migrated across different computing platforms; (2) semantic information of data structures in the process's memory space can be correctly collected and restored; (3) costs of data collection and restoration depend on the complexity of the MSR graph in the memory space and the amount of data involved; and (4) the implantation of the MSR model into the process is not a decisive factor of incurring execution overheads. With appropriate program analysis, we can practically achieve low overhead.",Checkpointing; Data collection and restoration; Distributed computing; Heterogeneous process migration; Network process migration,C (programming language); Computer programming; Computer simulation; Data acquisition; Data structures; Data transfer; Distributed computer systems; Fault tolerant computer systems; FORTRAN (programming language); Program compilers; Semantics; Sorting; Checkpointing; Data collection; Data restoration; Heterogeneous process migration; Memory space representation; Network process migration; Software engineering,Article,2-s2.0-0037173433,
ksc,"SNOW: Software systems for process migration in high-performance, heterogeneous distributed environments",2002,Proceedings of the International Conference on Parallel Processing Workshops,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878719617&doi=10.1109%2fICPPW.2002.1039781&partnerID=40&md5=ada3a3202a321db019aa67f2e871c657,"This paper reports our experiences on the scalable network of workstation (SNOW) project, which implements a novel methodology to support user-level process migration for traditional stack-based languages such as C and Fortran in heterogeneous distributed environments. Our methodology addresses the three outstanding problems of transferring execution state, memory state, and communication state. The concepts of migration point analysis and buffered data transfer mechanism are proposed for execution state migration. A memory space representation model is introduced to obtain the machine-independent format of the underlying data structures for memory state migration. Finally, process migration and communication protocols are developed to migrate the communication state and maintain the functionality and correctness of data communication. A coordinated software system consisting of compilation and runtime systems was developed based on these new mechanisms. The runtime systems include a runtime library and communication protocols. Sequential and parallel programs with different data structures and computing requirements are tested. Experimental results confirm our design analysis. They advocate the value of the migration methodology for distributed network computing. © 2002 IEEE.",,C (programming language); Computer software; Data structures; Data transfer; Snow; Data-communication; Distributed environments; Distributed network computing; Process migration; Representation model; Scalable networks; Stack-based languages; Transfer mechanisms; Distributed computer systems,Conference Paper,2-s2.0-84878719617,
yao,Recruitment filtering with personality-job fit model,2002,"Proceedings - International Conference on Information Technology: Coding and Computing, ITCC 2002",3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18844371454&doi=10.1109%2fITCC.2002.1000358&partnerID=40&md5=b00e49a82686da7105d1c36f3b41c81d,"Many job applications on the Web provide only simple Boolean comparison using only basic information requirements (i.e., degree, age) for searching and matching jobs to candidates. The net effect is too many 'hits'. We showed that personality is an important factor for the hiring process and satisfaction of employers and employees. In designing better filters for searching and matching a job application on the Web, we considered this factor with general job requirements and then combined it with our matching process. We have tested our model using data obtained from applicants who applied for jobs with a company in Thailand. The results show that we are able to give a better match than that achieved using basic qualifications only. © 2002 IEEE.",Employment; Information filtering; Information filters; Information technology; Internet; Job design; Matched filters; Mathematics; Recruitment; Testing,Employment; Industrial relations; Information technology; Internet; Matched filters; Mathematical techniques; Testing; Hiring process; Information requirement; Job application; Job design; Matching process; Net effect; Recruitment; Thailand; Information filtering,Conference Paper,2-s2.0-18844371454,
ksc,Data collection and restoration for heterogeneous process migration,2001,"Proceedings - 15th International Parallel and Distributed Processing Symposium, IPDPS 2001",7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981240585&doi=10.1109%2fIPDPS.2001.924992&partnerID=40&md5=b5848429db0c24a3f8b74cbb8dd6fb00,"This study presents a practical solution for data collection and restoration to migrate a process written in high level stack-based languages such as C and Fortran over a network of heterogeneous computers. We study a logical data model which recognizes complex data structures in process address space. Then, novel methods are developed to incorporate the model into a process and to collect and restore data efficiently. We have implemented a prototype software and performed experiments on different programs. Experimental and analytical results show that (I) a user-level process can be migrated across different computing platforms, (2) semantic information of data structures in the process's memory space can be correctly collected and restored, (3) the costs of data collection and restoration depend on the complexity of the logical model representing the process's data structures and the amount of data involved and (4) the implantation of the data collection and restoration mechanisms into the process is not a decisive factor of incurring execution overheads; with appropriate program analysis, we can achieve practically low overhead. © 2001 IEEE.",,C (programming language); Complex networks; Data structures; High level languages; Restoration; Semantics; Software prototyping; Space platforms; Complex data structures; Computing platform; Heterogeneous computers; Logical data models; Practical solutions; Restoration mechanism; Semantic information; Stack-based languages; Data acquisition,Conference Paper,2-s2.0-84981240585,
ksc,Communication state transfer for the mobility of concurrent heterogeneous computing,2001,Proceedings of the International Conference on Parallel Processing,6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898132672&doi=10.1109%2fICPP.2001.952087&partnerID=40&md5=647c5d7addd87379ed6cdca63377588d,"In a dynamic environment, where a process can be migrated from one host to another host, communication state transfer is a key issue of process coordination. This paper presents algorithms for data communication and migration protocols to support communication state transfer in a dynamic, distributed parallel environment. These algorithms collectively presence the semantics of the communication and are practical for large-scale distributed systems. The assumptions and validity of our solution are discussed. Based on our early results in process migration, we implement a prototype system for process state transfer. Experimental results confirm our design is valid and has a true potential in practice. © 2001 IEEE.",Computer network management; Computer science; Concurrent computing; Data communication; Distributed computing; Large-scale systems; Protocols; Prototypes; Snow; Sun,Computer networks; Computer science; Convolutional codes; Distributed parameter networks; Information management; Large scale systems; Network management; Network protocols; Semantics; Snow; Sun; Communication state transfer; Concurrent computing; Data-communication; Dynamic environments; Heterogeneous computing; Large-scale distributed system; Parallel environment; Prototypes; Distributed computer systems,Article,2-s2.0-84898132672,
ksc,A protocol design of communication state transfer for distributed computing,2001,Proceedings - International Conference on Distributed Computing Systems,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035009489&partnerID=40&md5=b1f6aaf27bfc53712ece8c3b5517ea9e,"This paper presents the design of a communication state transfer protocol to support process migration in a dynamic, distributed computing environment. In our design, processes in distributed computation communicate one another via message passing and are migration-enabled. Due to mobility, mechanisms to maintain reliability and correctness of data communication are needed. Following an event-based approach, such mechanisms are derived to handle various communication situations when a process migrates. These mechanisms collectively preserve the semantics of the communication and support efficient communication state transfer.",,Data communication systems; Data handling; Distributed computer systems; Dynamic programming; Fault tolerant computer systems; Mobile computing; Resource allocation; Semantics; Communication state transfer protocol; Data access locality; Load balancing; Migration protocol; Resource sharing; Network protocols,Conference Paper,2-s2.0-0035009489,
ksc,Memory space representation for heterogeneous network process migration,1998,"Proceedings of the 1st Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing, IPPS/SPDP 1998",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044816770&doi=10.1109%2fIPPS.1998.670020&partnerID=40&md5=1a6a71f349fca56550c275d71951e51a,"A major difficulty of heterogeneous process migration is how to collect advanced dynamic data-structures, transform them into machine independent form, and restore them appropriately in a different hardware and software environment. In this study we introduce a data model, the Memory Space Representation (MSR) model, to recognize complex data structures in program address spaces. Supporting mechanisms of the MSR model are also developed for collecting program data structures and restoring them in a heterogeneous environment. The MSR design has been implemented under a prototype heterogeneous process migration environment. Pointer-intensive programs with function and recursion calls are tested. Experimental results confirm that the newly proposed design is feasible and effective for heterogeneous network process migration. © 1998 IEEE.",,Data structures; Heterogeneous networks; Address space; Advanced Dynamics; Complex data structures; Hardware and software; Heterogeneous environments; Network process; Process migration; Supporting mechanisms; Data acquisition,Conference Paper,2-s2.0-85044816770,
ksc,Memory Space Representation for heterogeneous network process migration,1998,"Proceedings of the International Parallel Processing Symposium, IPPS",5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031702260&doi=10.1109%2fIPPS.1998.670020&partnerID=40&md5=72a74491c63b2b5660cf8fe51c8c50ef,"A major difficulty of heterogeneous process migration is how to collect advanced dynamic data-structures, transform them into machine independent form, and restore them appropriately in a different hardware and software environment. In this study we introduce a data model, the Memory Space Representation (MSR) model, to recognize complex data structures in program address spaces. Supporting mechanisms of the MSR model are also developed for collecting program data structures and restoring them in a heterogeneous environment. The MSR design has been implemented under a prototype heterogeneous process migration environment. Pointer-intensive programs with function and recursion calls are tested. Experimental results confirm that the newly proposed design is feasible and effective for heterogeneous network process migration.",,Heterogeneous networks; Memory space representation (MSR); Computer hardware; Computer software; Data acquisition; Data structures; Computer networks,Conference Paper,2-s2.0-0031702260,
skn,SCPlib: A concurrent programming library for programming heterogeneous networks of computers,1998,"1998 IEEE Information Technology Conference: Information Environment for the Future, IT 1998",8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002860139&doi=10.1109%2fIT.1998.713403&partnerID=40&md5=6ffdbaef01b41edd47390364347682df,"This paper describes the Scalable Concurrent Programming Library (SCPlib), basic technology that supports irregular applications on scalable concurrent hardware and heterogeneous computing environments. The library is optimized to take advantage of the best available underlying communication and synchronization on a variety of high-performance multicomputers, shared-memory multiprocessors, and networked PCs and workstations. It also provides a framework for heterogeneous communication and file I/O, load balancing, and dynamic granularity control. The effectiveness of the library has been demonstrated on a variety of industrial strength applications. © 1998 IEEE.",,Heterogeneous networks; Communication and synchronizations; Concurrent programming; Granularity controls; Heterogeneous communication; Heterogeneous computing; Industrial strength; Irregular applications; Shared memory multiprocessor; Computer systems programming,Conference Paper,2-s2.0-0002860139,
yao,Detection of access control flaws in a distributed database system with local site autonomy,1997,"Proceedings of the International Database Engineering & Applications Symposium, IDEAS",2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031334021&partnerID=40&md5=4efddb2446953532555c7d7ed0944dbf,This paper addresses the authorization of access privileges to users in a local database management system (DBMS) that is interoperating in a distributed database system (DDBMS). Our focus is on the potential in a next generation security system for delegation access concepts such as role hierarchy delegation to violate the security policy in a local database system that is part of the distributed system. This kind of violation can be categorized as a violation of local autonomy. We propose a labeled graph model to detect such a problem.,,Data acquisition; Data structures; Security of data; Hierarchy delegation; Labeled graph model; Local site autonomy; Distributed database systems,Conference Paper,2-s2.0-0031334021,
yao,Integrated search engine,1997,"Proceedings of the IEEE Knowledge & Data Engineering Exchange Workshop, KDEX",13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031367529&partnerID=40&md5=dcaa427df60f9ecaf14225b57ce8b484,"Searching for information has become an important part of the Internet life. New and more powerful search engines are being built everyday for public use. Even though the combined power of these engines are far more advanced than their past alternatives, an average user does not seem to be benefiting fully from the state-of-the-art of search technologies. In this paper, we develop an integrated search engine architecture called I.SEE (Integrated SEarch Engine) that combines the capabilities of search engines. I.SEE facilitates construction of complex search queries with the help of a uniform interface, transforms the queries by relaxing them when necessary to queries that can be supported by search engines, translates relaxed queries to the specific query language syntax used by different systems and executes multiple search requests in parallel to improve query response time. I.SEE is a flexible and extensible system that adapts to the changes in the search engines and provides users with customization options for developing specialized query interfaces. A current version of the system is available at: www.cs.rpi.edu/research/isee/.",,Query languages; Response time (computer systems); User interfaces; Wide area networks; Integrated search engine; Online searching,Conference Paper,2-s2.0-0031367529,
ksc,MpPVM: A software system for non-dedicated heterogeneous computing,1996,Proceedings of the International Conference on Parallel Processing,7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003233298&doi=10.1109%2fICPP.1996.538578&partnerID=40&md5=abd9e4c96563693489ff2632c3df917e,"This paper presents the design and preliminary implementation of MpPVM, a software system that supports process migration for PVM application programs in a non-dedicated heterogeneous computing environment. New concepts of migration point as well as migration point analysis and necessary data analysis are introduced. A preliminary implementation of MpPVM and its experimental results are also presented, showing the correctness and promising performance of our process migration mechanism in the scalable non-dedicated heterogeneous computing environment. © 1996 IEEE.",,Heterogeneous computing; Process migration; Software systems; Application programs,Conference Paper,2-s2.0-0003233298,
mvp,Data Visualization For Project Manager,2018,,,,"In organizations involved in software development Corporate. 
Organization Managements are complex. And have a diversity of processes. Because 
these organizations are operating a large number of projects. Each project is different 
in size of project. Characteristics of software development. The management of the 
organization involved with this software development. Need to be able to see 
the overall implementation of all projects in the organization. For planning both
manpower and schedule of attendance of each project. But at present, the overall 
management of the project is difficult to access. Each team is managed. And the 
implementation of independent projects. There are no tools to help manage the 
organization.

Dash+ is a web application designed to help solve all project 
management problems. It helps to see the remaining Change of scope and workload.
The movement of the job increases or decreases the project. Displayed in the form 
of a line chart. This will help to see the trends of each team in the overall.","Project mangers, Project management, Visualization, Dash Board, Dash+",,Special Project,,
mvp,,2018,,,,"Cervical cancer is a type of cancer that occurs in the cells of the cervix — 
the lower part of the uterus that connects to the vagina.
Early-stage cervical cancer generally produces no signs or symptoms.
Irregular vaginal bleeding after intercourse, between periods, or after menopause is mostly found in people suffering from the cancer.
At present, breast and cervical cancers are considered to be the most and the secondly found cancers among women in Thailand.
Every year, the number of women suffering from cervical cancer is increasing.
Meanwhile, the number of doctors specialized in cervical cancer is comparatively small, resulting in lack of proper treatment.
Developers have designed android Colposcopy Application to diagnose cervical cancer. All information is provided by medical experts.
This application aims to enable medics and volunteers to diagnose cervical cancer in other areas without requiring patients to go to central hospital.
Colposcopy Application connects to camera designed for cervix to help experts do the job.","Application, Colposcope",,Special Project,,
mvp,Traffy Foundue application user interaction improvement,2018,,,,"Nowaday, in Thammasat University has many problems such as garbage problem, electric problem, traffic problem and a lot of problems. Which problems that happen have many departments manage and solve the problems but some problems couldn't solve because there are only some staffs who know the way to report the problem. Therefore, when some people see the problem, they can't report that problem. There are another reason is the process of report is quite complicated. Therefore, it's difficult for personnel and students in the university to report the problem.
	
	The developers have created a complaint application for use in the Faculty of Science, Thammasat University. For the benefit of personnel and students along with agencies that are responsible for taking care of various parts of the university. 
The application is able to report problems within the university. It cans detect current position of users when they are reporting for know the location of the problem clearly. When users reported the problem, they can follow status of the problem that know the progress of operation. We expect this application will be useful for agencies and personnel in the Faculty of Science, Thammasat University","report, problem, detect",,Special Project,,
mvp,CSTU Chit Chat,2018,,,,"It presently almost all communications are on a digital platform. And connect more freely. To make communication easier and faster to reach groups of users on online media. The technology to help communicate is chatbot.
         Whether it is a questionnaire. Business contacts, such as sales of goods. The use of people as intermediaries to communicate or interact with users, can cause delays in communication and It may be too late for the needs of the user. Chatbot has more role in communication and is capable of interacting with users as close to human.",,,Special Project,,
mvp,Toy Exchange Application,2018,,,,"Toys can help improve the development and learning of children. But buying toys that are suitable for children of different ages can be costly. Sharing unused toys is another way to reduce costs. In many homes with children with toys that do not fit the age can be share their toys to people who want it.
We have seen these problems, so we have designed and developed a platform for exchanging toys on mobile applications. For users to find the toy they need and redeem the toy using a coin instead of money to reduce the chance of fraud. They can also share their toys with others. It creates a social exchange that can reduce the cost of buying toys and may help reduce the amount of trash generated by children's toys.",,,Special Project,,
mvp,DEmotion,2018,,,,"Mood Chart is a tool used by psychiatrists to track the mood of a patient. And patients use to track their emotions. In foreign country, there have an application to record and track emotional graphs. But while Thailand does not have an application to record emotional graphs that support Thai patient.
So, for Thai patients to use. Developers have an idea to create an application for record emotions that support Thai patients.","Mood Chart, Mental Health, Major depressive disorder, Bipolar disorder",,Special Project,,
pkl,Tracking system with IP cameras.,2018,,,,"In this project, we want to create a website that uses image processing technology which supports any IP camera. That website can receive up to four cameras input at the same time, then the system will detect every person that pass the cameras and process the density of population on the building layout in form of heatmaps. User can also see the path each person passed in the information table and user are able to select to see the direction of each person on building layout by clicking the button next to each person in the table.","Image processing, IP cameras, Multi-camera tracker",,Special Project,,
pkl,Basic symptom diagnosis chatbot for dog and cat,2018,,,,"Care of dog and cat when they begin to have abnormal symptoms or behavior. Especially The herdsmen don’t bring the dog and the cat to see veterinarian immediately because of the consulting and treatment price is high, so the herdsmen often take care the dog and the cat by themselves. Most of herdsmen don’t have knowledge about primary care before go to the veterinarian. The effect, it makes the dog and the cat worse and die. 
To decrease the problem above, we found the preliminary diagnosis for dogs and cats project by chat bot. By gathering the symptoms and behavior of dogs and cats from documents which were examined by experts. The data will keep in the database for analysis. The chat bot will ask the herdsmen how your dog and cat have symptoms or behavior to keep information to analysis which disease is most appropriate. After that the chat bot will show the disease and primary care method before see the veterinary. However, if they don’t have serious disease, they will advise how to care. The chat bot will help the herdsmen can take care properly to dog and cat. It helps the herdsmen understand the behavior and symptoms of dogs and cats well.",,,Special Project,,
pkl,Lost and Found System,2018,,,,"Lost and Found System project is intended to study this system on web
applications and develop them. It is a multi-platform system. The main functions are
as follows: the first page showing the news of all lost. There is a category of lost and
found. The user can enter the details by entering details such as phone number, details
photo of lost thelocation of found or lost items using the Google Map API.The system
has function search through text and users can use a map. In addition, there are read
detail news. Lost and Found System will be useful application for whoever use it.","Lost and Found, Web Application, Responsive Website",,Special Project,,
pkl,Facebook messenger chatbots for E-commerce,2018,,,,"Nowadays technology has developed a lot. Trading not only limited at face 
to exchange goods.  Online channel or E-Commerce popular a lot right now. 
Where ever you are, if you can connect to the internet you will able to exchange too.
Because E-Commerce can reduce a limitation of distance and time, that make seller 
have more customer from many places. So seller will not be able to serve every 
customer. And buyers have more options to buy. If the seller does not provide 
satisfactory service, customer may switch to buy with other vendors. This will make 
the seller lose revenue.
	Therefore, we have developed Facebook messenger chatbot for E-commerce 
for help seller reply customer. The message that will be used in the reply will come 
from the message of the customer who has chat with the chatbots, then cut the word 
with a tool called. go-thaiwordcut. After cut message will be analyzed for features, 
what type of message should be included and what kind of response should be provided 
through some algorithms to obtain a customer response. In addition, the chatbot also 
reduces the limitations of humans to rest. Chatbot is able to work 24 hours a day, 
that make sure whatever time that customer comes in, they will be served.",,,Special Project,,
pks,TUcoin,2018,,,,"Cashless Systems in Thammasat University ( TUCoin ) is a complete food trading system that using digital currency. It will work with food centers in this campus .The system consists of four main functions: the buyer system, the seller system, the exchange system and the blockchain  database. Buyers and sellers can trade between their self for using digital currency that comes from the concept of bitcoin. The coin  will be exchanged for real money through the central system that call “exchange system”. All of transaction in this system  will also store in blockchain database. In this project consists of information on the design of various parts of the system, the capabilities of the system, tools and technologies used to develop the system. Including a diagram showing the operation of the system. This project aims to make Thammasat University is a first model cashless university. Finally, I hope readers will get more insight and and can use knowledge from this project to develop the desired system in the future","Blockchain, Bitcoin, Digital Currency",,Special Project,,
pks,AR for technical support,2018,,,,"The industry is highly competitive nowadays. Many of industry entrepreneurs see the benefits of manufacturing technology that can reduce time and cost of production and 
maximize profit. They have adapted their strategies and use technologies instead of human to improve production output, but the problem is that machine can be damaged or 
broken and sometime that problem is too complicated to fix. Repairing such machines requires specialized technician that is very hard to find. So they have to spend a lot of time 
and budget to find and employing specialist for that machine to work.
The developer team develop Communication System for Technical Support to make direct contact with specialized technician for specific machine. 
This mobile application can mark and store photo of the broken part and it repair instruction from specialized technician. This feature will reduce the waiting time for machine maintenance 
and reduce travel expenses for technician also improve performance for our own technician.",,,Special Project,,
pkw,Face Detection,2018,,,,"Communication and Information Systems can develop into interaction between humans and computers. For example, face detection is a function of the cammera that detects human faces. It can be applied to finding location of the human faces in a frame or images.
Therefore, I am interested to study face detection algorithm that use images to predictions with convolutional neural network. It can help to increase efficiency of face detection
In conclusion, Convolutional neural network can help to increase training efficiency in higher resolution pictures. When I test image again, It can help to improve result of face description very well.","convolutional neural network, image description, image segmentation",,Special Project,,
ppr,performance study of truth inference mechanisms in spatial and temporal crowdsourcing systems,2018,,,,"Crowdsourcing Systems is a system that receives information from user reports. The mobile computing devices such as smartphones, notebooks are used in many different ways. They can send location and time information. The study and development of Crowdsourcing Systems based on space and time called Spatio-temporal Crowdsourcing. It uses for store and analyze information, such as traffic reporting. It allows users to report traffic conditions based on their time and space.
But the key problem is that when a lot of workers enter the system and answer differently. How will the truth answer be chosen? To get the truth answer with the most accurate. At present, there is a research study on the comparison of efficiency of truth inference. However, not comparable in Spatio-temporal Crowdsourcing
There are restrictions on the information that will be studied From this problem, we have studied the comparison of efficiency of inference in space and time
Majority Voting is the algorithm that selects the most of votes as the truth answer of the system and EM Algorithm. Calculates the probability that the user responds. The user's weight is adjusted by looking at the response history. Accuracy and  F1-score are used in the indicators. In this study, performance comparison is based on time and space reporting. This project will create a simulation tool for reporting on time and space systems, with the ability to adjust the parameters of interest. Assign the system to a delegated problem with worker selected tasks
There are two types of problems in the system: single answers, multiple answers, and numerical problems. Workers in the system are classified into two types of workers. And the worker who accidentally disrupt the system. For general system, there are two types of workers: beginners and expert reporters. It also studies the distribution patterns of user positions. There are two types of data distribution: Uniform Distribution with Gaussian Distribution.
The results showed that when the amount of space increased multiply The 
efficiency of collecting answers is significantly less. Even if the worker is the novice workers. The time for collecting the answers affects the efficiency. The time and space must be related. If the amount of time spent in collecting the answers is too short, the answers cannot be collected in all areas. Efficiency will decrease. The case that the system has both expert workers and novice workers to find that When system has more novice workers, the efficiency will be reduced. With a range of 20% - 50% of the novice workers, the EM Algorithm has a efficiency higher than Majority Voting. With the novice workers 40% of system, F1-Score of EM Algorithm is 92.72% While Majority Voting has an F1-Score of 80.83%, but when the proportion of workers begins to exceed 50%, the performance will decrease and the efficiency is similar in the end.  If there are many Novice Workers in system, EM Algorithm will have performance better than Majority Voting because EM Algorithm can select worker whose have more probability that correct the","Spatio-temporal Crowdsourcing , Truth Inference",,Special Project,,
ppr,A real-time performance monitoring system for distributed cloud applications,2018,,,,"Real-time monitoring of distributed applications in the cloud is very challenging for several reasons. First, system information is gathered from multiple machines residing in different networks. Then, the collected data needs to be visualized and presented properly to facilitate system and application performance analysis for various groups of users. In addition, deployment of the monitoring system must be made scalable with minimal overhead.
This project deployed a real-time monitoring system for distributed applications in the Cloud based on an open-source monitoring tool, TICK stack, which contains several different modules that facilitate the collection, storage, and visualization of metrics. Each application node is installed with a monitoring agent, Telegraf, that collects runtime system performance metrics (such as CPU usage, memory usage, I/O operations, and network utilization) as well as other application-specific metrics (such as total job status including DONE, RUN, IDLE, HOLD and TOTAL). InfluxDB stores time-series data such as operations monitoring, application metrics, with high-availability storage. The default visualization module in TICK stack was replaced with Grafana, an open-source visualization platform to display the collected data in a single dashboard. To facilitate the deployment of this system in different clusters of application nodes, we created automated deployment scripts to install the software stack with basic configurations.
To evaluate the scalability and overhead of our system, we performed experiments that monitor CPU, Memory, and Disk I/O of a Telegraf node that is sending different amounts of metrics at specified intervals. We also assess the system performance by collecting Network usage, Disk I/O, and Disk space with various number of Telegraf nodes. We found that Telegraf induces minimal overhead to the system if interval is 1 second or more; and InfluxDB induces minimal overhead to the system if number of nodes is small. To test the automated deployment of our system, we set up the base monitoring system (that monitors only runtime system metrics) in Google Cloud Platform cluster and a PRAGMA cloud cluster with the Distributed Collaborative Environment for Lake Ecosystem Modeling (GRAPLEr) application. We extended the base monitoring system with additional plugins to monitor other application-specific metrics for GRAPLEr to demonstrate the effectiveness of our monitoring system.","Distributed Application, Monitoring System, Application Performance, Automated Deployment
Acknowledgments: This project was funded in part by the AIST ICT International Collaboration Fund.",,Special Project,,
ppr,EyeMath:  A Mobile Application for reading books containing mathematical symbols for blind people. ,2018,,,,"Mathematics education for visually impaired students is difficult challenging because mathematics books that these students can learn are only limited to braille books, and audiobooks. In order to increase the chance of learning math content for people with visual impairment this project focuse on design and development of a mobile application, called EyeMath. The main purpose of EyeMath is to provide an access tool for reading books, including books with mathematical notations. An application user can input either an existing picture in a mobile phone. After that, the system will process the image starting with a segmentation process to divide the image into smaller pieces and identify which piece contains plain text or  mathematical symbols. After this step, the image pieces will be sent to the image processing modules according to that identified type of image. For transforming the images with mathematical symbols into Thai sentences that users can understand, there are several steps involved. Our application first uses Mathpix OCR API to recognize Mathematical expressions and get its LaTeX string. Then EyeMath converts the LaTeX string to an Abstract Syntax Tree (AST). We implemented this module by extending the MathLex Javascript library to support LaTeX as input and additional Mathematical operators.  Finally, the application processes the AST as a set of Thai language messages that the users can understand. For the images that contain only plain text, EyeMath converts the images into text by using Tessaract optical character recognition (Tessaract OCR) techniques. The results from both image types are eventually combined into one set of text that the screen reader program on the user's mobile phone can read aloud. As a result, visually impaired users can access the contents of the books containing mathematical symbols conveniently.","Mobile Application, Image Processing, Optical character recognition (OCR), Mathematical Expression Recognition, Translating Mathematical Expression into Sentences, Accessibility Learning",,Special Project,,
ppr,A Genogram Management System for Home Visit Care,2018,,,,,,,Special Project,,
pps,Number Extraction from  Meter using Image Recognition Technology,2018,,,,"Meter is a device that measures size amount time and angle.
It is used as a measure of household consumption to determine how much resources are being used. According to the information of the Provincial Waterworks Authority and the Metropolitan Waterworks Authority,staffs record the numbers on the dial gauge every month. After that, information is reported to those to make receipt. The whole process wastes time and brings about errors from staff. The major problem is the one-to-one notes, long time data gathering and error writing numbers from the water meter. 
Therefore, the developer has chosen to develop numeric extraction device from meter using the image recognition technology and internet of things technology. The system consists of three parts that work together.There are parts of device for read numbers from the gauge, parts of program for convert a photo from a gauge dial into a string or digital text can be used to calculate the cost and parts of database that holds the image was taken from the device and the message was converted from the image before being used.
 These three sections will reduce the time and trouble of reading water consumption meter to calculate water tariff.",,,Special Project,,
pps,Guitar Functions Box,2018,,,,,,,Special Project,,
pps,Concept word segmentation using alignment model,2018,,,,"A sentence structure in Thai language is very complicate because Thai language do not have a word delimiter such as blank space, comma or full stop. This is the cause why the word segmentation or text tokenization is necessary for the development of natural language processing. If the result of word segmentation has errors, Other process will crash. For example, is Full Text Search, Thai-English Machine Translation and Text to Speech.
Word segmentation in Thailand has been developing continuously since 2523 B.E. until a present. Researchers have devised various methods including the development of word segmentation tools. They cannot completely correct the segmentation of sentence because a complex orthography of the Thai language. An example of the word’s problem are new words from a foreign language, slang words and etc.
This research is manipulated to solve the problem of compound word or phrase segmentation to be a form of morphemes. The concept of this research is using Alignment model of Machine Translation to help in combining words after word segmentation process with various tools. The dual language that use for training Alignment model is English-Thailand. For this research, we choose the highest performance tool for doing word segmentation that is DeepCut. Thus we use DeepCut with Alignment Model and doing the experiments about a several method of words combination to measure a performance of each method.
A Principle of performance measurement is using a general test set from linguist to compare with result of our model .Then we use test results for finding precision recall and F1-score.","Word Segmentation, Alignment Model, DeepCut",,Special Project,,
pps,Real Estate Information System,2018,,,,"Real estate is a necessary thing that everybody must have it for daily life. At present, there are a lot of real estate project in Thailand for serving the needs of buyers. But it probably caused difficulty.
Project team realize the importance of buying real estate, so we have idea to develop website for facilities of buyers by using Vue JS and Flask as a tool for development. In this case, we use database from 26 room of dorm around Thammasat University. This website will have an ability for search, search by map, favorite, rating recommend and compare including estimate price of real estate that other websites don’t have. We expect our project to have benefit for buyers.",,,Special Project,,
psw,3D Room Construction and Decoration Application,2018,,,,"The 3D room construction and decoration application is developed for users who want to design their own rooms which can be rendered in three-dimensional view. This application makes your interior designing experience easier and also prevents you from wasting time in handwriting.    Users will be able to design interior and decorate their own rooms with many models and furniture providing in the application (i.e. walls, windows, doors, etc.), and they can also change the pattern of those things as they want.
This application can be used only with the Windows10 operating system, and can be updated through the Unity3D Engine, a software which helps to manage the 3D image system. It is a simulation in a first-person perspective.",,,Special Project,,
psw,Facial animation using 3d scan technique.,2018,,,,"This project uses animation technology with three-dimensional scans
and engine games to create a library for creating animations, facial expressions, and
gestures for interactive work. This library is designed to allow users to create simple
animations for use with the unreal engine. By using this method, users will be able to
import the library into the user's Unreal engine, and set up the animation via a level
that interacts with the user and saves data's facial expressions and gestures. It can
also be used to accompany speech with animation.
	After saving the animation data, it will be stored in the file, allowing
the user to play the animation in other levels desired by the user, and to write the
program to add additional commands. For flexibility to use and save time in the
animation settings through Unreal engine‘s animations graphs",,,Special Project,,
scw,Answer sheet grading application,2018,,,,"Present, multiple choice testing is very popular and widely use. They usually use in examination that have many people, so they will take a long time to check the answer. Even though there are many tools for help officer that do this task to decrease used time, they still has problem about cost and convenience. Applications on smart phone were developed to solve these problems, but the most of them were developed in foreign and always use English language that may inconvenient with Thai people with examination that use Thai language.
	Therefore, we develop answer sheet grading application that makes more convenience, reduces cost and decreases used time.
	This application will scan the examination then calculate the result. After that, application will analyze maximum, minimum, average and standard deviation. This application can export result files to apply with other statistical tools. Finally, it supports both Thai and English language for more user groups.","Multiple choice test, answer sheet grading application, Image processing",,Special Project,,
scw,Banknote recognition application,2018,,,,"Banknote recognition application Aims to develop applications on smartphones using the Android operating system to be able to recognize all 6 bank notes, namely Thai Baht, Won currency, Yen currency, RMB currency, Euro and US dollar Can tell the value Can convert and show value in other currencies that the user wants Which has a procedure Users can take pictures from a smartphone or choose a photo from an existing gallery. The system then processes and displays the result as the value of money in the image. The user can choose to convert to other preferred currency. Making it possible to check money value more quickly and conveniently. This project uses image processing used to develop the system by looking for the keypoint of banknotes to bring the strength to compare with the prototype image By finding the closest distance by processing the image that separates various currencies into what currency and value The researcher chose to use the visualization method with Oriented FAST and Rotated BRIEF (ORB) primarily.",,,Special Project,,
skr,Learning Management and Digital Document Management System,2018,,,,"Scheduling and Document Management System (SDMs) has been developed to advocate authorities's working to manage schedules and relevant documents because relevant informations often change then make authorities have to modify information every changing. That make it difficult to improve and easily lead to errorneousness of information.
The developed system able to respond requirement of user to improve and provide schedule information which is controlled under user's conditions. The developed system is help to facilitate to encrypt and data storage in electronic form which is easily to maintain data storage and search for required informations and documents. The developed system able to establish user's required platform document and able to send an electronic information that specified by user.","Digital Document, Scheduling Management, Digital Document Management",,Special Project,,
skr,Game for learning C language,2018,,,,"From our experience, most of undergraduate students got difficulty to study C programming. They have less opportunity to practice C programming skill. Moreover, most of them want a Game as instruction media for practice a programming skill.
Today, computers’ web browser become main medium for people in many purpose such as for entertainment. Many developers pay attention to develop web-based application. Since computer have been become more portable, people can easily access to a web-browser. One of the popular content that people love to access via their web-browser is games.
To make WarCode become a easily access in many moments and provide people as an entertainment in everywhere. WarCode is chosen to develop as a web-baesd application. In addition, to provide a convenience way to participate to a student and create the contents as a challenge in a game Instructor and Staff can participate in the WarCode. 
We hope WarCode will become a funny way to study C programming skill and be a convenience way for instuctor to create contents for their students.",,,Special Project,,
ssr,Hand Free Picking System in warehouse,2018,,,,"This project is the study and development about the warehouse management system. 
It developed from WMS ver.1. It uses a relational database to store information. 
This could be access with a lot of data is not good in the same time. Including using of 
barcode scanner is inconvenience. It’s cause of delayed works. Therefore, this project 
was made for study and develop for better performance of system.
	This project studied the work of the Relational Database and Non-Relational Database 
to develop WMS by bring them work together. To split access for reduce delay.  Including change 
from a portable barcode scanner that can record data to a barcode scanner ring wear on fingers 
that can connect Bluetooth with smartphone and record data by smartphone that stick on staff’s arm. 
	Results of the study within this project found that the operation of the database and changing 
form of device is faster and make more convenient. From using of barcode scanner ring can save 
time about 1 second per a scanning. It can save about 130 minutes per day. As a result, the overall 
performance of the system better for both convenience and saving in terms of time spent on work.",,,Special Project,,
ssr,Motion tracking application with beacon for preventing depression,2018,,,,"Psychiatric diseases are the leading cause of death in the world, such as depression. Depression causes symptoms leading to major depressive disorder.
	We can treat depression by using simple theory name “Behavioral Action for Depression”, which consists of 3 parts: Emotion, Behavior, Thoughts and Cognition. When there is depression, the 3-part relationship will be negative. In order to stop the relationship, we must add a positive relationship.
	So, the developers have developed a motion tracking application with beacons for prevent depression, in order to monitor the activities and emotions of users, and encourage users to have more activities.
      	The operation of the system is divided into section that track user’s behavior from rooms with beacons attached using BLE. And other section that introducing activities that users like to encourage users to do more activities and help users to have a better mood from their favorite activities.
Keywords: Depression, Beacons, Bluetooth low energy communication or BLE",,,Special Project,,
ssr,Preschool Nutrition Care System,2018,,,,"Nutrition is one of the most important factors that enhances the growth of preschool children between the age 1 and 3 years. Having good nutrition for preschool children is to gain sufficient nutrition for their physical needs as they are at the age of physical development and cognitive development. They acquire experience by having activities, or taking more exercises than any age; therefore they needed to gain complete and balanced nutrition in proportion to the needs of children in this age. It is important for parents to pay attention to the nutritional needs of their children in preschool between the age 1 and 3 years.
We developed nutritional care systems for preschool children to monitor the nutrition of children for their growth. There will be a record of nutrition in the children's meals to show the user about the child’s eating habits. Moreover, the system helps evaluate child's eating behavior for their physical growth.","Nutrition, Growth, Preschool children between the age 1 and 3 years",,Special Project,,
ssr,Voice assistant for warehouse management,2018,,,,"From the study, warehouse management needs to be developed in the scope of work. The forklift driver or folklift. 
That can improve the function of folklift driver to be worth the cost of finance has increased and reduces the delay time as a driving run around 
unnecessarily or can develop applications to facilitate its use is increasing. A reason for developers to choose to improve the part of the delegation 
to the driver folklift as appropriate sequence to the current position of each driver. The driver can operate by itself without having to select it manually.
And facilitate the work of the driver with the technology to use voice commands instead of responding with a touch-screen tablet PC from start to finish.
              Developers have studied the theory of optimizing driver assignments and sound techniques. To be improved from the old system. 
We have chosen the greedy algorithm theory to help decide the sequence of tasks from the location of the store that has the most similarity 
to choose to assign as the next work sequence. To achieve cost-effectiveness and achieve the objectives of most work. Voice assistant are adapted 
to the driver's commands to the system and system commands to the driver. The technology involves the use of it are Voice detection Speech Recognition 
and speech synthesis. The old system has been created in the form application available through Android studio program in Java. 
This is why developers choose the right tools or techniques for the program, including Google Cloud Speech-to-Text API , 
Google Cloud Text -to- Speech API and most importantly, We bring device bluetooth headphone and microphone access in speech between the forklift driver 
and the system. To change the system can be voice activated hands free appearance in.
	         This project is intended to reduce unnecessary work of the forklift driver and facilitate communication with the equipment. 
Therefore testing the system to compare the number of work processes(unit of time) between the old system and the developed system 
by using the simulation of the product shelves in real situations and part of the actual work from the database used to calculate the number 
of work steps with the test program that the developer created Which results were found from the same work list Developed systems help reduce 
overall workflow. In terms of voice commands Test the system by using a keyword that is used to actually work with the developed system. 
Become a task for users to enter shortcuts to the system by dividing the test into 5 cases, such as the environment in which the system is used 
and the accent. To test the actual usage and measure the percentage (percent), accuracy 
and error It was found that the voice commands were approximately 87.32 % accuracy.","warehouse management system, greedy algorithm,   
Plan the next task sequence, voice assistant",,Special Project,,
ssr,Tracking system for the elderly,2018,,,,"Thai society in the present day has a tendency to become Aged society .
For statistics on missing person in 2018 shows that elder missing had 27.8% and 
more than 14.6% had Alzheimer's disease. Memory loss is the main symptom 
of Alzheimer's disease that can affect routine .for example , forget the names 
of acquaintance, get lost in familiar places.
	Developer has developed an elderly tracking system to prevent the disappearance 
from home. This application installed on smartphone and connected with smart watch 
for recording information and location of elder can connect between the person who take care 
elder and elder immediately.
	The operation of this application had separated into 2 parts . For the first part is Beacon. 
Beacon used Bluetooth low energy which connected with smartwatch that can be check location 
when elder out of selected area. For the second part is application on smartphone 
that user can check the elder location anytime","Smartphone, Smartwatch, Beacon, Bluetooth low energy, Elderly, Disappearance Prevention",,Special Project,,
ssr,Door Access Control and Monitoring System using smartphone,2018,,,,"This project presents Door Access Control and Monitoring System using smartphone. The objective of this project was to provide convenience and safety to the users. Nowadays, there is a smartphone-controlled door opener technology for short distance usage. However, the technology is still not able to control door opening from long distance. We decided to use Smartphone that most people use for communication with Arduino which is a popular controller device that we can program using high-level programming language. Both devices are chosen for developing our remote door control system. 
Door Access Control and Monitoring System using smartphone is a system which users could use to open a door instantly. The users could open the door by entering passwords or scanning a RFID card on the door. When the door is opened, closed, or a wrong password is entered, the system will store the data such as date, time, and username when the door is closed or opened or when an error occurs to the database. This system will provide a convenience for opening or closing the door via smartphone.
This system can control the door from long distance, check the door opening history and many ways to open the door. In addition, Door Access Control and Monitoring System using smartphone enhance the safety such as notify when entering wrong password, notify when the door is ajar etc. More convenient and time saving to users.",,,Special Project,,
tnt,A Real-time Social Media Monitoring System for Thai Universities,2018,,,,"A Social Media Monitoring System for Thai Universities aims to gather comment information on social media related to 40 Thai universities, including 20 public universities and 20 private universities. University selection is based on the World University Rankings, reported on a Webometrics website. Data is displayed in statistical data format. For example, mentioned ranking of Thai universities. Additionally, it can display in patterns of depth-data analysis. For example, grouping frequency of uses, or common words in a sentence in order to show that the university is currently mentioned in what subject. This system is suitable for people who want to use the analyzed data to benefit in a variety of individuals, e.g., university administrators, students and people who interested in attending educational institutions.","sentiment analysis, twitter, clustering, Thai Universities",,Special Project,,
tnt,Indoor Navigation for Thammasat University Hospital,2018,,,,This special project is about developing an Indoor Navigation Application so as to solve the problem of finding a room in scaled building and complex interior design building. The purpose of developing this application is to improve conveniences in transportation by using QR Code and mobile  sensors approaches on Android platform,"QR Code, Sensors, Android  ",,Special Project,,
tnt,A Speech Recognition-based Solution for Screening Patients,2018,,,,"In the web application project for screening patients with voice (Web Application for Screening Patients), I would like to thank Dr. Thanathorn Thanthongthong, a project consultant. That has helped to give advice Support and suggestions about the project and would like to thank all professors. You in the Department of Computer Science who give good advice that are useful in project work. Thank you friends for helping and encouraging the project.","Text to speech, Chat bot, Screening Patients",,Special Project,,
wdc,Automatic Text Summarization of Disaster warning messages affected food security using IBM Watson's Natural Language Understanding,2018,,,,"Natural Language Understanding Technology is a tool for extracting knowledge from unstructured documents to create index for searching or to summarize the essence in a particular topic. However, no one use this tool to understand Disaster Warning Messages affected Food Security thar make limit for  searching, confirm location, and display on mobile devices with limited display. 
	This project proposed the development system of a Disaster Warning Message affect Food Security base on the IBM's Watson's natural language understanding to extract data from the Food and Agriculture Organization of the United Nations disaster documents. The Process of development is develop of type system to extract data, document annotation, develop a knowledge model, make  models into the IBM Watson Natural Language understanding system, web application development as a prototype for summarizing text from models, and compare between summary text from system and summary text from agricultural experts. After do all process, we got the javascript code that can work on purpose, extract knowledge from warning messages and summarize messages.",,,Special Project,,
wdp,GIS with Crowdsourcing as a Service Version 4,2018,,,,"GIS with Crowdsourcing as a Service (GCaaS) is primarily a platform providing services for creating and maintaining Geographic Information System based web site (GIS-based web site) without requiring any specific knowledge. GCaaS can be beneficial in facilitating creations of GIS-based websites and shorten delay of having interested information ready for particular groups of people. The resulting web site, which is so-called a deployment, can be used for gathering and displaying information relating to an event of interest, an ongoing disaster, and the effect of disaster afterwards. GCaaS can support gathering information from many sources: social media networks, specific-purpose mobile applications, etc. and displaying the gathered information in perspicuous and meaningful format on maps.
Deployments currently created by GCaaS are capable of filtering data from social media network ‘Twitter’ based on specified hashtags. Twitter provides both web based and mobile applications for users to participate in the social media network. Being able to filter data from Twitter facilitates receiving complete and valuable data from Twitter’s users which is known as ‘Crowdsourcing’.
GIS-based web sites, or deployments, created from any previous version of GCaaS share a major restriction.  Deployments are capable of receiving crowdsourcing data via social media networks only. This limitation may cause an incomplete information when operating disaster management and making critical decisions. As the idea of Internet-of-Things (IoT) prospering, “sensor networks” which is a practical and significant application of the IoT platform has become a sparkling and interesting source of crowdsourcing data. Being able to receiving and visualizing data from sensor networks heightens the value of deployments created by GCaaS since the deployments comprise more diverse useful information.  
GCaaS version 4 aims at enabling deployments created by GCaaS to be able to receive and visualize data from sensors networks. Dynamic data layers with data from remote sensor networks can be beneficial for both regular and advanced users. Regular users can overlay the data layers on top of a base map to demonstrate data from remote sensors of their interests in a concise and meaningful manner. Advanced users can overlay data layers with data from remote sensors on top of many other data layers to analyse impacts or sources of problems.","sensors, IoT, MQTT, broker, topic, publishers, subscribers",,Special Project,,
wrj,Classification of problems from online complaints,2018,,,,"At present, there are sources of complaints on various online channels, 
which require screening of messages useful for the state to recognize the problem
and be able to issue measures to solve the problem promptly. But with a large 
amount of complaint data, it makes the screening person difficult to work and 
takes a lot of time to classify before sending it to the relevant authorities.
	From research on systems related to the classification of Thai text, 
there are still limitations. Most of them has no data preprocessing and none uses
logistic regression technique. Research, this system, therefore, has been 
developed to collect online complaints from various sources and categorize 
the messages into five categories, including education, health, and well-being,
access to public services, economic by comparing the use of three techniques
of machine learning, including Support Vector Machine, Naive Bays and 
Logistic Regression. It was found that the accuracy of Support vector machine 
is to 94.5 percent, the accuracy of Naive Bays is 87.1 percent 
and the accuracy of Logistic Regression is 94.2 percent. Support Vector Machine
provides the best classification results with 94.5 percent accuracy.",,,Special Project,,
wrj,"Website for Searching and Comparing Products Posted
on Twitter",2018,,,,"Tweets Price is a website created to support the sale of products for individuals who like online shopping on Twitter or online merchants who sell or promote products via tweets.  And found many of problems. The developer therefore brought the problem to analyze and find a way to solve the problem. Therefore being a website for searching and comparing products posted on Twitter. The website consists of four main parts. The first part is the user interface, which users can use through a web browser that is designed responsively to support many sizes of screen. Users can use this interface to communicate to the site for all functions. The second part is The server part is the processing of data that is sent between users and the database, including requesting to post products from the Application Programming Interface (API) that is also created. The third part is the API section that Created for requesting product information from Twitter and sending information to the server. And the fourth part is The database is a section that collects various members' information, such as user names, passwords and saved products, for later viewing.
Keywords: Online shop, Twitter, Comparing",,,Special Project,,
wsl,Supply Chain Management Deep Crossdock,2018,,,,"At present, deep learning has been very popular. And being applied to a variety of problems And restaurants information  is interesting for deep learning Because the restaurant has ordered more raw materials through the business that is the distribution center Due to the convenience of direct delivery to restaurants and keeping the temperature during transportation to get raw materials that are always fresh Cause of cross dock platform that can distribute products quickly Because it can receives of product  and export at the same time By not store in warehouse But sometimes the amount of raw materials from the producer is not enough for Customers needs. Thus causing damage to the business 
	This project developed a prototype system for predicting the purchase of products to producer. By using deep learning techniques For use in planning for the purchase of raw materials for producer in the long term Together with the analysis of ordering raw materials on a daily basis of the store And the analysis of the amount of production of the producer to assist in the delivery of food products have been delivered in advance and can also reduce the cost of food raw materials, allowing producer to produce and plan for long-term production of customers","Deep Learning, Neural networks, Reinforcement Learning",,Special Project,,
wsl,Workflow Engine for Material Management,2018,,,,"One of the most important things in restaurant business is raw material management because it deals with the menu and the amount of materials needed in order to have a balanced amount of material. However, since the menu, the ingredients needed, and other activities in the restaurant do vary, obstructions, that cause problems to the whole process, do appear from time to time. Therefore, the project creator have developed a system which helps facilitate the process of restaurant management named Workflow Engine for Material Management.
The web application called Workflow Engine for Material Management aims to make work less complicated for restaurants. It also helps by displaying the amount of materials needed from the supplier. The project team has developed a system with various useful functions including the automatic workflow activities management function, and also the function that manage the amount of materials.",,,Special Project,,
wsl,Know Trash : Trash Classification Bin,2018,,,,"KnowTrash is the project that designed to enhance knowledge about waste separation. Cause in the present, most people still lack knowledge about waste separation.  As a result, trash is disposed incorrectly way. Cause environmental problems so it is the aim of the KnowTrash project to sort waste. To reduce the amount of waste that has been disposed incorrectly way. And increase the amount of recyclable waste for reuse. Waste separation can also generate revenue for the people from sale recyclable waste.","Deep learning, Object Recognition, Green Technology",,Special Project,,
ywt,Dental Health,2018,,,,"According to the public health policy issued by the Ministry of Public Health, students must have their oral health status checkup at least once every year. Normally, the process of the oral health status checkup for one student needs two persons, a dentist and a dentist assistant. The dentist does the checkup while the dentist assistant takes notes on a status of each teeth following the dentist instruction. The process is tedious and error prone. In addition, it need too many  medical personnel. Last year, an application named, “Voice Recognition Application for Oral Health Status Recording” was built to provide a solution to this problem. However, its accuracy performance is still unsatisfied. To continue improving the system, this project is a continuing project that tried to improve its performance.  Rather than using a speaker independent model, this project builds additional module to allow a user to record his/her voices and create the acoustic model specifically for each individual. The preliminary testing on individual voice model is promising. Thus, we build the new system with such additional feature. 
 This new application still consists of two main parts. The first part is the web application. This part is used for adding patient’s information and can be later used on the mobile application. The second part is mobile application this part uses for recording the oral health status using voice commands. When finish recording the oral health status, the data will save on database and later analyze on the web application. The mobile also has the simple analyze tool too.
Another part of the mobile is to create individual voice model for recording the oral health status. It uses Shell Script to command the API of CMUSphinx on the server to start creating voice model. After getting the model, the server sends such individual model back to the user.  
Finally, we test and evaluate the performance of the system. We find that the accuracy has been improved from 68% to 83.33% or 15% more accurate than the previous version.",,,Special Project,,
ywt,see beyond AR,2018,,,,"Nowadays, our society is becoming an aging society. One of the most important factors for the elderly to live a good life is to be healthy.  Most of their health is getting worse because of fewer movement and exercise lacking. With enough exercise, they will be more healthy, both mentally and physically. Most of them prefer to exercise outside because they think that inside the house is not a place for exercising.
        We came up with an idea which will help the elderly exercise inside their house using Augmented reality technology. We created a mobile application in form of a video game called AREx that let users complete exercise missions daily. Each mission mainly focus on increasing flexibility to the body and preventing brain deterioration for the elderly. The application uses an accelerometer and gyroscope in the smartphone to measure wrist and arm exercising performance and uses pedometer and barometer for measuring legs exercising performance. Furthermore, it also uses GPS to increasing it’s measuring performance. The users can also share exercising results and invite their friends to join AREx via Facebook.",,,Special Project,,
ywt,Application for design and manage control flow in web application.,2018,,,,"Javascript is one of the most popular programming languages for web development. The learning curve is low and it is easy to understand. There are plenty of documentation and tools which allow us to develop websites efficiently. However, we cannot predict what will happen in the Javascript system because Javascript is an asynchronous programming language which makes the code complicate and hard to manage. There is no tool which helps us design and manage events in Javascript and making documentation.
According to the problems above, I want to create a system which helps people manage events in Javascript in web development. It can be worked only with ReactJS, it will help us see the whole probability of events possible and make documents that give us the whole system structure in order to facilitate users in the development process.","Javascript, Asynchronous, ReactJS ",,Special Project,,
ron,Application For Searching Dormitory Nearby Thammasat University Rangsit Campus,2019,,,,"In the past, public relation or advertisement of dormitory owner commonly uses advertising and promotes business them by postering in area where people live and talking people because people don’t use mobile device widely.In the past, if the customers would like to find dormitory, they can receive news through area that dormitory owners have advertised or receive information from people close to them who have used the service. Nowsadays, Computers and Mobile devices are becoming popular that influence the way how to promote and advertise the dormitories. Moreover, they add advertising channel in mobile application or website that allows people to receive information about dormitory more confidentiality. Therefore, we propose to develop a mobile application for  searching dormitory nearby Thammasat University. The purpose of application is helping customer that interest dormitory near Thammasat University and this application is tool that help promote dormitory’ s dorm owner.
The application has two parts. Part one, general users that want to find a dormitory near Thammasat University. Part two, dormitory owners can promote business themselves. This application helps general users receive information about dormitory near Thammasat University and it is a helper for the decision to choose a dormitory. ",,,Special Project,,
wil,,2019,,,,"Nowadays, online shopping has been widely used especially food market.
Food online can order food immediately by mobile phone and using internet, then the
food will send to the purchaser. Food online shopping is very comfortable and popular for
buyers, however they must to pay for shipping.
	TU Flash Food, food online shopping application, is created for food order in
Thammasat University (Rangsit) and it is very simple to use by students and educational
personnel. The students, who are interested in the part time job, can employ to be
deliverers.

Keywords: Application, Online, Mobile Phone","Application, Online, Mobile Phone",,Special Project,,
wil,,2019,,,,"Nowadays, cell phones have transformed to become smartphones which have many features. Many people use the smartphones for their purposes, for examples, some students use them for studying and preparing the exam, while others may use them for cheating the exam. Some prisoners might bring contraband smartphones into prisons for escaping or passing their orders to people on the outside. Thus, the cell phone detector devices have been developed in order to detects the presence and existence of cell phones in an area or within a stipulated range of operation. However, these devices can be interacted with via web browsers, which might be inconvenient for users. 
	Therefore, the objective of this project is to develop a mobile application that allows users to interact with the cell phone detector device without any use of web browser. Users can filter the results, such as choosing the specific range or mobile service provider.
Keywords: cell phone detector device, RF sensitivity, mobile service provider","cell phone detector device, RF sensitivity, mobile service provider",,Special Project,,
ron,Elder Travel Planning Website,2019,,,,"Since Thailand is entering an aged society, the tourism market began to pay more attention to the elderly tourists. At present, the existing tourism websites are not designed to be suitable for the elderly. Also, the contents on the existing tourism website neither offer tourist attractions nor specify facilities and travel plans suitable for the elderly. It inspired us to develop a tourism website whose user interface is designed for the elderly and that helps schedule a travel plan appropriate for the elderly, which is an itinerary with tourist attractions offering facilities needed for the elderly. The travel planning function is for one day trip plan which takes into account the physical health factors of the elderly during planning. The appropriate tour for the elderly in one day should not cover more than 3 places and if the journey to each place take more than 1 hour, our website will show the rest stop on the way to that place. 
Elderly Travel Planning Website will help increase the efficiency of using the website of the elderly. It will also recommend tourist attractions and provide information to the elderly so that they can decide which attractions are right for them.","Tourism, Travel planning, Website, Elderly",,Special Project,,
rat,"Electronic Medical Record System for Home Care Service of Family Medicine Department, Faculty of Medicine Ramathibodi Hospital, Mahidol University",2019,,,,"The Home Visit Service Unit is part of the Home Patient Health Care Service by the Department of Family Medicine. Faculty of Medicine Ramathibodi Hospital Mahidol University.
The goal is to allow physicians to get access to and offer treatment for patients at home from a real living environment. And providing knowledge about caring for patients families. 
Each home visit requires a home team consisting of a variety of medical staff. The data was recorded on paper. And there may have some redundant information causing problems in both document storage and data searching Resulting in not being able to fully utilize the information, And there is a redundant work process when wanting to analyze the data In addition, it is difficult to see an overview of each home visit. because the earlier process did not systematically gather information from different medical staff.
In this project, the production team focuses on the design and development of the visiting information management system of the visiting service unit. By using web application technology and the API website Came to help with the storage of various processes of the home visit. Such as Visiting appointments, Confirmation of the date of visiting with the patient before visiting, Preparatory meeting before visiting, And recording systematic results of home visits for medical staff on the home visit team to increase the efficiency of the home visiting process, Reduce duplication and errors in the work of medical staff. Which will use the information for further improvement of the efficiency of patient care services.

Keywords: Visiting information management system, Medical staff, The Home Visit Service Unit","Visiting information management system, Medical staff, The Home Visit Service Unit",,Special Project,,
ron,Program for behavior adjustment in asthma patient,2019,,,,"Asthma is a respiratory disease that causes the narrowing of the bronchi. Due to bronchitis with chronic inflammation, the bronchus is more responsive to allergens and pollutants The current treatment for asthma can only relieve symptoms by spraying medication and by taking care of yourself, but most asthma patients do not have knowledge in self-care because access to the majority of information comes from consulting a doctor. Using the network to find information, which some articles are not reliable. Causes asthma patients to have a difficult life. The developers clearly see the use of mobile phones, so they created this application to help them take care of themselves for asthma disease and change the lifestyle habits of asthmatic patients
	The application consists of users with asthma disease who want to take care of themselves and changing health habits. The application will provide information about appropriate health behaviors and how to change the lifestyle of asthmatic patients to be able to live like normal people.",,,Special Project,,
yao,Tooth Status Collector : AlphaNumeric Recognizer (ToothSCANR),2019,,,,"To monitor the oral health of primary students, the Ministry of Public Health requires each school to have oral examination for each student at least once a year. 
The examination performs by dentists and the results are generally recorded in papers and then later key into a computer by human. 
The task is so laborious and tedious. In addition, papers are hard to keep and analyze if not transformed them into a digital format. 
This project aims to develop a system that helps to transform a student’s oral record in paper to digital format and keep them in a database system for later analysis. 
     Currently, we developed this system as a web site. Users can upload students’ oral status in the form of an image or excel file. 
For any record in paper format, users take photo of each paper to get an image file. In case that the records already key into a computer, users can import the excel file into the system. 
Either ways the student records are kept in the database and can be analyzed to see the progress of each student’s oral condition. 
     To transform oral status in an image to digital form, the system uses a digital image processing to segment and crop each character and uses Convolutional Neural Network (CNN) to predict it. 
We tested the accuracy of the character cropping based on 45 papers. We found that the system can crop all of them correctly. 
We also tested the character prediction using the CNN model which is trained with the EMNIST Dataset. We found that the model has the prediction accuracy 86.20%","OCR, Digital Image Processing, Oral Health",,Special Project,,
rat,Integrated Real-Time Application and Infrastructure Data Monitoring at the IoT Edge,2019,,,,"In the present, Internet of Things have been becoming popular and used in various kind of areas, for instance, Agriculture, Industry, Antiquity Preservation and so on. The technology advancements make edge devices having more computing capabilities. This makes a monitoring become a very important thing for IoT System workflow, especially for a large scale IoT system. It is literally challenging work for the system admin to monitor and investigate many edge devices that might belong to the different organizations and deployed in different places. Logging in to each device with remote login and incapability to understand the overall view of the system might slow down the monitoring process and cause a bad damage to the system afterward. 
	This project aims to build the real-time application and infrastructure data monitoring system from opensource software stack called TIG Stack. The TIG Stack consists of Telegraf (T) that is a data collector agent, InfluxDB (I) that is real-time database and Grafana (G) that is for creating monitoring dashboards. The project members have studied and conducted experiments about resources usage of Telegraf when used with an IoT device and found that Telegraf does not significantly affect much resource usage of the IoT device; by average, Telegraf consumes less than 7% of the IoT device’s basic resource. We also prepared Telegraf configurations and extended Telegraf for collecting application metrics via commonly used approaches, for instance, MQTT and Log file. In addition, we also designed and improved the web service to monitor the data from Peer to Peer Network Overlay named IPOP and made an API for the web service to provide the network overlay data to the visualizer (not only Grafana). In order to evaluate the performance of our proposed monitoring system, we demonstrated that the monitoring system can monitor the simulated IoT system according to the goals of the project. We sincerely hope that  this monitoring system can be used in various kinds of real world IoT systems and help system admins to conveniently and efficiently monitor their systems. 

Keywords: IoT Monitoring System, Internet of Things, IoT Gateway","IoT Monitoring System, Internet of Things, IoT Gateway",,Special Project,,
rat,Linked Data Visualization for Management of Large-Scale IoT Systems,2019,,,,"In general, the system must be monitored by the system administrator. The purpose of monitoring is to make the system work efficiently. On a large-scale IoT system, the challenge is that the system administrator has to investigate the log file of each device. Since there are many devices, different sources and different owners, so this method will take a long time and might be hard for the system administrator to see the big picture of the IoT system.
	This objective of this project is to develop the visualizer system that can visualize linked data for the management of large-scale IoT systems. The system administrator can monitor the system in real-time by a web application and a SAGE application. The web application was created by using React.js to build a web page and base components. Graphs are created by Cytoscape.js and Plotly.js. The visualizer can display the big picture of the IoT system, the details of all components and have an anomaly notification to notify when the IoT system has any anomaly. The visualizer also supports visualization on the SAGE2 platform as well. The SAGE application and a tiled display wall will increase the capability to show the big picture, comparing data and its relationship.

Keywords: Large-Scale IoT System, System Administrator, Monitoring, Data Visualization, Network Overlay","Large-Scale IoT System, System Administrator, Monitoring, Data Visualization, Network Overlay",,Special Project,,
ron,RECORDING CHILDREN'S MEALS SYSTEM BY VOICE FOR PRESCHOOL NUTRITION CARE SYSTEM,2019,,,,"Recording children’s meal system by voice for preschool nutrition care system was developed to help address parent’s pain points 
in taking care of health and nutritional needs of preschool children (1-3 years old). 
This application was easy to use but the input of user data is not convenient for all users 
because some users have problem with using their finger to type on the phone’s display. 
In some situations, users are not able to use a finger typing on the phone’s display. 
Therefore users are not likely to use the program and do not fill out the information needed to 
analyze the nutrition of children. Therefore, the project developer has developed 
the function of importing data by voice to add.Voice input function helps users who can’t conveniently use 
their finger to type on a phone’s display or in some situations users are not comfortable to use finger typing. 
Users can be input easily by speaking into the microphone of the phone and processing 
the voice data of the user to translate into food item information and automatically analyze the nutrition 
by natural language processing techniques
Keywords: Voice input",Voice input,,Special Project,,
suk,Java Codewars,2019,,,,"Java Codewars web application was developed to help as an exercise storage for professors in the
Java Programming course, medium for students and professors to do exercises. Since the old exercises 
do not attract students to do the exercises. For professors, they must save the questions on their own devices.   
The developed Java Codewars web application can meet the professor needs for enabling professors to store 
homework and exercises, so it is easy to search and use as an exercise in the following courses. 
Create courses as a medium for disseminating exercises to students. Allowing the professors to know the 
difficulty of exercises for students from the following information: exercise-taking time, examination with 
Test JUnit and Copy-Paste during doing exercises. The last one is to attract students to be more interested in 
doing exercises and the results of the exercises will be shown immediately. Furthermore, it also provides many 
exercises in many difficulty levels to practice.

Keywords: Java Codewars web application, Disseminating Exercises, Test JUnit, Object-Oriented Programming","Java Codewars web application, Disseminating Exercises, Test JUnit, Object-Oriented Programming",,Special Project,,
rat,"Decision Support System for Managing Home Care Service of Family Medicine Department, Faculty of Medicine Ramathibodi Hospital, Mahidol University",2019,,,,"Home Visit is a service of Ramathibodi Hospital for providing patients health care at home. 
Patients are sent from several units in the hospital. The objective of this service is for the medical team to visit the patients and their families at home to evaluate patients 
and provide them treatments in the real environment. Moreover, medical staffs can also provide care instructions to the patients’ families. At present, the officers 
and medical team who visit the patients at their home perform history taking, gather patients’ information, make appointment, and conclude all home visit’s data by filling out on paper forms 
and then type those data again SPSS or Excel for statistical processing. This process creates duplicated work and delay. 
	From the above mentioned problems, a web application was designed and developed in this project to collect all data about Home Visit service electronically. 
This web application works together with the web application in the project titled for to support electronic medical record system for home care service family medicine department, faculty of medicine Ramathibodi Hospital, Mahidol University (Pakorn and Supanat 2020) 
to help reducing redundant process in filling in patients’ data. This project focuses on the part for recording basic personal information of patients, overview information of patient case (such as status), Information on managing roles 
and duties of medical personnel in each department during the home visit process. A business dashboard was designed and created to provide  statistic data visualization to reflect the operating results of the Home Visit service and support for the officers 
and executives to plan and improve the quality of service.

Keywords: Electronic Patient Records, Business Dashboard, Data Visualization, Decision Support System","Electronic Patient Records, Business Dashboard, Data Visualization, Decision Support System",,Special Project,,
pok,Trash2Go,2019,,,,"A Trash2Go application was created to encourage recyclable waste sorting and recycling methodically which results from constant upsurge of solid waste 
in every year. In 2016, according to a survey in 7782 Local Government Organizations from Pollution Control Department, there is around 27.06 million tons 
of solid waste in Thailand which is around 0.78% higher than in 2015 and likely to be higher in every year. However, merely 5.91 million tons (22%) of solid waste 
was reused while 85% of which is reusable. As developed countries in Europe aim for reusable rate of 50 %, we developed Trash2Go application. 
In the application, an information regarding what kind of waste is reusable is provided. The application is a medium for promoting recycling so that 
users can recycle more conveniently since when users already collect the minimum amount of recycle waste, users can call garbage trucks to pick up through 
the application. This creates convenience and can also be an additional income for users. We foresee that the application is utilitarian and will help people 
separate waste more.When more waste is sorted, the rate of solid waste that is reused will be higher which results in a reduce of total amount of solid 
waste in every year.",,,Special Project,,
ron,,2019,,,,,,,Special Project,,
rat,SmartMath: A Mobile Application to Improve Basic Numerical Skills for Preschoolers using Gamification and Positive Feedback,2019,,,,"Mathematical skills become basic skills that are necessary in everyday life and have impact toward academic 
and professional success. They are considered skills that promotes thinking and reasoning. 
These basic Math skills should be cultivated and integrated into every activity to encourage preschoolers to think, 
reason, and solve problems by themselves. Children who acquire mathematical concepts from doing activities 
and learning practices in basic calculation skills will be ready for calculation at higher levels.
We designed an application called “SmartMath” together with pediatricians from Ramathibodi hospital 
to develop numerical skills for preschoolers and assists as a tool in evaluation of preschoolers’ basic numerical skills. 
It is useful to provide preschoolers more opportunity to practice number recognition skills and basic numerical skills. 
The application has exercises and tests in the form of games with positive feedback. 
Children can learn and practice numbers and basic math principles with fun. 
Game format also stimulates them to learn and practice continuously every day, resulting in long-term benefits.

Keywords: Preschoolers, Basic numerical skills","Preschoolers, Basic numerical skills",,Special Project,,
skn,Personnel Management System : Case Study Institiute of East Asian Studies,2019,,,,"Currently, within the organization of the Institute of East Asian Studies There is a 
need to change the data storage and methods of working with data and documents. the storage 
management is divided into document files in various matters to be a system that can store data 
into the database and the data stored in accordance with the documents that are stored together. 


The project team developed the Personnel Management System: Case Study Institute 
of East Asian Studies. The purpose of this system is to design the process for managing 
personnel data within the organization to be convenient and easy to find. Which the system can 
store personnel data in the organization into the database Store documents in electronic file format 
or link to the address of other Cloud Storage documents in the database. the data collected as a
summary report as a tool to support decision-making for the management and use the information
to notify about the time taken by the calendar. Such as warning of contract renewal and Warning of 
probation day. 

This developed project will help reduce the time each person has to collect the data 
and reduce the time it takes to search the documents about the data that are stored together.
Reduce redundancy in data storage. And can help calculate the warning date quickly and
accurately. The management of personnel information is also consistent with modern technology.",,,Special Project,,
pak,Smart electrical outlet,2019,,,,"Internet of things (IoT) technology makes remote control of electrical equipment more convenience and instantly. Users can communicate 
and view working status from everywhere via internet connection. This project studies and develops a mobile application for controlling 
the electrical outlet equipment to start and stop the electricity supply. Smart electrical outlet can be turned on or off 
the electrical devices connected to the electrical outlet remotely. The mobile application can also automatically set the time to start 
and stop the electricity supply.
Keywords: Online outlet, Internet of things","Online outlet, Internet of things ",,Special Project,,
wil,,2019,,,,"Developing information system of WIFI and service system in stores 
should cover both of service person’s safety and convenience. In this study, 
Pfsense is used to design a program, consolidate data, test and apply for developing service system. 
Pfsense, a free and open source firewall, provides the Captive portal service before accessing the Internet. 
Moreover, it supports to work with other softwares, such as Freeradius for managing authentication accounts, 
Squid for gathering logfile, and Mysql for managing database.

The result of research and practice can build the system that issues 
tickets for internet service in stores with the usage time limits and record logs 
relating to Computer Crime Act B.E 2560. This can bring the system into practical and effective use.

Keywords: WIFI and service system , provide tickets for internet services , Computer Crime Act B.E 2560","WIFI and service system , provide tickets for internet services , Computer Crime Act B.E 2560",,Special Project,,
pak,Car User Application,2019,,,,"Conflict between car users has always been a problem that occurs occasionally in our society. Whether it is controversy caused by driving carelessly, double-parking cars or parked inappropriate. Due to the fact that there is no communication between drivers that can indicates true intention on the road. That could lead to misunderstanding
Therefore, Developer has design mobile app that makes communication between drivers possible through Instant Messaging that uses car plate number to find and communicate with other user.to sum up Developer intention is to make car users a way to communicate with good intentions understand each other and even help each other on the road.

Keywords: Communication between car users, Instant Messaging","Communication between car users, Instant Messaging",,Special Project,,
pak,Clamp Digital Electricity Meter with Mobile Application,2019,,,,"In the rental apartment, when the apartment manager wants to place the electricity bill to the resident, he will manually record the electricity usage from the electricity meter. Most Electricity meters, which are used, is analog type and not able to store electricity usage data in the meter itself. Moreover, the installation of the analog meter is complicated due to being a serial connection.
In the project, an electricity meter has been designed and built with the application on Android operating system smartphone. The application can calculate the electricity usages of each rooms in the apartment. So that the residents and the apartment manager can monitor the electricity usage all the time. The electricity meter that the project created is a digital electricity meter with the capability to record electricity usage. Moreover, the developed electricity meter is using a Non-Invasive Sensor (Clamp Current Sensor) which makes it convenient to use.

Keywords: Arduino, Electrical measurements, Application","Arduino, Electrical measurements, Application",,Special Project,,
scw,Automatic Car Brand Classification System,2019,,,,"The objective of this research is to propose automatic logo detection and cutting method that uses the license plate position as a reference location in which the images taken are the front view of 6 brands of cars from CCTV, 2 views, 150 images per view. The result from the detection is the horizontal angle that correctly detected by 90 percent and the top angle that correctly detected by 86.667 percent. In addition, propose methods to classify the car brand from the logo image by using the method of feature extraction in 3 methods which are Scale Invariant Feature Transform or SIFT, Shape features extraction, and Histogram of Oriented Gradient (HOG) to extract image features. And then use brand classification with 3 types of classifiers, which are Decision Tree Classifier, Support Vector Machines (SVM) and Neural network in which the images are classified as the logo of the car which has been detected and automatically cut, amount 2 views, 240 images per view. Experimental results show that when extracting the image features using the HOG method with the SVM classifier, the best result in brand identification can be obtained. As follows, the horizontal angle is correctly classified by 89.583 percent; the top angle is correctly classified 96.25 percent.

Keywords: Car Logo Detection, Car Brand Classification, Car Logo Classification, Vehicle Logo Recognition","Car Logo Detection, Car Brand Classification, Car Logo Classification, Vehicle Logo Recognition",,Special Project,,
pon,"The VR system for analyzing the eye movement of

dentists",2019,,,,"This project uses eye tracking technology with virtual reality to record the dentist’s eye movement values while treating the dental patient, so that the user feels like treating a real patient. Virtual reality helps to reduce costs caused by user errors instead of training with an expensive and one-time plastic tooth model. When the dental is done, the data will be saved to a log file for further study of the behavior and experience of each user.
		The scene of a dental room will be created in the virtual reality will be created to look like a dental room. So that users feel like they are treating real patient. Every object in VR is a 3D model, combined into a scene in the Unity program so they can connect to the virtual reality that runs on a computer and the eye tracking library.",,,Special Project,,
onj,Health Care Application for Elder with Medical Problem,2019,,,,"Thai society is now entering the aging society completely. The elderly is the age of deterioration of the body causing disease easily. The elders therefore are increasingly interested in health care such as exercising, choosing healthy food to eat or avoiding foods that affect existing diseases. The information on health services is widespread in the age of the internet being popular such as exercise video guides, food information that we can find easily on many internet platforms.
We, the developers therefore have the idea of developing health care applications for the elderly with medical conditions which are 4 diseases: chronic kidney disease, diabetes, blood pressure and hyperlipidemia. This is an application that can calculate calories from raw materials used in cooking, notifying users if eating nutrients that affect the disease beyond the specified threshold, recommend a menu suitable for the current illness and can detect user’s current activity by walking and running through the Activity Recognition Client. This application is for encouraging the elderly to easily take care of their health at home.

Keywords: Calories, Activity Recognition Client, Elder","Calories, Activity Recognition Client, Elder",,Special Project,,
nng,"Data collection system for safety inspection of radioactive materials

and off-site radiation generator",2019,,,,"Radioactive material and radiation generator have been long applied in Thailand. The implementation of the appropriate regulations to ensure the radiation safety is officially practiced 
since such sources are potentially danger to life and property. In Thailand, the regulations have been authorized and conducted by the Office of Atoms for Peace It is found that the officers
 employ a paper-based recording, which requires them to collect and record on-site data using papers and then digitalizes the recorded data into the office’s database upon their return.
 According to the finding from the interview, this method contains some limitations: time constraints, limited database storages, and increasing data growth. 
        Therefore, this study aims to develop the system that facilitates the data recording and storage for the Office of Atoms for Peace’s on-site authorized officers
 who inspect and evaluate the radioactive material and radiation generator. The system involves two main parts: (1) the mobile application for the on-site data recording;
 and (2) the web application which processes and display the information to assist the officers’ report, inspection, and planning. This system is prospectively expected to be the timesaving 
facilitator for the pre-onsite visit planning and documentation, the operative assistant for data recording and storage, and the effective engine for data search and query. 
In addition, the system improves the issue of the limited database storages by using the cloud service. 

Keywords: Regulations for Radiation safety, Inspection and evaluation of on-site radioactive material and radiation generator, Application for data recordings for the radiation safety inspection of the on-site radioactive material and         radiation generator","Regulations for Radiation safety, Inspection and evaluation of on-site radioactive material and radiation generator, Application for data recordings for the radiation safety inspection of the on-site radioactive material and 	radiation generator",,Special Project,,
suk,Streaming for online learning,2019,,,,"Most of the population in Thailand are able to access the internet. Internet access has increased every year, whether the usage concerns communication, data search, or the purchasing/selling of items. Usage of Multimedia has been introduced in various forms as it allows the viewers a better understanding than the introduction via wordings, pictures, sounds, or through a single showing. Streaming is one of the methods. The objective of this project is to make adjustments in coordination with the learning methods in universities. In university, students are required to attend classes in accordance to the schedule established by the university. However, on some occasions, student are unable to attend classes according to the schedule. This may cause some students to fall behind in their classes. In addition, listening in class may not provide full comprehension. Therefore, this project is the creation of a website for online learning. This allows students the ability to study past classes and review the content of each class.

Keywords: Streaming, Video, VOD – Video On Demand","Streaming, Video, VOD – Video On Demand",,Special Project,,
wir,,2019,,,,"Nowadays, investment in funds has become more popular. Those interested in investing usually have different purposes causing financial planning to become more crucial in their lives. All investors want to allocate their funds or assets in the proportion that received the most profit.
         However, there are approximately 300 financial planners in Thailand who have the knowledge and expertise in investment planning and the cost of consulting services is very high. Also, each planner may use technique differently based on his expertise and experience in some asset classes.
         This system has therefore designed a method for selecting funds using machine learning techniques by clustering assets into groups based on their risks and returns. The objective is to help searching a group of funds that match investors' ability to accept risks and provide the best returns. The performance of the result from the system and the result derived from financial planner are compared.
         
         Keywords: Asset Allocation , Machine Learning, Return, Risk (Standard deviation)
         (2)","Asset Allocation , Machine Learning, Return, Risk (Standard deviation)
         (2)",,Special Project,,
onj,,2019,,,,"The beauty business is a business with a growth rate and continuously expanding every year. Because nowadays, both males, females, and during adolescents and older, they pay more attention and care about health, beauty, skin, as well as taking care of themselves more. Likewise, consultations and inquiries about beauty are increasing every year as well. This is caused by many factors, such as whether consumers want to ask for more information or lack of knowledge about beauty. As there are a few ways that consumers can get the right advice, and finding someone who can give the right advice is quite difficult. Which has many users who want to consult about plastic surgery directly with a consultant. In addition, questions or advice from the consultants that most users need are very similar that it makes the consultants take a long time to answer the questions to the users. This is the reason we design and create an automatic chat bot to give people answers about handicrafts and surgery in order to help beauty consultants reduce their time in replying questions. This chat bot uses NLP technology and works on Line application.
       
Keywords: Beauty, Consultants","Beauty, Consultants",,Special Project,,
yao,Food Allergy Assistant,2019,,,,"Nowadays, there are a growing number of people with food allergies. The allergic reactions can range from mild to severe. Thammasat Chalermprakiet Hospital has produced and distributed the leaflets to patients with food allergies, with the intent of preventing them from taking certain allergic ingredients. However, because of the excessive information on food and basic guideline for the allergies, the leaflets became ineffective. Most people usually forget and do not carry them around. 
This project is aimed to design and develop an application that would promoting healthcare for people with the allergies. The application provides information on food ingredients, suggestions, effective self-care, and how to deal with certain situations if the allergic reactions occur. Furthermore, the patients can record their allergies in the application to keep an allergy history and make it easier for the caretaker to treat the patient. The application has been tested on its usability with some sample users. We found that users can use all of functions in application successfully. Also, they can perform all functions within the expected time, i.e., a symptom checking function within 20 seconds, read the Epipen instruction function within 5 seconds and could make emergency calls within 5 seconds. 
Keywords: Food allergy, Food product barcode scanner, Allergy history","Food allergy, Food product barcode scanner, Allergy history",,Special Project,,
pho,Kin Keb Ru,2019,,,,"Currently, There are a large number of people with Non-Communicable diseases (NCDs). 
Most of them are caused by their own health risk behavior. For example consumption of foods that are sweet, oily and salty too much and lack of exercise, etc. 
However, If reducing this behavior, it can prevent and reduce risk factors of this disease. But food labels found on each product it has a small size and specific terminology. 
Some people don’t know the meaning of the vocabulary, Misunderstand and take time to study how to read the correct label and specific terminology is a waste of time. 
There is currently no source of information or an organization that collects food label information, many types of ready-made foods and still new production all the time. 
It is difficult to collect information.
     Therefore, the authors of this project has design and develop the mobile application ""Kin Keb Ru"" with a system that can collect food label information through Crowdsourcing 
Methods and OCR technique is used to process images into text to help users fill in information and to improve the efficiency of OCR, which the application can help control dietary 
habits. The risk groups are NCDs that are interested is diabetes mellitus, hypertension, metabolic syndrome and stroke.
       
Keywords: Non-Communicable diseases(NCDs), Health risk behavior, Label
","Non-Communicable diseases(NCDs), Health risk behavior, Label",,Special Project,,
onj,Price Estimation System for Commercial Building,2019,,,,"Currently, the construction of commercial buildings for commercial or residential use is widespread, especially land. 
Which has a high appraised value The producer therefore has developed a Price Estimation System for Commercial Building for contractors or general users to assess the price of commercial building structures. From choosing the structure and province of construction And displays the amount of raw materials, raw material prices, and the estimated price of commercial building structures To use the price to make a decision to build a building This application enhances the decision to purchase raw materials for building construction or building. ",,,Special Project,,
wir,RoomandRoommate matching system,2019,,,,"Nowaday, Life of university student need many facilities include accommodation or dorm that is the one of necessary thing. Meanwhile technology call phone is become one of most influential technology in our routine. Therefore making and developing website to finding dorm from phone and computer will make us find it easily than ever. But exist website has some restriction in system that can not doing. In addition managing system or roommate recommendation and matching system make us as developer see the opportunity to solve problems. As a result we have to develop and make website for finding dorms that can      manage roommate. Working system is to make users put in their information and other requirements such as prices, type of room, contracts, majors, degree and gender then system will matching similar requirements to other users of this website.",,,Special Project,,
ron,Pet-Connect,2019,,,,"Pet-connect is a web application that develop from platform that helps 
the cooperation among people in society.To help pets by using the function 
within the web application to provide convenience for pet owners and person 
need to provide helping with lost pet. When found lost pet, people who found 
can scan QR Code attached to the collar for inform location 
that found lost pet latest to pet owner.Make the helping efficiently and quickly.

Keywords: web application, platform, QR Code","web application, platform, QR Code",,Special Project,,
pho,Azzembly,2019,,,,"Tourism is a journey that to discover new places. It is traveling to
explore the place that we have never experienced before or the places that make one
feel fun and relax. Mostly, tourism is group activity; people enjoy tourism with friends,
family, relatives, and acquaintances. However, the problem that is encountered regularly
is that the free time of group members going out to travel is not the same. Therefore,
there is a form of group on social networks such as Facebook in order to find friends to
travel. 
	Nevertheless, the Facebook system does not yet support the specific filtering
function including the security, accuracy of user information on Facebook. It cannot be
trusted.
Hence, the project team has developed a travel companion system that
has a travel recommendation system to filter specific information to meet the needs of
users.
Keywords: Facebook, Recommendation system","Facebook, Recommendation system",,Special Project,,
nng,HomeOwnerHelpdesk,2019,,,,"HomeownerHelpdesk is conducted to notify the problems of people who 
buy real estate. To help the residents report various problems and follow up with 
smartphone . The residents can set time for staff to take corrective actions and also 
receive a warning message from staff if there is news to notify.This project to coordinate 
between  employees and  residents so that the employees can prioritize problems to help 
the residents full efficiency and make satisfaction.


Keywords: Satisfaction , Coordination
","Satisfaction , Coordination",,Special Project,,
nng,,2019,,,,"Nowadays, Office of Atoms for Peace (OAP) have a database
management system but that can be accessed everywhere, because of the closed
system to use at the office and entrepreneur the use of mail or go to the office. Cloud
storage to help provide access to information anywhere and enables operators to
request the license system online. Thus saving the time and expense of sending the
document in the mail or at the office.
              The purpose to Office of Atoms for Peace, the most comfortable in the
current authorities must act to fill in the database manually. Staff can access
information anywhere. Entrepreneur save transmission time and Office documents to
operator can request to shorten the time to do that, at least on paper. Develop a
database of Amazon Web Service (AWS), using Amazon Relational Database Service
(RDS) and use Amazon EC2 to connect to the database.",,,Special Project,,
pho,GroupUp,2019,,,,"The GroupUP appointment management system is a prototype system developed to help service Manage appointment times for large groups of people. With the system running on a mobile application There is a function to help users. Manage attendees, suggest a suitable day and time, suggest location matching with appointment and Track attendee expenses.
The author expects that the system will improve the current group appointment system.


Keywords: Time management, Scheduling times for large groups of people, Mobile application
","Time management, Scheduling times for large groups of people, Mobile application",,Special Project,,
onj,,2019,,,,"Children are valuable resources and are important to future domestic development. Children should be developed to be equipped with physical, mood, emotions, society and intellectuals to keep children quality, From education found that developing people who will be the most quality is the development of the early days of life in 0-5 years. But because the Thailand structure changes. Parents need to go out to work both, Single family in Bangkok, there is no time to pick up the child.

So developers have developed. ""Application for matching nanny services” To help facilitate the parent in service. It also helps to nanny to serve.

Application for matching nanny services divide the use of 2 parts. The first one for parents who can find nanny in the nearest area. The second one for nanny can choose to get a parenting request. For convenience to parents and nanny.

Keywords: Children, Parent, Nanny","Children, Parent, Nanny",,Special Project,,
yao,Tooth Pick,2019,,,,"The dentistry program at Thammasat University requires the fifth or higher year dentistry students to practice their skills with real patients via designed clinical courses. These courses are arranged in some fixed time slots. Students must organize themselves and book slots based on the availability of dental operatory bays, equipment and patient’s free time. In the past, students use Google Sheet as a bookkeeping of the timetable to reserve slots among them and use Discord to communicate the arrangement.
	The aim of this project is to develop a reservation system for the clinical courses for the dentistry students. The system is developed as a website using React.JS, PHP and MySQL. The main functionalities of the system are reserve slot, automatic schedule and assign time slot to students, exchange a slot among peers if desired. The system is easy to maintain and could help students to reserve and schedule the time slot easily. 
Keywords: reservation system, automatic schedule","reservation system, automatic schedule",,Special Project,,
pok,RECOMMEND TOURIST ATTRACTIONS KANCHANABURI WITH WEB APPLICATION,2019,,,,"To day, the tourism industry plays a role in Thailand's economy, including the development of information technology. Allowing tourists to search for tourist attractions of interest via the internet. Which recommends the tourist spots of most general websites, will introduce a wide range of tourist information. Causing tourists to not be recommended tourist attractions that are appropriate for their needs.
The developer has designed the website to recommend travel plans, Karnchanaburi. That can recommend travel plans that are most suitable for tourists, calculate travel expenses, recommend interesting places and important places in an emergency. To increase convenience in tourism planning for tourists who want to travel to Kanchanaburi.

Keywords: Travel plans, Recommended System, Travel","Travel plans, Recommended System, Travel",,Special Project,,
wan,Integration of Machine Learning and Electromactnetic Survey for Hydrocarbon Exploration,2019,,,,"Nowadays, petroleum industry has a new method call ‘Electromagnetic Survey for Hydrocarbon Exploration’ by releasing electromagnetic waves to the survey area and waiting to receive the reflected signal back. Once the measured results have been processed and analyzed by experts will be able to determine whether the liquid that appears inside the rock is water or petroleum.

	The purpose of this research is to study the guidelines for applying machine learning techniques to analyze the data received from reflections from the survey area with the aim of reducing the time spent by experts. And increase the accuracy in data analysis. This research will compare the algorithms used to analyze the survey data with a focus on supervised learning and unsupervised learning to find the most accurate method.
	
The experiments of the producer are made in two ways, which are The statistical values and the machine learning patterns The results of both accuracy and duration will be compared in order to see whether the integration of machine learning knowledge in the survey is really useful or not.
Keywords: machine learning, supervised learning, unsupervised learning","machine learning, supervised learning, unsupervised learning",,Special Project,,
pho,My Mood Day,2019,,,,"Depression is becoming a threat to people on both genders and in every range of ages. The World Health Organization (WHO) have found that there are more than 300 million of depression sufferers worldwide, with at least a person committing suicide per 40 seconds. 
	As a countermeasure to prevent the said threat, Mood Tracking was invented as a psychological technique that helps patients by recording their emotions daily and adjusting them based on the statistics. There are currently mood-tracking applications created for that purpose and have been widely used in various countries. Although unfortunately, It is crucial to raise a point that there is not a single one of said applications available for Thai patients at the moment.
	For this reason, the students of the Bachelor of Science Program in Computer Science, Thammasat University, decided to invent the mood-tracking application specifically for Thai patients in the academic year of 2019. Although, it is noted that the risk of patients causing self-harm could not be determined simply by tracking emotions. Moreover, some of the users also suggested that the application is not appealing and difficult to interact. Subsequently, the developers took note to the feedback and improved the application by introducing the ability to diagnose the risk of suffering from depression or bipolar disorder, and the risk of causing self-harm. 
	The application would determine the risk by assessing the written text of patients using trie-based natural language processing.

Keywords: Depression, Bipolar Disorder, Mental Health","Depression, Bipolar Disorder, Mental Health",,Special Project,,
scw,Alzheimer’s patient screening system from brain image,2019,,,,"Alzheimer's patient screening system from brain imaging Developed for the
study of the use of image processing techniques to analyze brain images by being
able to separate normal and human brain with Alzheimer's disease. As for the image
processing techniques, there are a variety of algorithm applications, so it is a
challenge to choose which method of creating this project.
The format of the system is created in the form of a web application by
allowing the user to select their own brain image and the system will compare that
world with the original image processed as a result. Can tell the user whether their
photo is Alzheimer's or not is Alzheimer's

Keywords: Alzheimer's , image processing, brain image","Alzheimer's , image processing, brain image",,Special Project,,
scw,"Mobile Application For Pterygium Analysis
",2019,,,,"Cataracts caused by human behavior are Pinguecula is a disease caused by abnormal tissue that 
turns into a benign tumor in the sclera conjunctiva can be observed with the naked eye and still able 
to grow into the cornea. In the case that it has spread into the cornea, it will be called Pterygium. 
The cause of cataracts comes from the eyes that are exposed to wind, sun, sunlight, dust, heat, and radiation. 
Pterygium will affect the vision a lot because the tissue has spread into and obscure the vision of the eyes. 
When the doctor found that the tissue began to spread to the black eye and the size of the cataract is so large 
that the doctor can see that it penetrates into the iris or is frequently inflamed, the doctor may consider surgery to remove. 
To measure the size of pterygium in the eyes, it is necessary to use medical tools. For treatment it is necessary to be diagnosed 
by a medical specialist. By using medical devices to measure pterygium size. In which the same patient has to undergo medical 
examination multiple times, results may vary by deviation
     Therefore, developers have created an application for measuring the size of pterygium with image processing 
for helping physicians in classifying and measuring pterygium sizes of patients. Resulting in pterygium size measurement more 
effective and reduce the discrepancy of results. Which the system will analyze and shows that what percentage of the pterygium 
that spread into the black eye or width and length of the pterygium to be suitable for medical treatment.

Keyword : Application for measuring the size of pterygium",Application for measuring the size of pterygium,,Special Project,,
pok,,2019,,,,"The corona virus, or Covid-19, is a virus that has spread worldwide, changing people's lifestyle. Many situations and problems cause people to adjust. Including how to closely monitor the risk areas for infection, the epidemic situation 	Therefore, the author sees the problem of tracking the source of the data of infected people and the risk areas and therefore has an idea and interested in developing a chat bot which is the center for tracking news about coronary virus or Covid-19.
",,,Special Project,,
pho,Practeeth,2019,,,,"Oral abnormalities screening is an important and necessary dental
procedure for the diagnosis and planning of oral health care. This is considered
to stop the disease process and maintain good oral health. Having someone with
basic dental knowledge to help screen basic oral disorders is a good thing.
However, those with basic dental knowledge are not able to receive training only.
It is necessary to go through a certain level of practice in order to be able to
perform screening for abnormalities in the mouth correctly and efficiently. But
practical training is difficult and has many limitations, such as after the training
is completed there is no source of knowledge to review and practice screening
skills for oral disorders.
    The system developers have the idea to develop media in the form
of mobile application for screening dental disorders by bringing the concepts of
E- learning into use. The system consists of 3 parts which are learning section,
assessment section, and the screening tools. To be a tool for improving the
potential of oral screening for Village Health Volunteers (VHV) and nurses to have
more knowledge and expertise.
Keywords: Oral Screening Tools, Dental, E-learning.","Oral Screening Tools, Dental, E-learning.",,Special Project,,
wan,CSTU Academic Information System,2019,,,,"Department of Computer science at Thammasat University have to work with a lot of information, some are in form of papers, some are in form of .xlsx files. And we have no database that storage all information together. So it could make Ineffective decisions about both of academic and student affairs part. Regarding to this problem the project team decided to develop a web application which gather all necessary information together and display information which will help staff or related person making more effective decision.
CSTU Academic Information System includes two parts. First is database management part. This part is for storage necessary information into database. We designed to allows administrator to import data by typing data in form and importing file to database. Second is data visualization part. This part is for display data from database in form of dashboard which contains understandable clearly and up to date information.
Finally, we expect that our CSTU Academic Information System will be helpful to all staffs and related persons at Department of Computer science to making more effective decision and making useful plan for our department in the future",,,Special Project,,
was,Performance test  for WebRTC technology,2019,,,,"In the past, real-time communication via voice or video required technology. Signal processing That is owned by a group of people or a company, and users need to install additional plug-ins or other applications in order to be able to use, which makes it a limitation to applications in this way.
Currently, WebRTC has developed a real-time data communication system. Such as audio, video through a Web Browser that supports the HTML 5 standard without using a device additional programs or plug-ins Which can be done directly through a browser that supports WebRTC.
The objective of this project is to test the WebRTC system in the video streaming, taking about video quality received by the user, and therefore tested the latency, Bit rate. And also test video resolution suitable for use 

Keywords: WebRTC, Low latency, Video Streaming, Bit rate, WebRTC","WebRTC, Low latency, Video Streaming, Bit rate, WebRTC",,Special Project,,
wil,,2019,,,,"At present, there are many car users. When the drivers use the parking lot of various places such as government buildings, hospitals, shopping malls, etc., the drivers will encounter problems, because the drivers cannot know the vacancy of the parking lot. Traffic inside the parking lot is stuck. Resulting in people who use the service to waste time and waste fuel to find parking spaces.The objective of this research is to develop a parking management system by using the Arduino UNO R3 SMD and Ultrasonic Module HC-SR04 Distance Sensor by using the parking sensor to display the signal through the signal light. When users use it in various parking places, it will help car users save time in finding parking spaces, reduce pollution and reduce costs due to fuel waste.

Keywords: parking, sensor ultrasonic","parking, sensor ultrasonic",,Special Project,,
wil,The Spammail Detection System with Privacy Concern,2019,,,,"Nowadays, Spam mail has been causing damages and annoyances to users. Most of the email providers are still not be able to detect spam mail effectively. The process of detecting spam mail has to access data inside in order to determine whether it is a spam mail or not. Therefore, users can not protect their privacy. On the other hand, Spam mail detector will help identifying spam mail without accessing data inside so as to keeps user’s privacy.
	The process of Spam mail detector starts when users turn the Spam mail detector on. The program will use header of the email to calculate points based on rule and score that was defined. After process completed, Program will add spam status and spam score into email header for specific that email’s a spam mail or not. The program is able to evaluate the success rate by compare f-measure score with SpamAssassin’s score and the result show that detection of system has efficiency score close to detection of SpamAssassin.

Keywords: Spam Mail, Spam Mail Detector, SPF, DKIM, SMTP, Mail
","Spam Mail, Spam Mail Detector, SPF, DKIM, SMTP, Mail",,Special Project,,
pak,SILENCE BROKEN KEYBOARD APPLICATION,2019,,,,"Communication is important for a good relationship. But sometimes people don't know 
how to communicate for maintaining relationship. This project aims to develop application 
that can help users with dialogue suggestion methods and solve problems if users do not know what to talk.

Keywords: Keyboard Application, Conversation, Recommendation System","Keyboard Application, Conversation, Recommendation System",,Special Project,,
pok,PM 2.5 Tracker,2019,,,,"This research can be accomplished because of kindly assistance, advice, and checking for various defects. As well as equipment support from Dr.Pokpong Songmuang, a special project advisor. Until the project was accomplished well The producer wishes to thank you very much for this opportunity.
Thank you to the friends and brothers from the Department of Computer Science that provide advice and answers to various questions.
Thank you to the parents for always encouraging me.
Mr. Ritthinon Soontornarom",,,Special Project,,
ron,Development of Web Application and Analytic Data Dashboard Using Message Queue,2019,,,,,,,Special Project,,
wir,,2019,,,,,,,Special Project,,
pak,Application for Thai pronunciation practice,2019,,,,"This project presents an Application for Thai Pronunciation Practice. 
The Application can identify wrong pronouns word then give scores to user and record high score in application.
This application used CMUSphinx,an Automatic Speech Recognition Technology by create new Acoustic Model 
for systems to identify wrong pronouns word. We expects this project can help Application 
Users to learn about how to pronouns word in Thai correctly.

Keywords: Speech Recognition Technology, Acoustic Model, Difficult Word in Thai Language.","Speech Recognition Technology, Acoustic Model, Difficult Word in Thai Language.",,Special Project,,
tan,Big Data Analysis System for Real-Time Extracting an Important Event on Twitters,2019,,,,"In this research, we present a big data analysis system for real-time detection 
of important events on Twitter, consisting of 3 steps. First, all tweets relating 
to university in Thailand were collected and prepared for further analysis. 
Then, those tweets were presented to Twitter’s users to provide trends relating 
to university in Thailand and in-depth information also available such as 
number of retweets per minutes, popular tweets for a period of time, etc. 
Lastly, the events data collected from Twitter were analyzed by using 3 attributes 
which are the average slope, the difference between the maximum and the minimum slopes,
and the cumulative number of retweets. After comparison of 3 evaluation metrics; 
accuracy, precision, and recall, the results are (a) the average slope at 7.5 threshold 
has the highest accuracy which equals to 92.65% (b) the average slope at 9.75 
has the highest precision of 80% (c) the cumulative number of retweets at 23.1 threshold 
has the highest recall of 55.10%

Keywords: Data collection, Data preparation, Twitter’s data collection system, 
Twitter’s data analysis","Data collection, Data preparation, Twitter’s data collection system, 
Twitter’s data analysis",,Special Project,,
pok,Robot for Education using Block-based Programming,2019,,,,"Nowadays, robotics education is increasingly used in education. This project is a project that develops educational robots that 
integrate with drag-and-drop programming using the Arduino circuit developed from the Google Blocky program from Google 
since Thailand has not yet developed robotics for education using drag-and-drop programming using Arduino language. 
And use only drag-and-drop programming makes children unable to see tangible results


Keywords: Arduino, Google Blockly, Drag-and-Drop Programming","Arduino, Google Blockly, Drag-and-Drop Programming",,Special Project,,
wan,Water Quality Classification using Data Mining Technique: A case study on Wang river.,2019,,,,"In order to survive, the creatures need water, food, air, and residency. However, there are many water crises that influence water quality or cause water shortages. Rivers, which are important freshwater sources for human consumption, are often waste caused by their activities. This research utilizes data mining techniques to create classification models for water quality issues. The results are used in the application to alert the public about water quality problems that are aimed at changing their behavior. The data set consists of nine input features to identify dissolved oxygen in the next quarter, divided into two levels: ""good"" and ""bad"". The process of data preparation using various methods applies to the raw data before creating the classification model. This research proposes neural networks with a multilayer perceptron type 2 and k-nearest neighbor as a classifier called MLP2-kNN. The results show that the proposed model is more effective when compared to various MLP algorithms. The classification rate is more than 0.95 while the F-score of both two classes is more than 0.9. Finally, the proposed model is implemented on the web application to report and prepare for water utilization planning.
Keywords: Water quality classification, Data Mining, Neural networks, k-nearest neighbor.","Water quality classification, Data Mining, Neural networks, k-nearest neighbor.",,Special Project,,
pok,Smart Grow Box,2019,,,,"Indoor plants is one of the way to plants that does not grow in nature environment, 
there is a simple way to plant but there is also a trouble.There are many growth factors 
must be controlled such as temperature ,humidity ,light and soil moisture.Grower need to have 
knowledges ,patience and high of responsibility for planting.This project intend to bring technology 
to help in planting that the grower can control these growth factors via application on smartphone in 
order to make life easier for planting, also the result will be as expected and correct.",,,Special Project,,
rdk,Healthy Food Recommendation Robot,2019,,,,"Health care received a lot of attention today. Nutrition information is a part of important health care. Network nutritional recommendation system is another channel that has useful. For the chat bot system, the introduction of this nutrition information has calculate The body mass index (BMI), Basal metabolic rate (BMR), Calorie value and recommend a healthy food menu.
Chat bot system for nutrition information recommendations to chat and collect individual data for calculate  The body mass index (BMI) , Basal metabolic rate (BMR) , Calorie value of nutrients that must be received at each age to set a  goal and recommend to recommend a healthy food menu from the assessment of usage from the system trial.",,,Special Project,,
rdk,,2019,,,,"Depression has caused loss to the family and loved ones. If knowing beforehand and able to accept, it will help reduce the loss that can occur. The producer has therefore developed Chatbot Friends to talking to be like ears and eyes for the observation that will help depression person to know they are, and able to return to normal life. The system will screen normal and depressed people. After the system gets the group of depression people then the chatbot will start chatting with the user. In conversation will be a conversation to create peace of mind.
For people with depression will feel better and hope that this chatbot will help reduce depression person then can live like as normal people.",,,Special Project,,
wil,QuickShow - Web Presentation & Slide Show,2019,,,,"Data collection can be a tedious task. If there are lots of data to work with, the team might not have enough time to make an attractive presentation, and when the presentation is not attractive, they tend to be ignored no matter now useful their data can be. This project has a goal to help those people, by focusing on user communication and user collaboration. By separating workspace, unlike existing solution, user will theoratically be more focused on the task they're assigned to, thus the job can be closed quicker. 

Keywords: presentation, slide, collaboration.","presentation, slide, collaboration. ",,Special Project,,
pok,Guitar Function Box,2019,,,,"Guitars Function Box was developed Equipment from Board Arduino by C language in order to control the process of working and ordering. This instrument was developed to work with musical instruments such as the electric guitar and acoustic electric guitar by passing through amplifier. Guitar Function Box can change the guitar’s sound from the basic (original). Processes of work is similar to the guitar effect pedal, when you pressed the buttons the sound will change. And If you pressed the button again the voice will change to be normal.",,,Special Project,,
rat,,2019,,,,"Loan of equipment in the TONKIT Laboratory at CSTU has no management system for managing equipment information or recording loan data or tracking loan status. Those who would like to borrow an equipment just need to ask for admin lecturer’s permission.
This makes it hard to keep track of equipment that students borrowed and whether they have returned the borrowed equipment or not. Because there is no management system for equipment borrowing that records loaning data or tracking equipment loaning for the TONKIT Laboratory.
      This project designed and developed the Equipment Loan Management System for the TONKIT Laboratory to help recording and tracking equipment loan within the laboratory. This system used QR Code to identify equipment and provide convenience when checking out and returning the equipments. It also reduces errors and redundancy in equipment loaning. The system is divided into two parts : a web application and a mobile application.System Administrators can use both the web application and the mobile application while general users can use the mobile application.The applications are easy to use and do not pose any learning curve for the users
Keywords: Equipment Loan management, Loan Version Control, QR Code.","Equipment Loan management, Loan Version Control, QR Code.",,Special Project,,
wdp,DISCRETE STRUCTURES,2013,,,,"Intensive introduction to discrete mathematics as applied in computer science: 
Basic logic and its main application in Digital logic, proof techniques, sets, functions, 
relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
nng,DISCRETE STRUCTURES,2013,,,,"Intensive introduction to discrete mathematics as applied in computer science: 
Basic logic and its main application in Digital logic, proof techniques, sets, functions, 
relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
,DISCRETE STRUCTURES,2018,,,,"Intensive introduction to discrete mathematics as applied in computer science: 
Basic logic and its main application in Digital logic, proof techniques, sets, functions, 
relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
wdp,DISCRETE STRUCTURES,2018,,,,"Intensive introduction to discrete mathematics as applied in computer science: 
Basic logic and its main application in Digital logic, proof techniques, sets, functions, 
relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
nng,DISCRETE STRUCTURES,2018,,,,"Intensive introduction to discrete mathematics as applied in computer science: 
Basic logic and its main application in Digital logic, proof techniques, sets, functions, 
relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
wlr,COMPUTER PROGRAMMING FUNDAMENTALS,2013,,,,"Algorithmic problem solving, structural design and programming, programming 
language syntax and semantics, data types, control structures, functions and parameter 
passing, recursive functions, testing and debugging",,,Course,,
,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming 
language syntax and semantics, data types, control structures, functions and parameter 
passing, recursive functions, testing and debugging",,,Course,,
wlr,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming 
language syntax and semantics, data types, control structures, functions and parameter 
passing, recursive functions, testing and debugging",,,Course,,
scw,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming 
language syntax and semantics, data types, control structures, functions and parameter 
passing, recursive functions, testing and debugging",,,Course,,
skn,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
scw,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
nth,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
ojs,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
yao,SOFTWARE DEVELOPMENT USING APPLICATION PROGRAMMING INTERFACE,2013,,,,"Programming using Application Programming Interface, user interface programming including streams and files, data collection library, and event-driven programming.",,,Course,,
,SOFTWARE DEVELOPMENT USING APPLICATION PROGRAMMING INTERFACE,2013,,,,"Programming using Application Programming Interface, user interface programming including streams and files, data collection library, and event-driven programming.",,,Course,,
wjr,DATA STRUCTURES,2009,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data 
structures including linear data structures and nonlinear data structures, dynamic storage 
allocation, searching and sorting techniques.",,,Course,,
,DATA STRUCTURES,2009,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data 
structures including linear data structures and nonlinear data structures, dynamic storage 
allocation, searching and sorting techniques.",,,Course,,
,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data 
structures including linear data structures and nonlinear data structures, dynamic storage 
allocation, searching and sorting techniques.",,,Course,,
wjr,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data 
structures including linear data structures and nonlinear data structures, dynamic storage 
allocation, searching and sorting techniques.",,,Course,,
,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data 
structures including linear data structures and nonlinear data structures, dynamic storage 
allocation, searching and sorting techniques.",,,Course,,
,PROGRAMMING PRACTICUM USING APPLICATION PROGRAMMING INTERFACE,2013,,,,Practicum to enhance programming knowledge in CS211 course.,,,Course,,
,PROGRAMMING PRACTICUM USING APPLICATION PROGRAMMING INTERFACE,2013,,,,Practicum to enhance programming knowledge in CS211 course.,,,Course,,
pkw,COMPUTER ORGANIZATION AND ARCHITECTURE,2013,,,,"Data representation, assembly level organization, memory systems, interfacing and communication, functional organization, multiprocessors and alternative architecture, performance enhancement, and contemporary architectures",,,Course,,
wdc,COMPUTER ORGANIZATION AND ARCHITECTURE,2013,,,,"Data representation, assembly level organization, memory systems, interfacing and communication, functional organization, multiprocessors and alternative architecture, performance enhancement, and contemporary architectures",,,Course,,
ddp,COMPUTER ORGANIZATION AND ARCHITECTURE,2013,,,,"Data representation, assembly level organization, memory systems, interfacing and communication, functional organization, multiprocessors and alternative architecture, performance enhancement, and contemporary architectures",,,Course,,
nth,HUMAN INFORMATION PROCESSING,2013,,,,"Introduction to research and theory on topics in human information processing 
including perception, attention, pattern recognition, memory, representation of 
knowledge, language, problem solving, reasoning, and learning, with emphasis on the 
relationship to computer models of these processes and implications of this body of 
knowledge for building information systems",,,Course,,
,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology,
software process models, basics of software project planning and project management, 
basics of time and cost estimation, requirement elicitation and specification, software 
analysis and design, concept of software architecture, software construction techniques 
including design pattern, component-oriented development, basics of verification and 
validation, software evolution, software development environment, and sample of CASE
tools",,,Course,,
ssr,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology,
software process models, basics of software project planning and project management, 
basics of time and cost estimation, requirement elicitation and specification, software 
analysis and design, concept of software architecture, software construction techniques 
including design pattern, component-oriented development, basics of verification and 
validation, software evolution, software development environment, and sample of CASE
tools",,,Course,,
,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology,
software process models, basics of software project planning and project management, 
basics of time and cost estimation, requirement elicitation and specification, software 
analysis and design, concept of software architecture, software construction techniques 
including design pattern, component-oriented development, basics of verification and 
validation, software evolution, software development environment, and sample of CASE
tools",,,Course,,
ssr,PRACTICES AND PATTERNS IN OBJECT-ORIENTED PROGRAMMING,2013,,,,"Principle of advanced object-oriented programming, abstract classes, interfaces, inheritance, threads, advanced distributed programming, streams, serialization, inspection, reflection, event-driven programming, exception handling and case studies.",,,Course,,
mvp,SOFTWARE PROCESS AND QUALITY ASSURANCE,2013,,,,"Study essential components of software process including activities, methods, and 
practices used to develop and maintain software development including its work 
products. Introduce basic knowledge of process and product quality assurance, how to 
define quality for measures and measurement",,,Course,,
pkw,SOFTWARE REQUIREMENT SPECIFICATION AND MANAGEMENT,2013,,,,"Quantify many aspects of requirement processes include eliciting, analyzing, negotiating, specifying, validating and basic principle of managing requirements introduce methods, techniques and tools used to define, document and ensure customer satisfaction",,,Course,,
nng,SOFTWARE REQUIREMENT SPECIFICATION AND MANAGEMENT,2013,,,,"Quantify many aspects of requirement processes include eliciting, analyzing, negotiating, specifying, validating and basic principle of managing requirements introduce methods, techniques and tools used to define, document and ensure customer satisfaction",,,Course,,
mvp,SOFTWARE PROCESS AND QUALITY ASSURANCE,2009,,,,"Study essential components of software process including activities, methods, and practices used to develop and maintain software development including its work products. Introduce basic knowledge of process and product quality assurance, how to define quality for measures and measurement.",,,Course,,
,ART AND DESIGN FOUNDATIONS,2013,,,,"Fundamentals of art and design; color theory; composition, layout, 2D expression:ู painting and drawing; 3D expression: handmade, computer aided; overview of theoretical, practical and historical aspects of animation, film and video, game design, sound and audio, web design; creativity and ideation; impact: media as a social, cultural and political force",,,Course,,
wdp,NET-CENTRIC COMPUTING 1,2009,,,,"Study network standards, client-server models, internet protocol, network management, basic concepts of distributed computing, multimedia systems, mobile and wireless computing, and network security.",,,Course,,
tnt,NET-CENTRIC COMPUTING 1,2009,,,,"Study network standards, client-server models, internet protocol, network management, basic concepts of distributed computing, multimedia systems, mobile and wireless computing, and network security.",,,Course,,
wdp,NET-CENTRIC COMPUTING 1,2013,,,,"Study network standards, client-server models, internet protocol, network management, basic concepts of distributed computing, multimedia systems, mobile and wireless computing, and network security.",,,Course,,
,NET-CENTRIC COMPUTING 1,2013,,,,"Study network standards, client-server models, internet protocol, network management, basic concepts of distributed computing, multimedia systems, mobile and wireless computing, and network security.",,,Course,,
pkl,MOBILE APPLICATION DEVELOPMENT,2013,,,,"Developing landscape of mobile applications, web-based mobile applications, mobile platforms, the specific constraints and requirements of user interface design for limited-resource devices, conceptual overview, design issues, and practical development issues",,,Course,,
nth,COMPUTER SIMULATION AND FORECASTING TECHNIQUES IN BUSINESS,2013,,,,"Study basic concepts of simulation including design, experiment, testing and evaluating, constraints of simulation techniques and their uses in business decision, concepts and techniques of quantitative and qualitative forecasting and their applications in businesses.",,,Course,,
,DOCUMENT INDEXING AND RETRIEVAL,2013,,,,"Theories and techniques on computerized document indexing and retrieval.ูู Topics include Boolean model, vector model, text processing and analysis, evaluation of document retrieval systems, Web-based search engines.",,,Course,,
pps,BASIC THEORY IN ARTIFICIAL INTELLIGENCE,2013,,,,"Definition of Intelligent behavior, design of intelligent agents (nature of environments and nature of agents), problem solving by searching, uninformed search,  informed search, two-player search, constraint satisfaction problem, knowledge representation with logics, automated reasoning and theorem proving.",,,Course,,
,BASIC THEORY IN ARTIFICIAL INTELLIGENCE,2013,,,,"Definition of Intelligent behavior, design of intelligent agents (nature of environments and nature of agents), problem solving by searching, uninformed search,  informed search, two-player search, constraint satisfaction problem, knowledge representation with logics, automated reasoning and theorem proving.",,,Course,,
mvp,HUMAN – COMPUTER INTERACTION,2013,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
,SEMINAR IN SOFTWARE ENGINEERING,2009,,,,Seminar and field trip to study industrial use of best practices and standards of Software Engineering.,,,Course,,
,SEMINAR IN SOFTWARE ENGINEERING,2013,,,,Seminar and field trip to study industrial use of best practices and standards of Software Engineering.,,,Course,,
,SELECTED TOPICS IN COMPUTER SCIENCE,2009,,,,"Topics selected from areas in Computer Science, keep up with the new technology and knowledge",,,Course,,
,SELECTED TOPICS IN COMPUTER SCIENCE,2013,,,,"Topics selected from areas in Computer Science, keep up with the new technology and knowledge",,,Course,,
wdc,PARALLEL ALGORITHM DESIGNS,2009,,,,"Principles of task decomposition, task-processor mapping techniques for load balancing, methods for minimizing interaction overheads, parallel algorithm design models (including data-parallel, work-pool, task graph, master-slave, pipeline, and hybrid models), and examples of parallel algorithms",,,Course,,
wdc,PARALLEL ALGORITHM DESIGNS,2013,,,,"Principles of task decomposition, task-processor mapping techniques for load balancing, methods for minimizing interaction overheads, parallel algorithm design models (including data-parallel, work-pool, task graph, master-slave, pipeline, and hybrid models), and examples of parallel algorithms",,,Course,,
,INTRODUCTION TO MODELING AND SIMULATION,2013,,,,"Queuing model, basics of modeling, simulation principles and analysis methodologies focusing on discrete-event simulation, simulation tools and conducting studies to address research issues for complex computer systems, model verification and statistical issues in analyzing simulation results",,,Course,,
ppr,OPERATING SYSTEMS 2,2013,,,,"Advanced topics in operating systems:ูconcurrent processing, inter-process communications, distributed computing, network operating systems and case study.",,,Course,,
wlr,NET-CENTRIC COMPUTING 3,2013,,,,"Advanced knowledge and hands-on experience in computer communications and networking:ู design and configuration of switching systems, virtual LANs, inter-VLANs routing, wireless networks, network security, and Wide Area Networks (WANs)ูusing both state-of-the art simulators and actual computer networking devices.",,,Course,,
ksc,SELECTED TOPICS IN NET-CENTRIC COMPUTING,2009,,,,Seminar on current research and development in computer and telecommunication networking,,,Course,,
ksc,SELECTED TOPICS IN NET-CENTRIC COMPUTING,2013,,,,Seminar on current research and development in computer and telecommunication networking,,,Course,,
wjr,MANAGEMENT INFORMATION SYSTEMS,2009,,,,"Fundamental of information systems, organizations and management; Information technology infrastructure; Key System Applications: management and enterprise information systems; Building information systems; Managing information systems; Ethics and social impact of information systems",,,Course,,
wjr,MANAGEMENT INFORMATION SYSTEMS,2013,,,,"Fundamental of information systems, organizations and management; Information technology infrastructure; Key System Applications: management and enterprise information systems; Building information systems; Managing information systems; Ethics and social impact of information systems",,,Course,,
skn,DATABASE SYSTEMS 2,2009,,,,"Advanced database management system design principles and techniques. Topics may be selected from: access methods, query processing and optimization, transaction management: concurrency control, recovery, and data security; distributed database systems: concepts, design and transaction management, and introduction to objectoriented database systems",,,Course,,
skn,DATABASE SYSTEMS 2,2013,,,,"Advanced database management system design principles and techniques. Topics may be selected from: access methods, query processing and optimization, transaction management: concurrency control, recovery, and data security; distributed database systems: concepts, design and transaction management, and introduction to objectoriented database systems",,,Course,,
pps,ADVANCED SEARCH STRATEGIES,2013,,,,"Search space, combinatorial search, heuristic search, stochastic search, search tree, simulated annealing algorithm, evolutionary algorithm, local optimum problem, ridge and plateau problem, Performance evaluation, and example of applications",,,Course,,
,ADVANCED SEARCH STRATEGIES,2013,,,,"Search space, combinatorial search, heuristic search, stochastic search, search tree, simulated annealing algorithm, evolutionary algorithm, local optimum problem, ridge and plateau problem, Performance evaluation, and example of applications",,,Course,,
scw,SELECTED TOPICS IN MULTIMEDIA CONTENT ANALYSIS,2013,,,,Selected Topic on current research and development in multimedia content analysis,,,Course,,
ddp,FORMAL METHODS,2009,,,,"Introduction to different mathematical models and languages, known as formal methods used to model software and verify its correctness",,,Course,,
ddp,FORMAL METHODS,2013,,,,"Introduction to different mathematical models and languages, known as formal methods used to model software and verify its correctness",,,Course,,
,GAME PROGRAMMING,2013,,,,"ntroduction to current and future techniques for electronic game design and programming; Graphics for Games; Game Engines; motion generation, behavioral control for autonomous characters, interaction structure, and interface issues; Play Testing; marketing",,,Course,,
pkw,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
wjr,INTRODUCTION TO COMPUTER PROGRAMMING USING PYTHON,2018,,,,"Basic computer programming and problem solving using Python. Structural design 
and Programming, basic data types. Control Structures. Functions, python built-in data 
structures: lists, tuples, sets, dictionaries, and range, numerical processing library, arrays 
and matrix, library for Data Visualization.",,,Course,,
wlr,OBJECT-ORIENTED PROGRAMMING,2013,,,,"Developing of analytical and problem-solving skills through object-oriented 
paradigm. Integrating of conceptual and state-of-the-art practical approaches in software 
development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design 
and source code",,,Course,,
ssr,OBJECT-ORIENTED PROGRAMMING,2018,,,,"Developing of analytical and problem-solving skills through object-oriented 
paradigm. Integrating of conceptual and state-of-the-art practical approaches in software 
development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design 
and source code",,,Course,,
yao,OBJECT-ORIENTED PROGRAMMING,2018,,,,"Developing of analytical and problem-solving skills through object-oriented 
paradigm. Integrating of conceptual and state-of-the-art practical approaches in software 
development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design 
and source code",,,Course,,
,OBJECT-ORIENTED PROGRAMMING,2018,,,,"Developing of analytical and problem-solving skills through object-oriented 
paradigm. Integrating of conceptual and state-of-the-art practical approaches in software 
development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design 
and source code",,,Course,,
nth,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2009,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
nth,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
nng,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
wdc,SOCIAL AND PROFESSIONAL ETHICS,2013,,,,"Social context of computing, methods and tools of analysis, professional and 
ethical responsibilities, intellectual property, risks and liabilities of computer-based 
systems, privacy and civil liberties, intellectual property infringement, computer crime in 
computing",,,Course,,
pkw,PROGRAMMING LANGUAGES AND PARADIGMS,2013,,,,PROGRAMMING LANGUAGES AND PARADIGMS,,,Course,,
wjr,PROGRAMMING LANGUAGES AND PARADIGMS,2013,,,,PROGRAMMING LANGUAGES AND PARADIGMS,,,Course,,
,DATABASE SYSTEMS 1,2013,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management",,,Course,,
skn,DATABASE SYSTEMS 1,2013,,,,"Fundamental database concepts and architecture, conceptual data models, 
design and implementation, query languages, metadata, physical data storage, file 
organizations, and introduction to transaction management",,,Course,,
scw,DATABASE SYSTEMS 1,2013,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management",,,Course,,
ssr,OBJECT-ORIENTED ANALYSIS AND DESIGN,2009,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management",,,Course,,
,OBJECT-ORIENTED ANALYSIS AND DESIGN,2013,,,,"Object-oriented concepts, object-oriented methodology with unified process, objectoriented analysis, object-oriented design, object-oriented modeling with unified modeling language (uml), system architecture design, user interface design, functional design, database design and comparative study between structural analysis and design methodology and object-oriented analysis and design methodology.",,,Course,,
ssr,OBJECT-ORIENTED ANALYSIS AND DESIGN,2013,,,,"Object-oriented concepts, object-oriented methodology with unified process, objectoriented analysis, object-oriented design, object-oriented modeling with unified modeling language (uml), system architecture design, user interface design, functional design, database design and comparative study between structural analysis and design methodology and object-oriented analysis and design methodology.",,,Course,,
,OBJECT-ORIENTED ANALYSIS AND DESIGN,2013,,,,"Object-oriented concepts, object-oriented methodology with unified process, objectoriented analysis, object-oriented design, object-oriented modeling with unified modeling language (uml), system architecture design, user interface design, functional design, database design and comparative study between structural analysis and design methodology and object-oriented analysis and design methodology.",,,Course,,
mvp,INTRODUCTION TO SOFTWARE TESTING,2013,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system 
levels, and software testing process and planning.",,,Course,,
nng,INTRODUCTION TO SOFTWARE TESTING,2013,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system 
levels, and software testing process and planning.",,,Course,,
,MATHEMATICS FOR COMPUTER GRAPHICS,2013,,,,"Coordinate systems; local vs. world coordinate systems, cartesian, polar spherical, 2D and 3D coordinate systems, homogenous; transformations: viewing, perspective, orthographic, rotation, translation, scaling, deformations; random numbers; geometryplane and solid geometry; matrix and vector algebra; complex numbers and quaternions; parametric/non-parametric representation; numerical methods",,,Course,,
,COMPUTER GRAPHICS 1,2013,,,,"Software and hardware principles of interactive graphics, general methods for 
designing and displaying output, elementary operations in two-and three-dimensional 
space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
wlr,COMPUTER SECURITY,2013,,,,"Security of information, computer systems, databases, networks and 
communications, symmetric and asymmetric cryptography, digital signatures, digital 
certificates, malwares, security policies and models, security assurance, risk analysis, 
security administration of information systems",,,Course,,
,DESIGN AND ANALYSIS OF ALGORITHMS,2013,,,,"Techniques used in design and analysis of algorithms such as divide-and-conquer, 
greedy algorithms, dynamic programming, graph traveling, backtracking, branch and 
bound. Study the topics of sorting, searching, Fourier transform, randomized algorithms, 
graph algorithms and optimization methods.",,,Course,,
ddp,DESIGN AND ANALYSIS OF ALGORITHMS,2013,,,,"Techniques used in design and analysis of algorithms such as divide-and-conquer, 
greedy algorithms, dynamic programming, graph traveling, backtracking, branch and 
bound. Study the topics of sorting, searching, Fourier transform, randomized algorithms, 
graph algorithms and optimization methods.",,,Course,,
ojs,DESIGN AND ANALYSIS OF ALGORITHMS,2013,,,,"Techniques used in design and analysis of algorithms such as divide-and-conquer, 
greedy algorithms, dynamic programming, graph traveling, backtracking, branch and 
bound. Study the topics of sorting, searching, Fourier transform, randomized algorithms, 
graph algorithms and optimization methods.",,,Course,,
pkw,THEORY OF COMPUTATION,2013,,,,"Fundamentals of abstract machine and language, automata theory, regular 
expression, regular language, pushdown automata, context-free language, Turing machine, 
the halting problem, undecidability and intractability",,,Course,,
ddp,THEORY OF COMPUTATION,2013,,,,"Fundamentals of abstract machine and language, automata theory, regular 
expression, regular language, pushdown automata, context-free language, Turing machine, 
the halting problem, undecidability and intractability",,,Course,,
pkl,EMBEDDED SYSTEMS DESIGN,2013,,,,"Design and implementation of integrated hardware and software for embedded 
systems to meet specifications/constraints, applications of embedded systems and 
related research.",,,Course,,
ksc,OPERATING SYSTEMS I,2013,,,,"Basic concepts of operating systems design and implementation mutual exclusion, 
process management, memory management, file and device management, computer 
security and case studies.",,,Course,,
ppr,OPERATING SYSTEMS I,2013,,,,"Basic concepts of operating systems design and implementation mutual exclusion, 
process management, memory management, file and device management, computer 
security and case studies.",,,Course,,
tnt,OPERATING SYSTEMS I,2013,,,,"Basic concepts of operating systems design and implementation mutual exclusion, 
process management, memory management, file and device management, computer 
security and case studies.",,,Course,,
ksc,INTRODUCTION TO CLUSTER COMPUTING AND DISTRIBUTED COMPUTING,2013,,,,"Study the construction of Beowulf and cluster computers, building a small cluster, 
using cluster installation and configuration tools, cluster monitoring, job scheduling, user 
management, cluster systems maintenance, security measures and policies.",,,Course,,
skn,BUSINESS COMPUTING AND ELECTRONIC COMMERCE,2013,,,,"Concepts of information systems in business management, business application 
software, business processes over computer networks, electronic commerce 
infrastructures, computing and data exchange standards",,,Course,,
ppr,COMPUTER SIMULATION AND FORECASTING TECHNIQUES IN BUSINESS,2013,,,,"Study basic concepts of simulation including design, experiment, testing and 
evaluating, constraints of simulation techniques and their uses in business decision, 
concepts and techniques of quantitative and qualitative forecasting and their applications 
in businesses. ",,,Course,,
wjr,DOCUMENT INDEXING AND RETRIEVAL,2013,,,,"Theories and techniques on computerized document indexing and retrieval.ูู Topics include Boolean model, vector model, text processing and analysis, evaluation of document retrieval systems, Web-based search engines",,,Course,,
,ARTIFICIAL INTELLIGENCE IN PRACTICE,2013,,,,"Probabilistic reasoning, non-monotonic reasoning, frame, conceptual graph, 
Bayesian networks, markov networks, relational probability models, hidden Markov 
models, decision theory. This course focuses on implementation using artificial 
intelligence programming such as prolog, lisp or python.",,,Course,,
pps,ARTIFICIAL INTELLIGENCE IN PRACTICE,2013,,,,"Probabilistic reasoning, non-monotonic reasoning, frame, conceptual graph, 
Bayesian networks, markov networks, relational probability models, hidden Markov 
models, decision theory. This course focuses on implementation using artificial 
intelligence programming such as prolog, lisp or python.",,,Course,,
pkl,MACHINE LEARNING,2013,,,,"Learning theory, inductive and deductive learning, naïve Bayesian learning, 
decision trees, supervised learning, unsupervised learning, reinforcement learning, 
overfitting problem, measuring learning accuracy, and applications of machine learning, 
such as data mining, robotic control, autonomous navigation, and bioinformatics.",,,Course,,
scw,DIGITAL IMAGE PROCESSING,2013,,,,"Digital image fundamentals, color models, image transform, image enhancement, 
spatial-domain and frequency-domain filters, image segmentation, binary morphology, 
image representation and description, applications of digital image processing.",,,Course,,
ssr,COMPONENT-BASED SOFTWARE DEVELOPMENT,2013,,,,"Concepts of component-based software development, including techniques, tools, languages for modelling, design, construction, and component decomposition and composition",,,Course,,
,SOFTWARE CONSTRUCTION AND EVOLUTION,2013,,,,"Knowledge of design to code translation.ู Coding practices for building quality programs including defensive programming, exception handling, self-documenting code and coding standards.ู Development and use of program documentation.ู The course extends towards concepts, methods, tools and techniques that support the ability of software to change and evolve over time.ู Associated issues include change tracking, implementation analysis, refactoring, program transformation and reverse engineering",,,Course,,
wdc,PARALLEL COMPUTING,2013,,,,"Motivations and applications of parallel computing, parallel computer architecture, Theory and practice of parallelizing serial computations, parallel programming for shared-memory multiprocessors, distributed-memory multiprocessors, and graphical processing units, performance issues covering speedup, efficiency, scalability and overhead analysis",,,Course,,
,SELECTED TOPICS IN PROGRAMMING LANGUAGES,2013,,,,Seminar on current research and development in programming languages.,,,Course,,
wdp,NET-CENTRIC COMPUTING 2,2013,,,,"Advanced knowledge and hands-on experience in computer communications and networking:ู IP addressing, routing protocols, and network configuration for internetworking with TCP/IP Using both state-of-the-art simulators and actual computer networking devices",,,Course,,
,SELECTED TOPICS IN INFORMATION SYSTEMS,2013,,,,Seminar on current research and development in information systems.,,,Course,,
,NATURAL LANGUAGE PROCESSING,2013,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g. machine translation, speech recognition and synthesis, and text mining)",,,Course,,
tnt,SELECTED TOPICS IN ARTIFICIAL INTELLIGENT SYSTEMS,2013,,,,Seminar on current research and development in artificial intelligent systems.,,,Course,,
nth,SELECTED TOPICS IN HUMAN-COMPUTER INTERACTION,2013,,,,Seminar on current research and development in human-computer interaction,,,Course,,
yao,WEB APPLICATION DEVELOPMENT,2013,,,,"Basic systems and protocols for providing services on the internet, developing and deploying web applications, topics covered web application development techniques for both client side and server side, session management, interfacing with other online services, web application security, web design patterns and reusable web application components",,,Course,,
mvp,SOFTWARE PROJECT MANAGEMENT,2013,,,,"Introduce knowledge in managing and controlling software project, project proposal development techniques including business case analysis, software cost and schedule estimation, project planning, risk management, resource management, software project control and software project evaluations",,,Course,,
,REAL-TIME GRAPHICS,2013,,,,"Requirements:visual realism for RTS (real-time systems), HCI for RTS, optimization of performance and visual realism; Hardware:ูCPU and GPU, networking for RTS, rendering pipeline, data structures (buffer, color depth, texture, accumulation, stencil); software:ู algorithm (rendering pipeline, level of detail, collision detection), data structures (texture maps, mip maps, light maps, space partitioning); applications: gaming and simulation",,,Course,,
nng,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
ojs,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
nng,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
scw,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, 
programming language syntax and semantics, data types, control structures, functions and 
parameter passing.",,,Course,,
wdp,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
wjr,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
pkw,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, 
inheritance, polymorphism, and overloading.",,,Course,,
wdc,DISCRETE STRUCTURES,2018,,,,"Intensive introduction to discrete mathematics as applied in computer science: Basic logic and its main application in Digital logic, proof techniques, sets, functions, relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
wdc,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
tnt,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
nth,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing.",,,Course,,
skn,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing.",,,Course,,
wdc,Statistics and Research methodology,2018,,,,"Fundamentals of research, research problem formulation design, and implementation research for interactive multimedia learning, basic statistics to analyze research data, data preparation and analysis, interpretation result of data analysis, conclusion and discussion and writing a research report.",,,Course,,
,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
yao,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
,DATA STRUCTURES,2018,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
yao,DATA STRUCTURES,2018,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
wjr,DATA STRUCTURES,2018,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
pps,DATA STRUCTURES AND ALGORITHMS,2018,,,,"Analysis of running time and complexity of the algorithms, data structures: arrays, linked lists, stacks, queues, trees, hash, graph, binary heap, analysis of running time and memory requirements in searching, algorithms for searching and sorting, algorithm techniques: brute force, divide-and-conquer, greedy algorithms, space and time tradeoffs",,,Course,,
wdc,DATA STRUCTURES AND ALGORITHMS,2018,,,,"Analysis of running time and complexity of the algorithms, data structures: arrays, linked lists, stacks, queues, trees, hash, graph, binary heap, analysis of running time and memory requirements in searching, algorithms for searching and sorting, algorithm techniques: brute force, divide-and-conquer, greedy algorithms, space and time tradeoffs",,,Course,,
wjr,DATA STRUCTURES AND ALGORITHMS,2018,,,,"Analysis of running time and complexity of the algorithms, data structures: arrays, linked lists, stacks, queues, trees, hash, graph, binary heap, analysis of running time and memory requirements in searching, algorithms for searching and sorting, algorithm techniques: brute force, divide-and-conquer, greedy algorithms, space and time tradeoffs",,,Course,,
ddp,INTRODUCTION TO COMPUTER SYSTEM AND ORGANIZATION,2018,,,,"Understanding of the internal structure and implementation of digital computers, computer boot up process, data representation and manipulation of atomic data, floating-point errors. The structure of a typical instruction set. The environment in which a program is compiled and executed. Computer Memory hierarchy, concepts for the hardware implementation of a processor: Combinatorial Circuits, Sequential Circuits, and Processor structure. Input/Output interfaces.",,,Course,,
pkl,INTRODUCTION TO COMPUTER SYSTEM AND ORGANIZATION,2018,,,,"Understanding of the internal structure and implementation of digital computers, computer boot up process, data representation and manipulation of atomic data, floating-point errors. The structure of a typical instruction set. The environment in which a program is compiled and executed. Computer Memory hierarchy, concepts for the hardware implementation of a processor: Combinatorial Circuits, Sequential Circuits, and Processor structure. Input/Output interfaces.",,,Course,,
,COMPUTER ORGANIZATION AND ARCHITECTURE,2013,,,,"Instruction set architectures (ISA), pipeline processing, multiprocessors, memory management, cache and virtual memory organization, process synchronization, CPU scheduling, multithreading, file system, input/output control and devices.",,,Course,,
wdc,COMPUTER ARCHITECTURE AND OPERATING SYSTEMS,2018,,,,"Instruction set architectures (ISA), pipeline processing, multiprocessors, memory management, cache and virtual memory organization, process synchronization, CPU scheduling, multithreading, file system, input/output control and devices.",,,Course,,
wlr,COMPUTER ARCHITECTURE AND OPERATING SYSTEMS,2018,,,,"Instruction set architectures (ISA), pipeline processing, multiprocessors, memory management, cache and virtual memory organization, process synchronization, CPU scheduling, multithreading, file system, input/output control and devices.",,,Course,,
ppr,COMPUTER ARCHITECTURE AND OPERATING SYSTEMS,2018,,,,"Instruction set architectures (ISA), pipeline processing, multiprocessors, memory management, cache and virtual memory organization, process synchronization, CPU scheduling, multithreading, file system, input/output control and devices.",,,Course,,
,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
wdc,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
ojs,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
,INTRODUCTION TO SOFTWARE ENGINEERING,2018,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
wdc,INTRODUCTION TO SOFTWARE ENGINEERING,2018,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
ssr,INTRODUCTION TO SOFTWARE ENGINEERING,2018,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
tnt,INTRODUCTION TO SOFTWARE ENGINEERING,2018,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
wdc,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
tnt,INTRODUCTION TO SOFTWARE ENGINEERING,2013,,,,"Fundamental of software engineering principles, software development methodology, software process models, basics of software project planning and project management, basics of time and cost estimation, requirement elicitation and specification, software analysis and design, concept of software architecture, software construction techniques including design pattern, component-oriented development, basics of verification and validation, software evolution, software development environment, and sample of CASE tools.",,,Course,,
ssr,PRACTICES AND PATTERNS IN OBJECT-ORIENTED PROGRAMMING,2013,,,,"Introduction to research and theory on topics in human information processing including perception, attention, pattern recognition, memory, representation of knowledge, language, problem solving, reasoning, and learning, with emphasis on the relationship to computer models of these processes and implications of this body of knowledge for building information systems.",,,Course,,
,SOFTWARE REQUIREMENT SPECIFICATION AND MANAGEMENT,2013,,,,"Quantify many aspects of requirement processes include eliciting, analyzing, negotiating, specifying, validating and basic principle of managing requirements introduce methods, techniques and tools used to define, document and ensure customer satisfaction.",,,Course,,
wdc,SOFTWARE REQUIREMENT SPECIFICATION AND MANAGEMENT,2013,,,,"Quantify many aspects of requirement processes include eliciting, analyzing, negotiating, specifying, validating and basic principle of managing requirements introduce methods, techniques and tools used to define, document and ensure customer satisfaction.",,,Course,,
,COMPUTER GRAPHICS 1,2013,,,,"Software and hardware principles of interactive graphics, general methods for designing and displaying output, elementary operations in two-and three-dimensional space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
,COMPUTER GRAPHICS 1,2013,,,,"Software and hardware principles of interactive graphics, general methods for designing and displaying output, elementary operations in two-and three-dimensional space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
,COMPUTER GRAPHICS 1,2018,,,,"Software and hardware principles of interactive graphics, general methods for designing and displaying output, elementary operations in two-and three-dimensional space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
,COMPUTER GRAPHICS 1,2018,,,,"Software and hardware principles of interactive graphics, general methods for designing and displaying output, elementary operations in two-and three-dimensional space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
wdc,BASIC THEORY IN ARTIFICIAL INTELLIGENCE,2013,,,,,,,Course,,
,BASIC THEORY IN ARTIFICIAL INTELLIGENCE,2013,,,,,,,Course,,
,HUMAN – COMPUTER INTERACTION,2013,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
wdc,HUMAN – COMPUTER INTERACTION,2013,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
ppr,HUMAN – COMPUTER INTERACTION,2013,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
pkw,OBJECT-ORIENTED PROGRAMMING,2013,,,,,,,Course,,
,OBJECT-ORIENTED PROGRAMMING,2013,,,,,,,Course,,
pkw,OBJECT-ORIENTED CONCEPTS,2018,,,,"Developing of analytical and problem-solving skills through object-oriented paradigm. Integrating of conceptual and state-of-the-art practical approaches in software development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design and source code.",,,Course,,
ssr,OBJECT-ORIENTED CONCEPTS,2018,,,,"Developing of analytical and problem-solving skills through object-oriented paradigm. Integrating of conceptual and state-of-the-art practical approaches in software development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design and source code.",,,Course,,
,OBJECT-ORIENTED CONCEPTS,2018,,,,"Developing of analytical and problem-solving skills through object-oriented paradigm. Integrating of conceptual and state-of-the-art practical approaches in software development life cycle (SDLC), object-oriented design techniques and tools, objectoriented programming based on the design, consistency verification between the design and source code.",,,Course,,
nng,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2018,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, inheritance, polymorphism, and overloading.",,,Course,,
nth,INTRODUCTION TO OBJECT-ORIENTED PROGRAMMING,2018,,,,"Introduction to object-oriented programming, abstract data types, encapsulation, inheritance, polymorphism, and overloading.",,,Course,,
ssr,SOCIAL AND PROFESSIONAL ETHICS,2013,,,,"Social context of computing, methods and tools of analysis, professional and ethical responsibilities, intellectual property, risks and liabilities of computer-based systems, privacy and civil liberties, intellectual property infringement, computer crime in computing.",,,Course,,
ojs,DESIGN AND ANALYSIS OF ALGORITHMS,2018,,,,"Techniques used in design and analysis of algorithms such as divide-and-conquer, greedy algorithms, dynamic programming, graph traveling, backtracking, branch and bound. Study the topics of sorting, searching, Fourier transform, randomized algorithms, graph algorithms and optimization methods.",,,Course,,
ddp,DESIGN AND ANALYSIS OF ALGORITHMS,2018,,,,"Techniques used in design and analysis of algorithms such as divide-and-conquer, greedy algorithms, dynamic programming, graph traveling, backtracking, branch and bound. Study the topics of sorting, searching, Fourier transform, randomized algorithms, graph algorithms and optimization methods.",,,Course,,
ppr,OPERATING SYSTEMS I,2018,,,,"Basic concepts of operating systems design and implementation mutual exclusion, process management, memory management, file and device management, computer security and case studies.",,,Course,,
wlr,OPERATING SYSTEMS I,2018,,,,"Basic concepts of operating systems design and implementation mutual exclusion, process management, memory management, file and device management, computer security and case studies.",,,Course,,
wdp,PROGRAMMING LANGUAGES AND PARADIGMS,2013,,,,"Principles of programming languages, syntax and semantics. Different programming paradigms, performance-aware programming. Programming using Application Programming Interface (API), library for data manipulation event-driven programming.",,,Course,,
pps,DATA VISUALIZATION,2018,,,,"This course focuses on the design implementation and evaluation of complementary visual and verbal representations of patterns, and learn how to acquire, parse, and analyze large datasets in order to convey findings answer questions, drive decisions, and provide persuasive evidence supported by data",,,Course,,
tnt,DATA VISUALIZATION,2018,,,,"This course focuses on the design implementation and evaluation of complementary visual and verbal representations of patterns, and learn how to acquire, parse, and analyze large datasets in order to convey findings answer questions, drive decisions, and provide persuasive evidence supported by data",,,Course,,
wjr,DATA VISUALIZATION,2018,,,,"This course focuses on the design implementation and evaluation of complementary visual and verbal representations of patterns, and learn how to acquire, parse, and analyze large datasets in order to convey findings answer questions, drive decisions, and provide persuasive evidence supported by data",,,Course,,
pkw,DATABASE SYSTEMS 1,2018,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management.",,,Course,,
skn,DATABASE SYSTEMS 1,2018,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management.",,,Course,,
scw,DATABASE SYSTEMS 1,2018,,,,"Fundamental database concepts and architecture, conceptual data models, design and implementation, query languages, metadata, physical data storage, file organizations, and introduction to transaction management.",,,Course,,
nng,INTRODUCTION TO SOFTWARE TESTING,2018,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
mvp,INTRODUCTION TO SOFTWARE TESTING,2018,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
ppr,INTRODUCTION TO SOFTWARE TESTING,2018,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
wdc,ARTIFICIAL INTELLIGENCE FUNDAMENTALS,2018,,,,"Problems in artificial intelligent systems, knowledge representation, uniform search, inform search, constraint-based problems, logics and automatic reasoning, game theory, introduction to machine learning",,,Course,,
tnt,ARTIFICIAL INTELLIGENCE FUNDAMENTALS,2018,,,,"Problems in artificial intelligent systems, knowledge representation, uniform search, inform search, constraint-based problems, logics and automatic reasoning, game theory, introduction to machine learning",,,Course,,
nth,HUMAN INFORMATION PROCESSING,2018,,,,"Introduction to research and theory on topics in human information processing including perception, attention, pattern recognition, memory, representation of knowledge, language, problem solving, reasoning, and learning, with emphasis on the relationship to computer models of these processes and implications of this body of knowledge for building information systems.",,,Course,,
ppr,INTRODUCTION TO SOFTWARE TESTING,2013,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
wdc,COMPUTER SECURITY,2013,,,,"Security of information, computer systems, databases, networks and communications, symmetric and asymmetric cryptography, digital signatures, digital certificates, malwares, security policies and models, security assurance, risk analysis, security administration of information systems.",,,Course,,
wlr,OPERATING SYSTEMS I,2013,,,,"Basic concepts of operating systems design and implementation mutual exclusion, process management, memory management, file and device management, computer security and case studies.",,,Course,,
,ARTIFICIAL INTELLIGENCE IN PRACTICE,2013,,,,"Probabilistic reasoning, non-monotonic reasoning, frame, conceptual graph, Bayesian networks, markov networks, relational probability models, hidden Markov models, decision theory. This course focuses on implementation using artificial intelligence programming such as prolog, lisp or python.",,,Course,,
,NATURAL LANGUAGE PROCESSING,2013,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g. machine translation, speech recognition and synthesis, and text mining)",,,Course,,
,NATURAL LANGUAGE PROCESSING,2013,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g. machine translation, speech recognition and synthesis, and text mining)",,,Course,,
,DATA STRUCTURES,2013,,,,"Introduction to algorithm analysis, data abstraction, and fundamental data structures including linear data structures and nonlinear data structures, dynamic storage allocation, searching and sorting techniques.",,,Course,,
pkw,COMPUTER PROGRAMMING FUNDAMENTALS,2013,,,,"Procedural programming, algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
scw,COMPUTER PROGRAMMING FUNDAMENTALS,2013,,,,"Procedural programming, algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
tnt,COMPUTER PROGRAMMING FUNDAMENTALS,2013,,,,"Procedural programming, algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
tnt,INTRODUCTION TO COMPUTER PROGRAMMING,2013,,,,"Introduction to algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing.",,,Course,,
ssr,OBJECT-ORIENTED PROGRAMMING,2013,,,,"Design model and object-oriented programming, abstract data types, encapsulation, inheritance, polymorphism, overloading, generic programming features, and exception handling.",,,Course,,
yao,OBJECT-ORIENTED PROGRAMMING,2013,,,,"Design model and object-oriented programming, abstract data types, encapsulation, inheritance, polymorphism, overloading, generic programming features, and exception handling.",,,Course,,
wlr,SOFTWARE DEVELOPMENT USING APPLICATION PROGRAMMING INTERFACE,2013,,,,"Programming using Application Programming Interface, user interface programming including streams and files, data collection library, and event-driven programming.",,,Course,,
wlr,PROGRAMMING LANGUAGES AND PARADIGMS,2013,,,,"Principles of programming languages, syntax and semantics, different programming paradigms.",,,Course,,
ssr,INTRODUCTION TO SOFTWARE TESTING,2013,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
wlr,NET-CENTRIC COMPUTING 1,2013,,,,"Study network standards, client-server models, internet protocol, network management, basic concepts of distributed computing, multimedia systems, mobile and wireless computing, and network security.",,,Course,,
tnt,BASIC THEORY IN ARTIFICIAL INTELLIGENCE,2013,,,,"Definition of Intelligent behavior, design of intelligent agents (nature of environments and nature of agents), problem solving by searching, uninformed search, มคอ.2 76 informed search, two-player search, constraint satisfaction problem, knowledge representation with logics, automated reasoning and theorem proving.",,,Course,,
tpb,ARTIFICIAL INTELLIGENCE IN PRACTICE,2013,,,,"Probabilistic reasoning, non-monotonic reasoning, frame, conceptual graph, bayesian networks, markov networks, relational probability models, hidden markov models, decision theory.ู This course focuses on implementation using artificial intelligence programming such as prolog, lisp or python.",,,Course,,
ssr,SOFTWARE CONFIGURATION MANAGEMENT,2013,,,,"Knowledge and fundamental of control and maintenance software development work products integrity throughout project’s life cycles , software configuration management process, configuration baseline, and auditions.",,,Course,,
ssr,SEMINAR IN SOFTWARE ENGINEERING,2013,,,,Seminar and field trip to study industrial use of best practices and standards of Software Engineering.,,,Course,,
skn,SELECTED TOPICS IN COMPUTER SCIENCE,2013,,,,Topics selected from areas in Computer Science.ู Keep up with the new technology and knowledge.,,,Course,,
wdp,NETWORK DESIGN AND MANAGEMENT,2013,,,,"Resource allocation, fixed allocation, time division multiplexing (TDM), frequency division multiplexing (FDM), wavelength division multiplexing (WDM), dynamic allocation, fairness, congestion control, dedicated allocation of network bandwidth for high-demand, dynamic circuit network (DCN), and example of applications.",,,Course,,
skn,SELECTED TOPICS IN INFORMATION SYSTEMS,2013,,,,"Advanced database management system design principles and techniques.ูTopics may be selected from:ู access methods, query processing and optimization, transaction management:ู concurrency control, recovery, and data security; distributed database systems:ู concepts, design and transaction management, and introduction to object-ูู oriented database systems.",,,Course,,
tpb,NATURAL LANGUAGE PROCESSING,2013,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g.ูmachine translation, speech recognition and synthesis, and text mining)",,,Course,,
lpp,SELECTED TOPICS IN HUMAN-COMPUTER INTERACTION,2013,,,,Seminar on current research and development in human-computer interaction,,,Course,,
mvp,SELECTED TOPICS IN SOFTWARE ENGINEERING,2013,,,,Seminar on current research and development in software engineering.,,,Course,,
pkw,DISCRETE STRUCTURES,2018,,,,"Intensive introduction to discrete mathematics as applied in computer science: Basic logic and its main application in Digital logic, proof techniques, sets, functions, relations, Boolean algebra, basic of counting, and graphs.",,,Course,,
pkw,PROBLEM SOLVING BASICS AND COMPUTER PROGRAMMING,2018,,,,"Algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing, recursive functions, testing and debugging.",,,Course,,
tnt,INTRODUCTION TO COMPUTER PROGRAMMING,2018,,,,"Introduction to algorithmic problem solving, structural design and programming, programming language syntax and semantics, data types, control structures, functions and parameter passing.",,,Course,,
ojs,INTRODUCTION TO COMPUTER PROGRAMMING USING PYTHON,2018,,,,"Basic computer programming and problem solving using Python. Structural design and Programming, basic data types. Control Structures. Functions, python built-in data structures: lists, tuples, sets, dictionaries, and range, numerical processing library, arrays and matrix, library for Data Visualization.",,,Course,,
pps,Statistics and Research methodology,2018,,,,"Fundamentals of research, research problem formulation design, and implementation research for interactive multimedia learning, basic statistics to analyze research data, data preparation and analysis, interpretation result of data analysis, conclusion and discussion and writing a research report.",,,Course,,
pkw,DATA STRUCTURES AND ALGORITHMS,2018,,,,"Analysis of running time and complexity of the algorithms, data structures: arrays, linked lists, stacks, queues, trees, hash, graph, binary heap, analysis of running time and memory requirements in searching, algorithms for searching and sorting, algorithm techniques: brute force, divide-and-conquer, greedy algorithms, space and time tradeoffs",,,Course,,
ddp,INTRODUCTION TO COMPUTER SYSTEM AND ORGANIZATION,2018,,,,"Understanding of the internal structure and implementation of digital computers, computer boot up process, data representation and manipulation of atomic data, floating-point errors. The structure of a typical instruction set. The environment in which a program is compiled and executed. Computer Memory hierarchy, concepts for the hardware implementation of a processor: Combinatorial Circuits, Sequential Circuits, and Processor structure. Input/Output interfaces.",,,Course,,
ddp,COMPUTER ARCHITECTURE AND OPERATING SYSTEMS,2018,,,,"Instruction set architectures (ISA), pipeline processing, multiprocessors, memory management, cache and virtual memory organization, process synchronization, CPU scheduling, multithreading, file system, input/output control and devices.",,,Course,,
wdc,COMPUTER NETWORK AND SECURITY,2018,,,,"Internet protocol including information security, digital signatures, digital certificates, malwares, security policies and models, security assurance, risk analysis, security administration of information systems.",,,Course,,
wdp,COMPUTER NETWORK AND SECURITY,2018,,,,"Internet protocol including information security, digital signatures, digital certificates, malwares, security policies and models, security assurance, risk analysis, security administration of information systems.",,,Course,,
wlr,PROGRAMMING LANGUAGES AND PARADIGMS,2018,,,,"Principles of programming languages, syntax and semantics. Different programming paradigms, performance-aware programming. Programming using Application Programming Interface (API), library for data manipulation event-driven programming.",,,Course,,
pps,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
wdp,PRINCIPLES OF DATA SCIENCE,2018,,,,"Fundamental principles of data science including algorithms, processes, methods, data-analytic thinking and tools to support problem-focused data-analytic thinking. Introduction to topics in data science: Data Manipulation, Data Analysis with Statistics and Machine Learning, Data Communication with Information Visualization, and Data at Scale.",,,Course,,
wdp,COMPUTER PROGRAMMING FOR BUSINESS,2018,,,,"Computer programming to solve business problems, algorithm design and implementation via programming paradigm and tools suitable for business problems domain, testing and debugging, basic graphical user interface development, event-driven programming.",,,Course,,
tpb,COMPUTER PROGRAMMING FOR BUSINESS,2018,,,,"Computer programming to solve business problems, algorithm design and implementation via programming paradigm and tools suitable for business problems domain, testing and debugging, basic graphical user interface development, event-driven programming.",,,Course,,
ojs,COMPUTER PROGRAMMING USING PYTHON,2018,,,,"Computer programming and problem solving using python, structural design and Programming, basic data types, control Structures, functions. Recursions, python built-in data structures: lists, tuples, sets, dictionaries, and range, testing and debugging, objectoriented programming using python, class, methods, numerical processing library, arrays and matrix, library for data visualization, introduction to library for data analytics.",,,Course,,
ssr,INTRODUCTION TO SOFTWARE TESTING,2018,,,,"Software testing and quality assurance concepts, test data generation using whitebox and black-box techniques, software testing at unit, module, sub-system, and system levels, and software testing process and planning.",,,Course,,
pkw,ARTIFICIAL INTELLIGENCE FUNDAMENTALS,2018,,,,"Problems in artificial intelligent systems, knowledge representation, uniform search, inform search, constraint-based problems, logics and automatic reasoning, game theory, introduction to machine learning",,,Course,,
mvp,SOFTWARE PROCESS AND QUALITY ASSURANCE,2018,,,,"Study essential components of software process including activities, methods, and practices used to develop and maintain software development including its work products. Introduce basic knowledge of process and product quality assurance, how to define quality for measures and measurement.",,,Course,,
nng,SOFTWARE REQUIREMENT SPECIFICATION AND MANAGEMENT,2018,,,,"Quantify many aspects of requirement processes include eliciting, analyzing, negotiating, specifying, validating and basic principle of managing requirements introduce methods, techniques and tools used to define, document and ensure customer satisfaction.",,,Course,,
wlr,COMPUTER GRAPHICS 1,2018,,,,"Software and hardware principles of interactive graphics, general methods for designing and displaying output, elementary operations in two-and three-dimensional space, transformational geometry, viewing transformations, clipping, color theory.",,,Course,,
wdc,SOCIAL AND PROFESSIONAL ETHICS,2018,,,,"Social context of computing, methods and tools of analysis, professional and ethical responsibilities, intellectual property, risks and liabilities of computer-based systems, privacy and civil liberties, intellectual property infringement, computer crime in computing.",,,Course,,
ssr,SOCIAL AND PROFESSIONAL ETHICS,2018,,,,"Social context of computing, methods and tools of analysis, professional and ethical responsibilities, intellectual property, risks and liabilities of computer-based systems, privacy and civil liberties, intellectual property infringement, computer crime in computing.",,,Course,,
nth,LEARNING MEASUREMENT AND EVALUATION,2018,,,,"Significance of measurement and evaluation in learning, principles and techniques of learning measurement and evaluation, development and use of measurement and evaluation tools, application of learning measurement and evaluation result in learning through multimedia, basic statistics and software for measurement and evaluation of learning",,,Course,,
lpp,LEARNING PRINCIPLES FOR INTERACTIVE MULTIMEDIA,2018,,,,"Principles and theory of learning, learning process, type of learning, teaching and learning problems, factors influencing learning, motivation, and application to interactive multimedia design and enhance learning.",,,Course,,
ddp,AUTOTMATA THEORY,2018,,,,"Fundamentals of abstract machine and language, automata theory, regular expression, regular language, pushdown automata, context-free language, Turing machine, the halting problem, undecidability and intractability",,,Course,,
wlr,NET-CENTRIC COMPUTING,2018,,,,"Basic network components and major network standards for computer communications via the Internet. The OSI layered model. The TCP/IP layered model. Basic functions of each layer. Major network protocols in each layers and their interoperability, socket programming.",,,Course,,
wlr,COMPUTER SECURITY,2018,,,,"Security of information, computer systems, databases, networks and communications, symmetric and asymmetric cryptography, digital signatures, digital certificates, malwares, security policies and models, security assurance, risk analysis, security administration of information systems.",,,Course,,
wdp,PRACTICAL COMPUTER NETWORKING,2018,,,,"Advanced knowledge and hands-on experience in computer communications and networking, basic router, switch, and end-host configurations, internet protocol version 6 (Ipv6). IP addressing, network address translation (NAT), dynamic host configuration protocol (DHCP), switched networks, virtual LAN (VLAN), inter VLAN routing, routing protocols, basic network security, access control List (ACL).",,,Course,,
pkl,EMBEDDED SYSTEMS DESIGN,2018,,,,"Design and implementation of integrated hardware and software for embedded systems to meet specifications/constraints, applications of embedded systems and related research.",,,Course,,
wlr,WIRELESS NETWORK TECHNOLOGY,2018,,,,"Overview of mobile communications and wireless networks, wireless transmission basis, multiple access control techniques, mobile telephone systems, wireless LAN. Mobile IP, Bluetooth, zigbee. WiMAX, wireless network security.",,,Course,,
ppr,INTRODUCTION TO CLOUD COMPUTING TECHNOLOGY,2018,,,,"Cloud computing concepts and characteristics, cloud computing service models e.g. Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and software as a service (SaaS), cloud application development, cloud security",,,Course,,
wdp,COMPUTER ARCHITECTURE,2018,,,,"Topics in Computer System Performance. Advanced concepts in designing computer processors: Pipelining, Static and Dynamic scheduling, Superscalar and Vector executions. Advanced concepts in Computer Memory Hierarchy: Cache optimization and Virtual memory. Multi-processors and Cache coherence.",,,Course,,
skn,SELECTED TOPICS IN INFORMATION SYSTEMS,2018,,,,Seminar on current research and development in information systems.,,,Course,,
pps,PRACTICAL MACHINE LEARNING AND DATA MINING,2018,,,,"Fundamentals of algorithm design and data analysis associated with machine learning. Topics include techniques of statistical, probability theory, and combinatorial optimization. Concepts and techniques related to data mining; strengths and limitations of various data mining techniques, including classification, association analysis, and cluster analysis.",,,Course,,
wjr,PRACTICAL MACHINE LEARNING AND DATA MINING,2018,,,,"Fundamentals of algorithm design and data analysis associated with machine learning. Topics include techniques of statistical, probability theory, and combinatorial optimization. Concepts and techniques related to data mining; strengths and limitations of various data mining techniques, including classification, association analysis, and cluster analysis.",,,Course,,
pkl,PRACTICAL MACHINE LEARNING AND DATA MINING,2018,,,,"Fundamentals of algorithm design and data analysis associated with machine learning. Topics include techniques of statistical, probability theory, and combinatorial optimization. Concepts and techniques related to data mining; strengths and limitations of various data mining techniques, including classification, association analysis, and cluster analysis.",,,Course,,
wjr,DATA WAREHOUSING AND BUSINESS INTELLIGENCE,2018,,,,"Data warehousing, data warehouse architecture, physical database design, dimension modeling; extraction, transformation, and loading, its applications to business intelligence, business analytics, OLAP, introduction to data mining, data visualization, data warehouse storage models and technology, business analytics techniques and tools for business intelligence",,,Course,,
wdp,MODELING FOR DATA SCIENCE,2018,,,,"Fundamentals of statistical inference and testing, and introduction to statistical modeling. Inference and testing, covering topics such as maximum likelihood estimates, hypothesis testing, likelihood ratio test, Bayesian inference. Introduction to statistical modeling via introductory lectures on linear regression models, generalized linear regression models, nonparametric regression, and statistical computing. Application of statistical models as a means of representing and manipulating data, modeling uncertainty, and discovering new insights from data.",,,Course,,
tpb,MODELING FOR DATA SCIENCE,2018,,,,"Fundamentals of statistical inference and testing, and introduction to statistical modeling. Inference and testing, covering topics such as maximum likelihood estimates, hypothesis testing, likelihood ratio test, Bayesian inference. Introduction to statistical modeling via introductory lectures on linear regression models, generalized linear regression models, nonparametric regression, and statistical computing. Application of statistical models as a means of representing and manipulating data, modeling uncertainty, and discovering new insights from data.",,,Course,,
skn,BUSINESS COMPUTING AND ELECTRONIC COMMERCE,2018,,,,"Concepts of information systems in business management, business application software, business processes over computer networks, electronic commerce infrastructures, computing and data exchange standards.",,,Course,,
ppr,COMPUTER SIMULATION AND FORECASTING TECHNIQUES IN BUSINESS,2018,,,,"Concepts of information systems in business management, business application software, business processes over computer networks, electronic commerce infrastructures, computing and data exchange standards.",,,Course,,
wjr,INFORMATION RETRIEVAL,2018,,,,"Theories and techniques on computerized information indexing and retrieval. Topics include Boolean model, vector model, text processing and analysis, evaluation of information retrieval systems, Web-based search engines.",,,Course,,
wjr,MANAGEMENT INFORMATION SYSTEMS,2018,,,,"Fundamental of information systems, organizations and management; Information technology infrastructure; Key System Applications: management and enterprise information systems; Building information systems; Managing information systems; Ethics and social impact of information systems.",,,Course,,
mvp,SELECTED TOPICS IN SOFTWARE ENGINEERING,2018,,,,Seminar on current research and development in software engineering.,,,Course,,
pkl,MOBILE APPLICATION DEVELOPMENT,2018,,,,"Developing landscape of mobile applications, web-based mobile applications, mobile platforms, the specific constraints and requirements of user interface design for limited-resource devices, conceptual overview, design issues, and practical development issues.",,,Course,,
wdc,APPLICATION PROGRAMMING INTERFACE,2018,,,,"Workshop in using Application Programming Interface, user interface programming including streams and files, data collection library, and event-driven programming.",,,Course,,
mvp,ENTERPRISE SOFTWARE ARCHITECTURE,2018,,,,"Concepts of enterprise software architecture; enterprise software architectural styles from both the structural and behavioral viewpoints together with strengths and weaknesses of each; study architectural design patterns and how to apply these patterns, case studies of techniques towards how to design enterprise software architecture.",,,Course,,
mvp,WEB SERVICE DEVELOPMENT CONCEPTS,2018,,,,"Basic concepts of web services, web service architecture and related standards, security issues and protocols for web services, industrial technologies relating to web มคอ.2 95 service developments, applying theoretical concepts in designing and developing web services for businesses.",,,Course,,
mvp,ENTREPRENEURSHIP FOR DIGITAL PRODUCT,2018,,,,"Study of the entrepreneurial process focusing on the digital product development and management; ideation, innovation, startup models. Practice entrepreneurial activities: product vision creation, business case analysis, opportunities analysis, market position identification, digital product design and development.",,,Course,,
yao,WEB APPLICATION DEVELOPMENT,2018,,,,"Basic systems and protocols for providing services on the internet, developing and deploying web applications, topics covered web application development techniques for both client side and server side, session management, interfacing with other online services, web application security, web design patterns and reusable web application components.",,,Course,,
ssr,SEMINAR IN SOFTWARE ENGINEERING,2018,,,,Seminar and field trip to study industrial use of best practices and standards of Software Engineering.,,,Course,,
pkl,MACHINE LEARNING,2018,,,,"Learning theory, inductive and deductive learning, naïve Bayesian learning, decision trees, supervised learning, unsupervised learning, reinforcement learning, overfitting problem, measuring learning accuracy, and applications of machine learning, such as data mining, robotic control, autonomous navigation, and bioinformatics.",,,Course,,
tpb,NATURAL LANGUAGE PROCESSING,2018,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g. machine translation, speech recognition and synthesis, and text mining)",,,Course,,
scw,DIGITAL IMAGE PROCESSING,2018,,,,"Deterministic and stochastic grammars, morphological analysis, syntax analysis, semantic analysis, discourse analysis, and applications (e.g. machine translation, speech recognition and synthesis, and text mining)",,,Course,,
mvp,HUMAN – COMPUTER INTERACTION,2018,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
nth,HUMAN-CENTERED DATA SCIENCE,2018,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
mvp,HUMAN-CENTERED DATA SCIENCE,2018,,,,"Importance of human-computer interaction (HCI) to the development of acceptable and usable systems, context for HCI, user-centered design, evaluation, measure of usability, human-information processing model applicable to HCI, most common interface design mistakes, and user interface principle and guildines",,,Course,,
lpp,INSTRUCTIONAL DESIGN AND DEVELOPMENT OF INTERACTIVE MULTIMEDIA,2018,,,,"Principles of and research in interactive multimedia, design, develop and evaluate a prototype of interactive multimedia instructional system",,,Course,,
lpp,SELECTED TOPICS IN HUMAN-COMPUTER INTERACTION,2018,,,,Seminar on current research and development in human-computer interaction,,,Course,,
wdp,NETWORK DESIGN AND MANAGEMENT,2018,,,,"Topics in computer network design and management, case Studies. IP address subnetting, virtual local area networks (VLANs), congestion controls, resource allocations for providing quality of service (QoS), classic and state-of-the-art network management protocols, network performance analysis.",,,Course,,
ppr,SELECTED TOPICS IN DATA SCIENCE,2018,,,,The purpose of Selected Topics in Data Science being a topics course is to respond to new researches and technologies in the rapidly evolving fields of Data Science.,,,Course,,
pps,SELECTED TOPICS IN DATA SCIENCE,2018,,,,The purpose of Selected Topics in Data Science being a topics course is to respond to new researches and technologies in the rapidly evolving fields of Data Science.,,,Course,,
